{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3qfRoipQ8VH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import re\n",
        "\n",
        "EETA_DEFAULT = 0.001\n",
        "\n",
        "class LARS(Optimizer):\n",
        "    def __init__(self, params, lr=required, momentum=0.9, use_nesterov=False, weight_decay=0.0, exclude_from_weight_decay=None, exclude_from_layer_adaptation=None, classic_momentum=True, eeta=EETA_DEFAULT):\n",
        "        \"\"\"Constructs a LARSOptimizer.\n",
        "        Args:\n",
        "        lr: A `float` for learning rate.\n",
        "        momentum: A `float` for momentum.\n",
        "        use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
        "        weight_decay: A `float` for weight decay.\n",
        "        exclude_from_weight_decay: A list of `string` for variable screening, if\n",
        "            any of the string appears in a variable's name, the variable will be\n",
        "            excluded for computing weight decay. For example, one could specify\n",
        "            the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
        "            from weight decay.\n",
        "        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
        "            for layer adaptation. If it is None, it will be defaulted the same as\n",
        "            exclude_from_weight_decay.\n",
        "        classic_momentum: A `boolean` for whether to use classic (or popular)\n",
        "            momentum. The learning rate is applied during momeuntum update in\n",
        "            classic momentum, but after momentum for popular momentum.\n",
        "        eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
        "        name: The name for the scope.\n",
        "        \"\"\"\n",
        "        self.epoch = 0\n",
        "        defaults = dict(lr=lr, momentum=momentum, use_nesterov=use_nesterov, weight_decay=weight_decay, exclude_from_weight_decay=exclude_from_weight_decay,\n",
        "                        exclude_from_layer_adaptation=exclude_from_layer_adaptation, classic_momentum=classic_momentum, eeta=eeta)\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.use_nesterov = use_nesterov\n",
        "        self.classic_momentum = classic_momentum\n",
        "        self.eeta = eeta\n",
        "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "        if exclude_from_layer_adaptation:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "        else:\n",
        "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "    def step(self, epoch=None, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "        if epoch is None:\n",
        "            epoch = self.epoch\n",
        "            self.epoch += 1\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group[\"weight_decay\"]\n",
        "            momentum = group[\"momentum\"]\n",
        "            eeta = group[\"eeta\"]\n",
        "            lr = group[\"lr\"]\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                param = p.data\n",
        "                grad = p.grad.data\n",
        "\n",
        "                param_state = self.state[p]\n",
        "\n",
        "                # TODO: get param names\n",
        "                # if self._use_weight_decay(param_name):\n",
        "                grad += self.weight_decay * param\n",
        "\n",
        "                if self.classic_momentum:\n",
        "                    trust_ratio = 1.0\n",
        "\n",
        "                    # TODO: get param names\n",
        "                    # if self._do_layer_adaptation(param_name):\n",
        "                    w_norm = torch.norm(param)\n",
        "                    g_norm = torch.norm(grad)\n",
        "\n",
        "                    device = g_norm.get_device()\n",
        "                    trust_ratio = torch.where(\n",
        "                        w_norm.ge(0),\n",
        "                        torch.where(g_norm.ge(0), (self.eeta * w_norm / g_norm), torch.Tensor([1.0]).to(device)),\n",
        "                        torch.Tensor([1.0]).to(device),\n",
        "                    ).item()\n",
        "\n",
        "                    scaled_lr = lr * trust_ratio\n",
        "                    if \"momentum_buffer\" not in param_state:\n",
        "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
        "                            p.data\n",
        "                        )\n",
        "                    else:\n",
        "                        next_v = param_state[\"momentum_buffer\"]\n",
        "\n",
        "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
        "                    if self.use_nesterov:\n",
        "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
        "                    else:\n",
        "                        update = next_v\n",
        "\n",
        "                    p.data.add_(-update)\n",
        "                else:\n",
        "                    raise NotImplementedError\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _use_weight_decay(self, param_name):\n",
        "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "        if not self.weight_decay:\n",
        "            return False\n",
        "        if self.exclude_from_weight_decay:\n",
        "            for r in self.exclude_from_weight_decay:\n",
        "                if re.search(r, param_name) is not None:\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def _do_layer_adaptation(self, param_name):\n",
        "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
        "        if self.exclude_from_layer_adaptation:\n",
        "            for r in self.exclude_from_layer_adaptation:\n",
        "                if re.search(r, param_name) is not None:\n",
        "                    return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_features, n_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(n_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "C1PjPhd7RO9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# InfoNCE Lower bound\n",
        "def InfoNCE(mu, z):\n",
        "    mu = mu.unsqueeze(0)\n",
        "    z = z.unsqueeze(1)\n",
        "    score = -((z-mu)**2).sum(-1)/20.  # 80 for ImageNet\n",
        "    lower_bound = -score.logsumexp(dim=1).mean()\n",
        "    return lower_bound\n",
        "\n",
        "# def InfoNCE(mu, z):\n",
        "#     AA = (mu*mu).sum(1).unsqueeze(0)\n",
        "#     BB = (z*z).sum(1, keepdims=True)\n",
        "#     AB = torch.mm(z, mu.transpose(0, 1))\n",
        "#     score = -(AA-2.*AB+BB)/80.  # 80 for ImageNet\n",
        "#     lower_bound = -score.logsumexp(dim=1).mean()\n",
        "#     return lower_bound"
      ],
      "metadata": {
        "id": "wH2lhpOjRRvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def gen_mask(k, feat_dim):\n",
        "    mask = None\n",
        "    for i in range(k):\n",
        "        tmp_mask = torch.triu(torch.randint(0, 2, (feat_dim, feat_dim)), 1)\n",
        "        tmp_mask = tmp_mask + torch.triu(1-tmp_mask,1).t()\n",
        "        tmp_mask = tmp_mask.view(tmp_mask.shape[0], tmp_mask.shape[1],1)\n",
        "        mask = tmp_mask if mask is None else torch.cat([mask,tmp_mask],2)\n",
        "    return mask\n",
        "\n",
        "def entropy(prob):\n",
        "    # assume m x m x k input\n",
        "    return -torch.sum(prob*torch.log(prob),1)\n",
        "\n",
        "class NT_Xent(nn.Module):\n",
        "    def __init__(self, batch_size, temperature, mask):\n",
        "        super(NT_Xent, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.mask = mask\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        \"\"\"\n",
        "        We do not sample negative examples explicitly.\n",
        "        Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N − 1) augmented examples within a minibatch as negative examples.\n",
        "        \"\"\"\n",
        "        p1 = torch.cat((z_i, z_j), dim=0)\n",
        "        sim = self.similarity_f(p1.unsqueeze(1), p1.unsqueeze(0)) / self.temperature\n",
        "\n",
        "        sim_i_j = torch.diag(sim, self.batch_size)\n",
        "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
        "\n",
        "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(self.batch_size * 2, 1)\n",
        "        negative_samples = sim[self.mask].reshape(self.batch_size * 2, -1)\n",
        "\n",
        "        labels = torch.zeros(self.batch_size * 2).long().cuda()\n",
        "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
        "\n",
        "        loss = self.criterion(logits, labels)\n",
        "        loss /= 2 * self.batch_size\n",
        "        \n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "NHh1cAyERVXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super(Lambda, self).__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "class ReCon32(nn.Module):\n",
        "    def __init__(self, indim=512):\n",
        "        super(ReCon32, self).__init__()\n",
        "        self.indim = indim\n",
        "        self.recon = nn.Sequential(\n",
        "            nn.Linear(indim, 16*64, bias=False),\n",
        "            nn.BatchNorm1d(num_features=16*64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Lambda(lambda x: x.reshape(-1, 64, 4, 4)),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, output_padding=0, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.recon(x)\n",
        "        return out\n",
        "\n",
        "class ReCon64(nn.Module):\n",
        "    def __init__(self, indim=512):\n",
        "        super(ReCon64, self).__init__()\n",
        "        self.indim = indim\n",
        "        self.recon = nn.Sequential(\n",
        "            nn.Linear(indim, 16*128, bias=False),\n",
        "            nn.BatchNorm1d(num_features=16*128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Lambda(lambda x: x.reshape(-1, 128, 4, 4)),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=3, stride=1, padding=1, output_padding=0, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.recon(x)\n",
        "        return out\n",
        "\n",
        "class ReCon224(nn.Module):\n",
        "    def __init__(self, indim=2048):\n",
        "        super(ReCon224, self).__init__()\n",
        "        self.indim = indim\n",
        "        self.recon = nn.Sequential(\n",
        "            nn.Linear(indim, 49*64, bias=False),\n",
        "            nn.BatchNorm1d(num_features=49*64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Lambda(lambda x: x.reshape(-1, 64, 7, 7)),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.recon(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "2f9RzA3_Rant"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        if self.downsample: \n",
        "            self.ds_conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
        "            self.ds_bn1 = nn.BatchNorm2d(planes)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.ds_bn1(self.ds_conv1(x))\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        if self.downsample:\n",
        "            self.ds_conv1 = nn.Conv2d(inplanes, planes * self.expansion, kernel_size=1, stride=stride, bias=False)\n",
        "            self.ds_bn1 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample:\n",
        "            residual = self.ds_bn1(self.ds_conv1(x))\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet_imgnet(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        super(ResNet_imgnet, self).__init__()\n",
        "        self.inplanes = 64       \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)            \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.feat_dim = 512 * block.expansion\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = False\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = True\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample=downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)    \n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        layer1 = self.layer1(x)\n",
        "        layer2 = self.layer2(layer1)\n",
        "        out2 = self.avgpool(layer2).squeeze()\n",
        "        layer3 = self.layer3(layer2)\n",
        "        out3 = self.avgpool(layer3).squeeze()\n",
        "        layer4 = self.layer4(layer3)\n",
        "        out4 = self.avgpool(layer4).squeeze()\n",
        "        return out2, out3, out4\n",
        "\n",
        "\n",
        "def resnet18_imagenet():\n",
        "    return ResNet_imgnet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def resnet34_imagenet():\n",
        "    return ResNet_imgnet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "def resnet50_imagenet():\n",
        "    return ResNet_imgnet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "def resnet101_imagenet():\n",
        "    return ResNet_imgnet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "def resnet152_imagenet():\n",
        "    return ResNet_imgnet(Bottleneck, [3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "XRtngzpHRbUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def __init__(self, power=2):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.power = power\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1./self.power)\n",
        "        out = x.div(norm)\n",
        "        return out\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        self.shortcut_bn = None\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
        "            self.shortcut_bn = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.shortcut_bn:\n",
        "            out += self.shortcut_bn(self.shortcut(x))\n",
        "        else:\n",
        "            out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        self.shortcut_bn = None\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False))\n",
        "            self.shortcut_bn = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.shortcut_bn:\n",
        "            out += self.shortcut_bn(self.shortcut(x))\n",
        "        else:\n",
        "            out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, data='CIFAR10'):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        if data == 'STL-10':\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        else:  # CIFAR10\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.feat_dim = 512*block.expansion\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        layer1 = self.layer1(out)\n",
        "        layer2 = self.layer2(layer1)\n",
        "        out2 = self.avgpool(layer2).squeeze()\n",
        "        layer3 = self.layer3(layer2)\n",
        "        out3 = self.avgpool(layer3).squeeze()\n",
        "        layer4 = self.layer4(layer3)\n",
        "        out4 = self.avgpool(layer4).squeeze()\n",
        "        return out2, out3, out4\n",
        "\n",
        "\n",
        "def resnet18(data='CIFAR10'):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], data=data)\n",
        "\n",
        "def resnet34(data='CIFAR10'):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], data=data)\n",
        "\n",
        "def resnet50(data='CIFAR10'):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], data=data)\n",
        "\n",
        "def resnet101(data='CIFAR10'):\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3], data=data)\n",
        "\n",
        "def resnet152(data='CIFAR10'):\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3], data=data)"
      ],
      "metadata": {
        "id": "r3blNMpnRj0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "class SimCLR(nn.Module):\n",
        "    \"\"\"\n",
        "    We opt for simplicity and adopt the commonly used ResNet to obtain hi = f(xi) = ResNet(xi) where hi is the output after the average pooling layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, args, data='non_imagenet'):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.args = args\n",
        "        if data == 'imagenet':\n",
        "            self.encoder = self.get_imagenet_resnet(args.resnet)\n",
        "        else:\n",
        "            self.encoder = self.get_resnet(args.resnet)\n",
        "\n",
        "        self.n_features = self.encoder.feat_dim\n",
        "        self.projector = nn.Sequential(nn.Linear(self.n_features, self.n_features),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Linear(self.n_features, args.projection_dim))\n",
        "\n",
        "    def get_resnet(self, name):\n",
        "        resnets = {\n",
        "            \"resnet18\": resnet18(data=self.args.dataset),\n",
        "            \"resnet34\": resnet34(data=self.args.dataset),\n",
        "            \"resnet50\": resnet50(data=self.args.dataset),\n",
        "            \"resnet101\": resnet101(data=self.args.dataset),\n",
        "            \"resnet152\": resnet152(data=self.args.dataset)}\n",
        "        if name not in resnets.keys():\n",
        "            raise KeyError(f\"{name} is not a valid ResNet version\")\n",
        "        return resnets[name]\n",
        "     \n",
        "    def get_imagenet_resnet(self, name):\n",
        "        resnets = {\n",
        "            \"resnet18\": resnet18_imagenet(),\n",
        "            \"resnet34\": resnet34_imagenet(),\n",
        "            \"resnet50\": resnet50_imagenet(),\n",
        "            \"resnet101\": resnet101_imagenet(),\n",
        "            \"resnet152\": resnet152_imagenet()}\n",
        "        if name not in resnets.keys():\n",
        "            raise KeyError(f\"{name} is not a valid ResNet version\")\n",
        "        return resnets[name]\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.args.model == 'LBE':\n",
        "            mu2, mu3, mu4 = self.encoder(x)\n",
        "            esp2 = mu2.data.new(mu2.size()).normal_(0., self.args.zeta)\n",
        "            h2 = mu2 + esp2\n",
        "            esp3 = mu3.data.new(mu3.size()).normal_(0., self.args.zeta)\n",
        "            h3 = mu3 + esp3\n",
        "            esp4 = mu4.data.new(mu4.size()).normal_(0., self.args.zeta)\n",
        "            h4 = mu4 + esp4\n",
        "            z = self.projector(h4)\n",
        "            if self.args.normalize:\n",
        "                z = nn.functional.normalize(z, dim=1)\n",
        "            out = (mu2, mu3, mu4, h2, h3, h4, z)\n",
        "        elif self.args.model == 'MIB':\n",
        "            _, _, mu = self.encoder(x)\n",
        "            esp = mu.data.new(mu.size()).normal_(0., self.args.zeta)\n",
        "            h = mu + esp\n",
        "            z = self.projector(h)\n",
        "            if self.args.normalize:\n",
        "                z = nn.functional.normalize(z, dim=1)\n",
        "            out = (mu, h, z)\n",
        "        else:\n",
        "            _, _, h = self.encoder(x)\n",
        "            z = self.projector(h)\n",
        "            if self.args.normalize:\n",
        "                z = nn.functional.normalize(z, dim=1)\n",
        "            out = (h, z)\n",
        "        return out"
      ],
      "metadata": {
        "id": "etA_7ZUtRmRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "class Transforms:\n",
        "    \"\"\"\n",
        "    A stochastic data augmentation module that transforms any given data example randomly \n",
        "    resulting in two correlated views of the same example,\n",
        "    denoted xi and xj, which we consider as a positive pair.\n",
        "    \"\"\"\n",
        "    def __init__(self, size=32):\n",
        "        s = 1\n",
        "        color_jitter = torchvision.transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        normalize = torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        self.train_transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.RandomResizedCrop(size=size),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
        "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                normalize])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.train_transform(x), self.train_transform(x)\n",
        "\n",
        "class Transforms_imagenet:\n",
        "    \"\"\"\n",
        "    A stochastic data augmentation module that transforms any given data example randomly \n",
        "    resulting in two correlated views of the same example,\n",
        "    denoted xi and xj, which we consider as a positive pair.\n",
        "    \"\"\"\n",
        "    def __init__(self, size=224):\n",
        "        s = 1\n",
        "        color_jitter = torchvision.transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "        normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        self.train_transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.RandomResizedCrop(size=size),\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.RandomApply([color_jitter], p=0.8),\n",
        "                torchvision.transforms.RandomGrayscale(p=0.2),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                normalize])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.train_transform(x), self.train_transform(x)"
      ],
      "metadata": {
        "id": "mmsl6ogGRx7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_mean_and_stdev(dataset):\n",
        "    if dataset == 'CIFAR10' or dataset == 'CIFAR100':\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std  = [0.5, 0.5, 0.5]\n",
        "    elif dataset == 'STL-10':\n",
        "        mean = [0.491, 0.482, 0.447]\n",
        "        std  = [0.247, 0.244, 0.262]\n",
        "    elif dataset == 'ImageNet':\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    elif dataset == 'aircraft':\n",
        "        mean = [0.486, 0.507, 0.525]\n",
        "        std  = [0.266, 0.260, 0.276]\n",
        "    elif dataset == 'cu_birds':\n",
        "        mean = [0.483, 0.491, 0.424] \n",
        "        std  = [0.228, 0.224, 0.259]\n",
        "    elif dataset == 'dtd':\n",
        "        mean = [0.533, 0.474, 0.426]\n",
        "        std  = [0.261, 0.250, 0.259]\n",
        "    elif dataset == 'fashionmnist':\n",
        "        mean = [0.348, 0.348, 0.348] \n",
        "        std  = [0.347, 0.347, 0.347]\n",
        "    elif dataset == 'mnist':\n",
        "        mean = [0.170, 0.170, 0.170]\n",
        "        std  = [0.320, 0.320, 0.320]\n",
        "    elif dataset == 'traffic_sign':\n",
        "        mean = [0.335, 0.291, 0.295]\n",
        "        std  = [0.267, 0.249, 0.251]\n",
        "    elif dataset == 'vgg_flower':\n",
        "        mean = [0.518, 0.410, 0.329]\n",
        "        std  = [0.296, 0.249, 0.285]\n",
        "    else:\n",
        "        raise Exception('Dataset %s not supported.'%dataset)\n",
        "    return mean, std\n",
        "\n",
        "def get_data_nclass(dataset):\n",
        "    if dataset == 'CIFAR10':\n",
        "        nclass = 10\n",
        "    elif dataset == 'CIFAR100':\n",
        "        nclass = 100\n",
        "    elif dataset == 'STL-10':\n",
        "        nclass = 10\n",
        "    elif dataset == 'ImageNet':\n",
        "        nclass = 1000\n",
        "    elif dataset == 'aircraft':\n",
        "        nclass = 102\n",
        "    elif dataset == 'cu_birds':\n",
        "        nclass = 200\n",
        "    elif dataset == 'dtd':\n",
        "        nclass = 47\n",
        "    elif dataset == 'fashionmnist':\n",
        "        nclass = 10\n",
        "    elif dataset == 'mnist':\n",
        "        nclass = 10\n",
        "    elif dataset == 'traffic_sign':\n",
        "        nclass = 43\n",
        "    elif dataset == 'vgg_flower':\n",
        "        nclass = 102\n",
        "    else:\n",
        "        raise Exception('Dataset %s not supported.'%dataset)\n",
        "    return nclass"
      ],
      "metadata": {
        "id": "p1UmpgagR2FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"datasets\")"
      ],
      "metadata": {
        "id": "QtLZU9ueR7im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from collections import defaultdict\n",
        "import torch.utils.data as data\n",
        "\n",
        "DATA_ROOTS = 'datasets/Aircraft'\n",
        "\n",
        "class Aircraft(data.Dataset):\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.image_transforms = image_transforms\n",
        "        paths, bboxes, labels = self.load_images()\n",
        "        self.paths = paths\n",
        "        self.bboxes = bboxes\n",
        "        self.labels = labels\n",
        "\n",
        "    def load_images(self):\n",
        "        split = 'trainval' if self.train else 'test'\n",
        "        variant_path = os.path.join(self.root, 'data', 'images_variant_%s.txt'%split)\n",
        "        with open(variant_path, 'r') as f:\n",
        "            names_to_variants = [line.split('\\n')[0].split(' ', 1) for line in f.readlines()]\n",
        "        names_to_variants = dict(names_to_variants)\n",
        "        variants_to_names = defaultdict(list)\n",
        "        for name, variant in names_to_variants.items():\n",
        "            variants_to_names[variant].append(name)\n",
        "        variants = sorted(list(set(variants_to_names.keys())))\n",
        "\n",
        "        names_to_bboxes = self.get_bounding_boxes()\n",
        "        split_files, split_labels, split_bboxes = [], [], []\n",
        "        for variant_id, variant in enumerate(variants):\n",
        "            class_files = [join(self.root, 'data', 'images', '%s.jpg'%filename) for filename in sorted(variants_to_names[variant])]\n",
        "            bboxes = [names_to_bboxes[name] for name in sorted(variants_to_names[variant])]\n",
        "            labels = list([variant_id] * len(class_files))\n",
        "            split_files += class_files\n",
        "            split_labels += labels\n",
        "            split_bboxes += bboxes\n",
        "        return split_files, split_bboxes, split_labels\n",
        "\n",
        "    def get_bounding_boxes(self):\n",
        "        bboxes_path = os.path.join(self.root, 'data', 'images_box.txt')\n",
        "        with open(bboxes_path, 'r') as f:\n",
        "            names_to_bboxes = [line.split('\\n')[0].split(' ') for line in f.readlines()]\n",
        "            names_to_bboxes = dict((name, list(map(int, (xmin, ymin, xmax, ymax)))) for name, xmin, ymin, xmax, ymax in names_to_bboxes)\n",
        "        return names_to_bboxes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        bbox = tuple(self.bboxes[index])\n",
        "        label = self.labels[index]\n",
        "\n",
        "        image = Image.open(path).convert(mode='RGB')\n",
        "        image = image.crop(bbox)\n",
        "\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "06lBduokSm1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.utils.data as data\n",
        "\n",
        "DATA_ROOTS = 'datasets/CUBirds'\n",
        "\n",
        "class CUBirds(data.Dataset):\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.image_transforms = image_transforms\n",
        "        paths, labels = self.load_images()\n",
        "        self.paths, self.labels = paths, labels\n",
        "\n",
        "    def load_images(self):\n",
        "        image_info_path = os.path.join(self.root, 'images.txt')\n",
        "        with open(image_info_path, 'r') as f:\n",
        "            image_info = [line.split('\\n')[0].split(' ', 1) for line in f.readlines()]\n",
        "        image_info = dict(image_info)\n",
        "\n",
        "        # load image to label information\n",
        "        label_info_path = os.path.join(self.root, 'image_class_labels.txt')\n",
        "        with open(label_info_path, 'r') as f:\n",
        "            label_info = [line.split('\\n')[0].split(' ', 1) for line in f.readlines()]\n",
        "        label_info = dict(label_info)\n",
        "\n",
        "        # load train test split\n",
        "        train_test_info_path = os.path.join(self.root, 'train_test_split.txt')\n",
        "        with open(train_test_info_path, 'r') as f:\n",
        "            train_test_info = [line.split('\\n')[0].split(' ', 1) for line in f.readlines()]\n",
        "        train_test_info = dict(train_test_info)\n",
        "\n",
        "        all_paths, all_labels = [], []\n",
        "        for index, image_path in image_info.items():\n",
        "            label = label_info[index]\n",
        "            split = int(train_test_info[index])\n",
        "            if self.train:\n",
        "                if split == 1:\n",
        "                    all_paths.append(image_path)\n",
        "                    all_labels.append(label)\n",
        "            else:\n",
        "                if split == 0:\n",
        "                    all_paths.append(image_path)\n",
        "                    all_labels.append(label)\n",
        "        return all_paths, all_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = os.path.join(self.root, 'images', self.paths[index])\n",
        "        label = int(self.labels[index]) - 1\n",
        "        image = Image.open(path).convert(mode='RGB')\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "SRn6PLI3b_So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from itertools import chain\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOTS = 'datasets/DTD'\n",
        "\n",
        "class DTD(data.Dataset):\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.image_transforms = image_transforms\n",
        "        paths, labels = self.load_images()\n",
        "        self.paths, self.labels = paths, labels\n",
        "\n",
        "    def load_images(self):\n",
        "        if self.train:\n",
        "            train_info_path = os.path.join(self.root, 'labels', 'train1.txt')\n",
        "            with open(train_info_path, 'r') as f:\n",
        "                train_info = [line.split('\\n')[0] for line in f.readlines()]\n",
        "\n",
        "            val_info_path = os.path.join(self.root, 'labels', 'val1.txt')\n",
        "            with open(val_info_path, 'r') as f:\n",
        "                val_info = [line.split('\\n')[0] for line in f.readlines()]\n",
        "            split_info = train_info + val_info\n",
        "\n",
        "        else:\n",
        "            test_info_path = os.path.join(self.root, 'labels', 'test1.txt')\n",
        "            with open(test_info_path, 'r') as f:\n",
        "                split_info = [line.split('\\n')[0] for line in f.readlines()]\n",
        "\n",
        "        # pull out categoires from paths\n",
        "        categories = []\n",
        "        for row in split_info:\n",
        "            image_path = row\n",
        "            category = image_path.split('/')[0]\n",
        "            categories.append(category)\n",
        "        categories = sorted(list(set(categories)))\n",
        "\n",
        "        all_paths, all_labels = [], []\n",
        "        for row in split_info:\n",
        "            image_path = row\n",
        "            category = image_path.split('/')[0]\n",
        "            label = categories.index(category)\n",
        "            all_paths.append(join(self.root, 'images', image_path))\n",
        "            all_labels.append(label)\n",
        "        return all_paths, all_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        label = self.labels[index]\n",
        "        image = Image.open(path).convert(mode='RGB')\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "P2pInj8LcEY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "DATA_ROOTS = 'datasets'\n",
        "\n",
        "class FashionMNIST(data.Dataset):\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        if not os.path.isdir(root):\n",
        "            os.makedirs(root)\n",
        "        self.image_transforms = image_transforms\n",
        "        self.dataset = datasets.mnist.FashionMNIST(root, train=train, download=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.dataset.data[index], int(self.dataset.targets[index])\n",
        "        img = Image.fromarray(img.numpy(), mode='L').convert('RGB')\n",
        "        if self.image_transforms is not None:\n",
        "            img = self.image_transforms(img)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "JV-il4h8cIhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "DATA_ROOTS = 'datasets'\n",
        "\n",
        "class MNIST(data.Dataset):\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        if not os.path.isdir(root):\n",
        "            os.makedirs(root)\n",
        "        self.image_transforms = image_transforms\n",
        "        self.dataset = datasets.mnist.MNIST(root, train=train, download=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.dataset.data[index], int(self.dataset.targets[index])\n",
        "        img = Image.fromarray(img.numpy(), mode='L').convert('RGB')\n",
        "        if self.image_transforms is not None:\n",
        "            img = self.image_transforms(img)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "N2YjnY2LcP4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import operator\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from os.path import join\n",
        "from itertools import chain\n",
        "from scipy.io import loadmat\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOTS = 'datasets/TrafficSign'\n",
        "\n",
        "class TrafficSign(data.Dataset):\n",
        "    NUM_CLASSES = 43\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.image_transforms = image_transforms\n",
        "        paths, labels = self.load_images()\n",
        "        self.paths, self.labels = paths, labels\n",
        "\n",
        "    def load_images(self):\n",
        "        split = 'Final_Training'\n",
        "        rs = np.random.RandomState(42)\n",
        "        all_filepaths, all_labels = [], []\n",
        "        for class_i in range(self.NUM_CLASSES):\n",
        "            class_dir_i = join(self.root, split, 'Images', '{:05d}'.format(class_i))\n",
        "            image_paths = glob(join(class_dir_i, \"*.ppm\"))\n",
        "            # train test splitting\n",
        "            image_paths = np.array(image_paths)\n",
        "            num = len(image_paths)\n",
        "            indexer = np.arange(num)\n",
        "            rs.shuffle(indexer)\n",
        "            image_paths = image_paths[indexer].tolist()\n",
        "            if self.train:\n",
        "                image_paths = image_paths[:int(0.8 * num)]\n",
        "            else:\n",
        "                image_paths  = image_paths[int(0.8 * num):]\n",
        "            labels = [class_i] * len(image_paths)\n",
        "            all_filepaths.extend(image_paths)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "        return all_filepaths, all_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        label = self.labels[index]\n",
        "        image = Image.open(path).convert(mode='RGB')\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "gKYiK3ZpcWvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import operator\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from os.path import join\n",
        "from itertools import chain\n",
        "from scipy.io import loadmat\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOTS = 'datasets/VGGFlower'\n",
        "\n",
        "class VGGFlower(data.Dataset):\n",
        "    def __init__(self, root=DATA_ROOTS, train=True, image_transforms=None):\n",
        "        super().__init__()\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.image_transforms = image_transforms\n",
        "        paths, labels = self.load_images()\n",
        "        self.paths, self.labels = paths, labels\n",
        "\n",
        "    def load_images(self):\n",
        "        rs = np.random.RandomState(42)\n",
        "        imagelabels_path = os.path.join(self.root, 'imagelabels.mat')\n",
        "        with open(imagelabels_path, 'rb') as f:\n",
        "            labels = loadmat(f)['labels'][0]\n",
        "\n",
        "        all_filepaths = defaultdict(list)\n",
        "        for i, label in enumerate(labels):\n",
        "            all_filepaths[label].append(os.path.join(self.root, 'jpg', 'image_{:05d}.jpg'.format(i+1)))\n",
        "        # train test split\n",
        "        split_filepaths, split_labels = [], []\n",
        "        for label, paths in all_filepaths.items():\n",
        "            num = len(paths)\n",
        "            paths = np.array(paths)\n",
        "            indexer = np.arange(num)\n",
        "            rs.shuffle(indexer)\n",
        "            paths = paths[indexer].tolist()\n",
        "\n",
        "            if self.train:\n",
        "                paths = paths[:int(0.8 * num)]\n",
        "            else:\n",
        "                paths = paths[int(0.8 * num):]\n",
        "\n",
        "            labels = [label] * len(paths)\n",
        "            split_filepaths.extend(paths)\n",
        "            split_labels.extend(labels)\n",
        "        \n",
        "        return split_filepaths, split_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        label = int(self.labels[index]) - 1\n",
        "        image = Image.open(path).convert(mode='RGB')\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "E2mFJRercbS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = {\n",
        "    'aircraft': Aircraft,\n",
        "    'cu_birds': CUBirds,\n",
        "    'dtd': DTD,\n",
        "    'fashionmnist': FashionMNIST,\n",
        "    'mnist': MNIST,\n",
        "    'traffic_sign': TrafficSign,\n",
        "    'vgg_flower': VGGFlower}"
      ],
      "metadata": {
        "id": "QzDJfCsgcgky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_mean_and_stdev(dataset):\n",
        "    if dataset == 'CIFAR10' or dataset == 'CIFAR100':\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std  = [0.5, 0.5, 0.5]\n",
        "    elif dataset == 'STL-10':\n",
        "        mean = [0.491, 0.482, 0.447]\n",
        "        std  = [0.247, 0.244, 0.262]\n",
        "    elif dataset == 'ImageNet':\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    elif dataset == 'aircraft':\n",
        "        mean = [0.486, 0.507, 0.525]\n",
        "        std  = [0.266, 0.260, 0.276]\n",
        "    elif dataset == 'cu_birds':\n",
        "        mean = [0.483, 0.491, 0.424] \n",
        "        std  = [0.228, 0.224, 0.259]\n",
        "    elif dataset == 'dtd':\n",
        "        mean = [0.533, 0.474, 0.426]\n",
        "        std  = [0.261, 0.250, 0.259]\n",
        "    elif dataset == 'fashionmnist':\n",
        "        mean = [0.348, 0.348, 0.348] \n",
        "        std  = [0.347, 0.347, 0.347]\n",
        "    elif dataset == 'mnist':\n",
        "        mean = [0.170, 0.170, 0.170]\n",
        "        std  = [0.320, 0.320, 0.320]\n",
        "    elif dataset == 'traffic_sign':\n",
        "        mean = [0.335, 0.291, 0.295]\n",
        "        std  = [0.267, 0.249, 0.251]\n",
        "    elif dataset == 'vgg_flower':\n",
        "        mean = [0.518, 0.410, 0.329]\n",
        "        std  = [0.296, 0.249, 0.285]\n",
        "    else:\n",
        "        raise Exception('Dataset %s not supported.'%dataset)\n",
        "    return mean, std\n",
        "\n",
        "def get_data_nclass(dataset):\n",
        "    if dataset == 'CIFAR10':\n",
        "        nclass = 10\n",
        "    elif dataset == 'CIFAR100':\n",
        "        nclass = 100\n",
        "    elif dataset == 'STL-10':\n",
        "        nclass = 10\n",
        "    elif dataset == 'ImageNet':\n",
        "        nclass = 1000\n",
        "    elif dataset == 'aircraft':\n",
        "        nclass = 102\n",
        "    elif dataset == 'cu_birds':\n",
        "        nclass = 200\n",
        "    elif dataset == 'dtd':\n",
        "        nclass = 47\n",
        "    elif dataset == 'fashionmnist':\n",
        "        nclass = 10\n",
        "    elif dataset == 'mnist':\n",
        "        nclass = 10\n",
        "    elif dataset == 'traffic_sign':\n",
        "        nclass = 43\n",
        "    elif dataset == 'vgg_flower':\n",
        "        nclass = 102\n",
        "    else:\n",
        "        raise Exception('Dataset %s not supported.'%dataset)\n",
        "    return nclass"
      ],
      "metadata": {
        "id": "kZXJiKOdcm_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def mask_correlated_samples(batch_size):\n",
        "    mask = torch.ones((batch_size*2, batch_size*2)).bool()\n",
        "    mask = mask.fill_diagonal_(0)\n",
        "    for i in range(batch_size):\n",
        "        mask[i, batch_size+i] = 0\n",
        "        mask[batch_size+i, i] = 0\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "-Hw48Yi-cqKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_model(args, reload_model=False, load_path=None, data='non_imagenet'):\n",
        "    model = SimCLR(args, data=data)\n",
        "    if reload_model:\n",
        "        if os.path.isfile(load_path):\n",
        "            model_fp = os.path.join(load_path)\n",
        "        else:\n",
        "            print(\"No file to load\")\n",
        "            return\n",
        "        model.load_state_dict(torch.load(model_fp, map_location=lambda storage, loc: storage))\n",
        "    model = model.cuda()\n",
        "\n",
        "    if args.model == 'RC':\n",
        "        if args.dataset == 'STL-10':\n",
        "            recon = ReCon64(512).cuda()\n",
        "        elif args.dataset == 'ImageNet':\n",
        "            recon = ReCon224(2048).cuda()\n",
        "        else:\n",
        "            recon = ReCon32(512).cuda()\n",
        "        params = [{'params': model.parameters()}, {'params': recon.parameters()}]\n",
        "    else:\n",
        "        recon = None\n",
        "        params = model.parameters()\n",
        "\n",
        "    scheduler = None\n",
        "    if args.optimizer == \"Adam\":\n",
        "        optimizer = torch.optim.Adam(params, lr=args.lr)\n",
        "    elif args.optimizer == \"LARS\":\n",
        "        # LearningRate=(0.3×BatchSize/256) and weight decay of 10−6.\n",
        "        learning_rate = 0.3 * args.batch_size / 256\n",
        "        optimizer = LARS(params, lr=learning_rate, weight_decay=args.weight_decay, exclude_from_weight_decay=[\"batch_normalization\", \"bias\"])\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, eta_min=0, last_epoch=-1)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return model, recon, optimizer, scheduler\n",
        "\n",
        "\n",
        "def save_model(model_dir, model, epoch):\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        torch.save(model.module.state_dict(), model_dir + '_epoch_{}.pt'.format(epoch))\n",
        "    else:\n",
        "        torch.save(model.state_dict(), model_dir + '_epoch_{}.pt'.format(epoch))"
      ],
      "metadata": {
        "id": "3UP3pI5odIqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.argv=['']\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='SimCLR')\n",
        "parser.add_argument('--model', default=\"orig\", type=str, help=\"orig/RC/LBE/IP/MIB\")\n",
        "parser.add_argument('--batch_size', default=256, type=int, metavar='B', help='training batch size')\n",
        "parser.add_argument('--workers', default=2, type=int, help='workers')\n",
        "parser.add_argument('--epochs', default=30, type=int, help='epochs')\n",
        "parser.add_argument('--save_freq', default=20, type=int, help='save frequency')\n",
        "parser.add_argument('--resnet', default=\"resnet18\", type=str, help=\"resnet18/resnet34/resnet50/resnet101/resnet152\")\n",
        "parser.add_argument('--normalize', default=True, action='store_true', help='normalize')\n",
        "parser.add_argument('--projection_dim', default=128, type=int, help='projection_dim')\n",
        "parser.add_argument('--lamb', default=1., type=float, help='weight of regularization term')\n",
        "parser.add_argument('--zeta', default=0.1, type=float, help='variance')\n",
        "parser.add_argument('--optimizer', default=\"Adam\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--lr', default=3e-4, type=float, help='lr')\n",
        "parser.add_argument('--weight_decay', default=1e-6, type=float, help='weight_decay')\n",
        "parser.add_argument('--temperature', default=0.5, type=float, help='temperature')\n",
        "parser.add_argument('--gpus', default=8, type=int, help='number of gpu')\n",
        "parser.add_argument('--model_dir', default='output/checkpoint/', type=str, help='model save path')\n",
        "parser.add_argument('--dataset', default='CIFAR10', help='[CIFAR10, CIFAR100, ImageNet, STL-10]')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def train(train_loader, model, recon, criterion, optimizer):\n",
        "    loss_epoch = 0.\n",
        "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "        x_i, x_j = x_i.cuda(), x_j.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        if args.model == 'orig':\n",
        "            _, z_i = model(x_i)\n",
        "            _, z_j = model(x_j)\n",
        "            loss = criterion(z_i, z_j)\n",
        "        elif args.model == 'RC':\n",
        "            h_i, z_i = model(x_i)\n",
        "            h_j, z_j = model(x_j)\n",
        "            recon_loss = F.mse_loss(recon(h_i), x_i) + F.mse_loss(recon(h_j), x_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * recon_loss\n",
        "        elif args.model == 'LBE':\n",
        "            mu2_i, mu3_i, mu4_i, h2_i, h3_i, h4_i, z_i = model(x_i)\n",
        "            mu2_j, mu3_j, mu4_j, h2_j, h3_j, h4_j, z_j = model(x_j)\n",
        "            mu2, h2 = torch.cat([mu2_i, mu2_j], dim=0), torch.cat([h2_i, h2_j], dim=0)\n",
        "            mu3, h3 = torch.cat([mu3_i, mu3_j], dim=0), torch.cat([h3_i, h3_j], dim=0)\n",
        "            mu4, h4 = torch.cat([mu4_i, mu4_j], dim=0), torch.cat([h4_i, h4_j], dim=0)\n",
        "            if args.dataset == \"ImageNet\":\n",
        "                MI_estimitor = InfoNCE(mu4, h4)\n",
        "            else:\n",
        "                MI_estimitor = 0.25 * InfoNCE(mu2, h2) + 0.50 * InfoNCE(mu3, h3) + InfoNCE(mu4, h4)\n",
        "            loss = criterion(z_i, z_j) - args.lamb * MI_estimitor\n",
        "        elif args.model == 'IP':\n",
        "            h_i, z_i = model(x_i)\n",
        "            h_j, z_j = model(x_j)\n",
        "            IP = F.mse_loss(h_i, h_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * IP\n",
        "        elif args.model == 'MIB':\n",
        "            mu_i, _, z_i = model(x_i)\n",
        "            mu_j, _, z_j = model(x_j)\n",
        "            MIB = F.mse_loss(mu_i, mu_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * MIB\n",
        "        else:\n",
        "            assert False\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
        "        loss_epoch += loss.item()\n",
        "    return loss_epoch\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = 'non_imagenet'\n",
        "    root = \"datasets\"\n",
        "    if args.dataset == \"CIFAR10\":\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root, download=True, transform=Transforms(32))\n",
        "    elif args.dataset == \"CIFAR100\":\n",
        "        train_dataset = torchvision.datasets.CIFAR100(root, download=True, transform=Transforms(32))\n",
        "    elif args.dataset == \"STL-10\":\n",
        "        train_dataset = torchvision.datasets.STL10(root, split='unlabeled', download=True, transform=Transforms(64))\n",
        "    elif args.dataset == \"ImageNet\":\n",
        "        traindir = os.path.join(root, 'ImageNet/train')\n",
        "        train_dataset = torchvision.datasets.ImageFolder(traindir, Transforms_imagenet(size=224))\n",
        "        data = 'imagenet'\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.workers,\n",
        "        sampler=None)\n",
        "\n",
        "    log_dir = \"output/log/\" + args.dataset + '_%s/'%args.model\n",
        "    if not os.path.isdir(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    suffix = args.dataset + '_{}_batch_{}'.format(args.resnet, args.batch_size)\n",
        "    suffix = suffix + '_proj_dim_{}'.format(args.projection_dim)\n",
        "    test_log_file = open(log_dir + suffix + '.txt', \"w\")\n",
        "\n",
        "    model, recon, optimizer, scheduler = load_model(args, data=data)\n",
        "    if args.dataset=='ImageNet':\n",
        "        model = torch.nn.DataParallel(model, device_ids=list(range(args.gpus)))\n",
        "    args.model_dir = args.model_dir + args.dataset + '_%s/'%args.model\n",
        "    if not os.path.isdir(args.model_dir):\n",
        "        os.makedirs(args.model_dir)\n",
        "            \n",
        "    mask = mask_correlated_samples(args.batch_size)\n",
        "    criterion = NT_Xent(args.batch_size, args.temperature, mask)\n",
        "    for epoch in range(args.epochs):\n",
        "        loss_epoch = train(train_loader, model, recon, criterion, optimizer)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        if (epoch+1) % args.save_freq == 0:\n",
        "            save_model(args.model_dir+suffix, model, epoch+1)\n",
        "\n",
        "        print('Epoch {} loss: {}\\n'.format(epoch, loss_epoch / len(train_loader)))\n",
        "        print('Epoch {} loss: {}'.format(epoch, loss_epoch/len(train_loader)), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "\n",
        "    save_model(args.model_dir+suffix, model, args.epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkpD-AwndOgZ",
        "outputId": "7cecb074-4c17-4ec2-f7c1-f1c3a7e9ffb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:06<00:00, 26412363.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/cifar-10-python.tar.gz to datasets\n",
            "Step [0/195]\t Loss: 6.231577396392822\n",
            "Step [50/195]\t Loss: 5.892642021179199\n",
            "Step [100/195]\t Loss: 5.7658305168151855\n",
            "Step [150/195]\t Loss: 5.654880046844482\n",
            "Epoch 0 loss: 5.797138889019306\n",
            "\n",
            "Step [0/195]\t Loss: 5.59588623046875\n",
            "Step [50/195]\t Loss: 5.510222434997559\n",
            "Step [100/195]\t Loss: 5.545424461364746\n",
            "Step [150/195]\t Loss: 5.386889457702637\n",
            "Epoch 1 loss: 5.484603649530655\n",
            "\n",
            "Step [0/195]\t Loss: 5.398613452911377\n",
            "Step [50/195]\t Loss: 5.3236870765686035\n",
            "Step [100/195]\t Loss: 5.47713565826416\n",
            "Step [150/195]\t Loss: 5.31764554977417\n",
            "Epoch 2 loss: 5.326670230963291\n",
            "\n",
            "Step [0/195]\t Loss: 5.322944641113281\n",
            "Step [50/195]\t Loss: 5.213741302490234\n",
            "Step [100/195]\t Loss: 5.293124675750732\n",
            "Step [150/195]\t Loss: 5.1909685134887695\n",
            "Epoch 3 loss: 5.223193144186949\n",
            "\n",
            "Step [0/195]\t Loss: 5.169441223144531\n",
            "Step [50/195]\t Loss: 5.137589931488037\n",
            "Step [100/195]\t Loss: 5.121384620666504\n",
            "Step [150/195]\t Loss: 5.148230075836182\n",
            "Epoch 4 loss: 5.156870937347412\n",
            "\n",
            "Step [0/195]\t Loss: 5.150076389312744\n",
            "Step [50/195]\t Loss: 5.163863182067871\n",
            "Step [100/195]\t Loss: 5.1577534675598145\n",
            "Step [150/195]\t Loss: 5.1641716957092285\n",
            "Epoch 5 loss: 5.109294336270064\n",
            "\n",
            "Step [0/195]\t Loss: 5.105855464935303\n",
            "Step [50/195]\t Loss: 5.050668239593506\n",
            "Step [100/195]\t Loss: 5.094457149505615\n",
            "Step [150/195]\t Loss: 5.069894313812256\n",
            "Epoch 6 loss: 5.071587983155862\n",
            "\n",
            "Step [0/195]\t Loss: 5.098202705383301\n",
            "Step [50/195]\t Loss: 5.021291732788086\n",
            "Step [100/195]\t Loss: 5.039153099060059\n",
            "Step [150/195]\t Loss: 5.05411434173584\n",
            "Epoch 7 loss: 5.049821726481119\n",
            "\n",
            "Step [0/195]\t Loss: 5.022922515869141\n",
            "Step [50/195]\t Loss: 5.045216083526611\n",
            "Step [100/195]\t Loss: 4.955245494842529\n",
            "Step [150/195]\t Loss: 5.0284037590026855\n",
            "Epoch 8 loss: 5.022656242664044\n",
            "\n",
            "Step [0/195]\t Loss: 5.033908367156982\n",
            "Step [50/195]\t Loss: 4.993305206298828\n",
            "Step [100/195]\t Loss: 4.975376605987549\n",
            "Step [150/195]\t Loss: 4.993823051452637\n",
            "Epoch 9 loss: 5.008277736566005\n",
            "\n",
            "Step [0/195]\t Loss: 5.0056328773498535\n",
            "Step [50/195]\t Loss: 4.966790199279785\n",
            "Step [100/195]\t Loss: 5.023151397705078\n",
            "Step [150/195]\t Loss: 4.958976745605469\n",
            "Epoch 10 loss: 4.993439737955729\n",
            "\n",
            "Step [0/195]\t Loss: 5.003173351287842\n",
            "Step [50/195]\t Loss: 4.969313144683838\n",
            "Step [100/195]\t Loss: 4.993457317352295\n",
            "Step [150/195]\t Loss: 4.975901126861572\n",
            "Epoch 11 loss: 4.979922458453056\n",
            "\n",
            "Step [0/195]\t Loss: 4.948177337646484\n",
            "Step [50/195]\t Loss: 4.951280117034912\n",
            "Step [100/195]\t Loss: 4.9717278480529785\n",
            "Step [150/195]\t Loss: 4.952620983123779\n",
            "Epoch 12 loss: 4.968852302355644\n",
            "\n",
            "Step [0/195]\t Loss: 4.948939323425293\n",
            "Step [50/195]\t Loss: 4.981170654296875\n",
            "Step [100/195]\t Loss: 4.9807047843933105\n",
            "Step [150/195]\t Loss: 4.977915287017822\n",
            "Epoch 13 loss: 4.962911847921518\n",
            "\n",
            "Step [0/195]\t Loss: 4.962506294250488\n",
            "Step [50/195]\t Loss: 4.962273597717285\n",
            "Step [100/195]\t Loss: 4.979237079620361\n",
            "Step [150/195]\t Loss: 4.9696736335754395\n",
            "Epoch 14 loss: 4.949985494369115\n",
            "\n",
            "Step [0/195]\t Loss: 4.9539899826049805\n",
            "Step [50/195]\t Loss: 4.949748992919922\n",
            "Step [100/195]\t Loss: 4.910752773284912\n",
            "Step [150/195]\t Loss: 4.976195335388184\n",
            "Epoch 15 loss: 4.942127875792674\n",
            "\n",
            "Step [0/195]\t Loss: 4.98758602142334\n",
            "Step [50/195]\t Loss: 4.992812633514404\n",
            "Step [100/195]\t Loss: 4.934598445892334\n",
            "Step [150/195]\t Loss: 4.936690330505371\n",
            "Epoch 16 loss: 4.932570948967567\n",
            "\n",
            "Step [0/195]\t Loss: 4.959290504455566\n",
            "Step [50/195]\t Loss: 4.924519062042236\n",
            "Step [100/195]\t Loss: 4.911979675292969\n",
            "Step [150/195]\t Loss: 4.952922344207764\n",
            "Epoch 17 loss: 4.930655325376071\n",
            "\n",
            "Step [0/195]\t Loss: 4.856266498565674\n",
            "Step [50/195]\t Loss: 4.924047470092773\n",
            "Step [100/195]\t Loss: 4.986462116241455\n",
            "Step [150/195]\t Loss: 4.924124717712402\n",
            "Epoch 18 loss: 4.923004390031863\n",
            "\n",
            "Step [0/195]\t Loss: 4.9135565757751465\n",
            "Step [50/195]\t Loss: 4.8682861328125\n",
            "Step [100/195]\t Loss: 4.945479869842529\n",
            "Step [150/195]\t Loss: 4.884456634521484\n",
            "Epoch 19 loss: 4.915101765363644\n",
            "\n",
            "Step [0/195]\t Loss: 4.89439582824707\n",
            "Step [50/195]\t Loss: 4.917926788330078\n",
            "Step [100/195]\t Loss: 4.923543930053711\n",
            "Step [150/195]\t Loss: 4.929923057556152\n",
            "Epoch 20 loss: 4.913053507682605\n",
            "\n",
            "Step [0/195]\t Loss: 4.945533752441406\n",
            "Step [50/195]\t Loss: 4.893675804138184\n",
            "Step [100/195]\t Loss: 4.915451526641846\n",
            "Step [150/195]\t Loss: 4.921237468719482\n",
            "Epoch 21 loss: 4.904339650961069\n",
            "\n",
            "Step [0/195]\t Loss: 4.871401786804199\n",
            "Step [50/195]\t Loss: 4.9333319664001465\n",
            "Step [100/195]\t Loss: 4.832880020141602\n",
            "Step [150/195]\t Loss: 4.868557929992676\n",
            "Epoch 22 loss: 4.900300216674805\n",
            "\n",
            "Step [0/195]\t Loss: 4.877191543579102\n",
            "Step [50/195]\t Loss: 4.9071550369262695\n",
            "Step [100/195]\t Loss: 4.869978904724121\n",
            "Step [150/195]\t Loss: 4.869479656219482\n",
            "Epoch 23 loss: 4.897521280631041\n",
            "\n",
            "Step [0/195]\t Loss: 4.91506814956665\n",
            "Step [50/195]\t Loss: 4.930437088012695\n",
            "Step [100/195]\t Loss: 4.869736194610596\n",
            "Step [150/195]\t Loss: 4.909241199493408\n",
            "Epoch 24 loss: 4.895509749192458\n",
            "\n",
            "Step [0/195]\t Loss: 4.877723217010498\n",
            "Step [50/195]\t Loss: 4.865447998046875\n",
            "Step [100/195]\t Loss: 4.888064384460449\n",
            "Step [150/195]\t Loss: 4.928450584411621\n",
            "Epoch 25 loss: 4.889947695609851\n",
            "\n",
            "Step [0/195]\t Loss: 4.873283386230469\n",
            "Step [50/195]\t Loss: 4.859232425689697\n",
            "Step [100/195]\t Loss: 4.861941337585449\n",
            "Step [150/195]\t Loss: 4.876013278961182\n",
            "Epoch 26 loss: 4.883710193634033\n",
            "\n",
            "Step [0/195]\t Loss: 4.895131587982178\n",
            "Step [50/195]\t Loss: 4.878238201141357\n",
            "Step [100/195]\t Loss: 4.939142227172852\n",
            "Step [150/195]\t Loss: 4.844515800476074\n",
            "Epoch 27 loss: 4.884594215490879\n",
            "\n",
            "Step [0/195]\t Loss: 4.909432411193848\n",
            "Step [50/195]\t Loss: 4.861655235290527\n",
            "Step [100/195]\t Loss: 4.829662322998047\n",
            "Step [150/195]\t Loss: 4.8790602684021\n",
            "Epoch 28 loss: 4.875672474885598\n",
            "\n",
            "Step [0/195]\t Loss: 4.88146448135376\n",
            "Step [50/195]\t Loss: 4.836062431335449\n",
            "Step [100/195]\t Loss: 4.84157133102417\n",
            "Step [150/195]\t Loss: 4.849388122558594\n",
            "Epoch 29 loss: 4.872811131599622\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='linear Evaluation')\n",
        "parser.add_argument('--model', default=\"orig\", type=str, help=\"orig/RC/LBE/IP/MIB\")\n",
        "parser.add_argument('--logistic_batch_size', default=128, type=int, metavar='B', help='logistic_batch_size batch size')\n",
        "parser.add_argument('--logistic_epochs', default=100, type=int, help='logistic_epochs')\n",
        "parser.add_argument('--batch_size', default=256, type=int, metavar='B', help='training batch size')\n",
        "parser.add_argument('--workers', default=2, type=int, help='workers')\n",
        "parser.add_argument('--epochs', default=30, type=int, help='epochs')\n",
        "parser.add_argument('--resnet', default=\"resnet18\", type=str, help=\"resnet18/resnet34/resnet50/resnet101/resnet152\")\n",
        "parser.add_argument('--normalize', default=True, action='store_true', help='normalize')\n",
        "parser.add_argument('--projection_dim', default=128, type=int,help='projection_dim')\n",
        "parser.add_argument('--lamb', default=1., type=float, help='weight of regularization term')\n",
        "parser.add_argument('--optimizer', default=\"Adam\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--weight_decay', default=1e-6, type=float, help='weight_decay')\n",
        "parser.add_argument('--lr', default=3e-4, type=float, help='lr')\n",
        "parser.add_argument('--temperature', default=0.5, type=float, help='temperature')\n",
        "parser.add_argument('--model_dir', default='output/checkpoint/', type=str, help='model save path')\n",
        "parser.add_argument('--root', default=\"../datasets\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--dataset', default='CIFAR10', help='[CIFAR10, CIFAR100, STL-10]')\n",
        "parser.add_argument('--testset', default='CIFAR10', help='[CIFAR10, CIFAR100, STL-10, aircraft, cu_birds, dtd, fashionmnist, mnist, traffic_sign, vgg_flower]')\n",
        "args = parser.parse_args()\n",
        "\n",
        "def train(loader, simclr_model, model, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    model.train()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            if args.model == 'LBE':\n",
        "                _, _, h, _, _, _, _ = simclr_model(x.cuda())\n",
        "            elif args.model == 'MIB':\n",
        "                h, _, _ = simclr_model(x.cuda())\n",
        "            else:\n",
        "                h, _ = simclr_model(x.cuda())\n",
        "        output = model(h)\n",
        "        loss = criterion(output, y.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()}\")\n",
        "    return loss_epoch\n",
        "\n",
        "def test(loader, simclr_model, model):\n",
        "    right_num = 0\n",
        "    all_num = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "            if args.model == 'LBE':\n",
        "                _, _, h, _, _, _, _ = simclr_model(x)\n",
        "            elif args.model == 'MIB':\n",
        "                h, _, _ = simclr_model(x)\n",
        "            else:\n",
        "                h, _ = simclr_model(x)\n",
        "            output = model(h)\n",
        "\n",
        "            predicted = output.argmax(1)\n",
        "            right_num += (predicted == y).sum().item()\n",
        "            all_num += y.size(0)\n",
        "    accuracy = right_num*100./all_num\n",
        "    return accuracy\n",
        "\n",
        "def load_transform(dataset, size=32):\n",
        "    mean, std = get_data_mean_and_stdev(dataset)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)])\n",
        "    return transform\n",
        "\n",
        "def main():\n",
        "    data = 'non_imagenet'\n",
        "    if args.testset == \"CIFAR10\":\n",
        "        train_dataset = torchvision.datasets.CIFAR10(args.root, train=True, download=True, transform=load_transform('CIFAR10', 32))\n",
        "        test_dataset = torchvision.datasets.CIFAR10(args.root, train=False, download=True, transform=load_transform('CIFAR10', 32))\n",
        "    elif args.testset == \"CIFAR100\":\n",
        "        train_dataset = torchvision.datasets.CIFAR100(args.root, train=True, download=True, transform=load_transform('CIFAR100', 32))\n",
        "        test_dataset = torchvision.datasets.CIFAR100(args.root, train=False, download=True, transform=load_transform('CIFAR100', 32))\n",
        "    elif args.testset == \"STL-10\":\n",
        "        train_dataset = torchvision.datasets.STL10(args.root, split='train', download=True, transform=load_transform('STL-10', 96))\n",
        "        test_dataset = torchvision.datasets.STL10(args.root, split='test', download=True, transform=load_transform('STL-10', 96))\n",
        "    else:\n",
        "        if args.dataset=='STL-10':\n",
        "            train_dataset = DATASET[args.testset](train=True, image_transforms=load_transform(args.testset, 64))\n",
        "            test_dataset = DATASET[args.testset](train=False, image_transforms=load_transform(args.testset, 64))\n",
        "        else:\n",
        "            train_dataset = DATASET[args.testset](train=True, image_transforms=load_transform(args.testset, 32))\n",
        "            test_dataset = DATASET[args.testset](train=False, image_transforms=load_transform(args.testset, 32))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.logistic_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.logistic_batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=args.workers)\n",
        "\n",
        "    log_dir = \"output/log/\" + args.testset + '_%s/'%args.model\n",
        "    if not os.path.isdir(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    suffix = args.dataset + '_{}_batch_{}'.format(args.resnet, args.batch_size)\n",
        "    suffix = suffix + '_proj_dim_{}'.format(args.projection_dim) + '_epoch_%d'%args.epochs\n",
        "    args.model_dir = args.model_dir + args.dataset + '_%s/'%args.model\n",
        "    epoch_dir = args.model_dir + suffix + '.pt'\n",
        "    print(\"Loading {}\".format(epoch_dir))\n",
        "    \n",
        "    simclr_model, _, _, _ = load_model(args, reload_model=True, load_path=epoch_dir, data=data)\n",
        "    simclr_model = simclr_model.cuda()\n",
        "    simclr_model.eval()\n",
        "\n",
        "    # Logistic Regression\n",
        "    n_classes = get_data_nclass(args.testset)\n",
        "    model = LogisticRegression(simclr_model.n_features, n_classes).cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [60, 80], gamma=0.1)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.\n",
        "    test_log_file = open(log_dir + suffix + '_LR.txt', \"w\")\n",
        "    for epoch in range(args.logistic_epochs):\n",
        "        loss_epoch = train(train_loader, simclr_model, model, criterion, optimizer)\n",
        "        print(\"Train Epoch [{}]\\t Average loss: {}\".format(epoch, loss_epoch/len(train_loader)))\n",
        "        print(\"Train Epoch [{}]\\t Average loss: {}\".format(epoch, loss_epoch/len(train_loader)), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "\n",
        "        # final testing\n",
        "        test_current_acc = test(test_loader, simclr_model, model)\n",
        "        if test_current_acc > best_acc:\n",
        "            best_acc = test_current_acc\n",
        "        print(\"Test Epoch [{}]\\t Accuracy: {}\\t Best Accuracy: {}\\n\".format(epoch, test_current_acc, best_acc))\n",
        "        print(\"Test Epoch [{}]\\t Accuracy: {}\\t Best Accuracy: {}\\n\".format(epoch, test_current_acc, best_acc), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Final Best Accuracy: {}\".format(best_acc))\n",
        "    print(\"Final Best Accuracy: {}\".format(best_acc), file=test_log_file)\n",
        "    test_log_file.flush()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7N_sfPgSaYfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b9882e-00e8-4202-a369-6e419e2f485b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../datasets/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 28961140.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../datasets/cifar-10-python.tar.gz to ../datasets\n",
            "Files already downloaded and verified\n",
            "Loading output/checkpoint/CIFAR10_orig/CIFAR10_resnet18_batch_256_proj_dim_128_epoch_30.pt\n",
            "Step [0/390]\t Loss: 2.5697855949401855\n",
            "Step [50/390]\t Loss: 0.8785402178764343\n",
            "Step [100/390]\t Loss: 1.2511159181594849\n",
            "Step [150/390]\t Loss: 0.8775743842124939\n",
            "Step [200/390]\t Loss: 0.7960459589958191\n",
            "Step [250/390]\t Loss: 0.9824711680412292\n",
            "Step [300/390]\t Loss: 0.7867254614830017\n",
            "Step [350/390]\t Loss: 0.9047598838806152\n",
            "Train Epoch [0]\t Average loss: 0.9134342312812805\n",
            "Test Epoch [0]\t Accuracy: 68.72\t Best Accuracy: 68.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.7577430605888367\n",
            "Step [50/390]\t Loss: 1.1598427295684814\n",
            "Step [100/390]\t Loss: 0.7971104383468628\n",
            "Step [150/390]\t Loss: 0.6871945261955261\n",
            "Step [200/390]\t Loss: 0.880709171295166\n",
            "Step [250/390]\t Loss: 0.9510619044303894\n",
            "Step [300/390]\t Loss: 0.7515455484390259\n",
            "Step [350/390]\t Loss: 0.7608172297477722\n",
            "Train Epoch [1]\t Average loss: 0.840348033568798\n",
            "Test Epoch [1]\t Accuracy: 71.33\t Best Accuracy: 71.33\n",
            "\n",
            "Step [0/390]\t Loss: 1.0556261539459229\n",
            "Step [50/390]\t Loss: 0.7717757225036621\n",
            "Step [100/390]\t Loss: 0.7383070588111877\n",
            "Step [150/390]\t Loss: 0.9916107654571533\n",
            "Step [200/390]\t Loss: 0.9210379719734192\n",
            "Step [250/390]\t Loss: 0.895494818687439\n",
            "Step [300/390]\t Loss: 0.9364747405052185\n",
            "Step [350/390]\t Loss: 0.8253942131996155\n",
            "Train Epoch [2]\t Average loss: 0.807764767224972\n",
            "Test Epoch [2]\t Accuracy: 71.88\t Best Accuracy: 71.88\n",
            "\n",
            "Step [0/390]\t Loss: 0.7324914932250977\n",
            "Step [50/390]\t Loss: 0.7861162424087524\n",
            "Step [100/390]\t Loss: 0.9746243953704834\n",
            "Step [150/390]\t Loss: 0.7628796100616455\n",
            "Step [200/390]\t Loss: 0.8866131901741028\n",
            "Step [250/390]\t Loss: 0.8849250078201294\n",
            "Step [300/390]\t Loss: 0.6605941653251648\n",
            "Step [350/390]\t Loss: 0.8020973801612854\n",
            "Train Epoch [3]\t Average loss: 0.7945767018275384\n",
            "Test Epoch [3]\t Accuracy: 72.14\t Best Accuracy: 72.14\n",
            "\n",
            "Step [0/390]\t Loss: 0.6437878608703613\n",
            "Step [50/390]\t Loss: 0.6283717155456543\n",
            "Step [100/390]\t Loss: 0.7786809802055359\n",
            "Step [150/390]\t Loss: 0.7116725444793701\n",
            "Step [200/390]\t Loss: 0.7323694229125977\n",
            "Step [250/390]\t Loss: 0.8050941228866577\n",
            "Step [300/390]\t Loss: 0.668971061706543\n",
            "Step [350/390]\t Loss: 0.8316501379013062\n",
            "Train Epoch [4]\t Average loss: 0.7823387212478198\n",
            "Test Epoch [4]\t Accuracy: 72.88\t Best Accuracy: 72.88\n",
            "\n",
            "Step [0/390]\t Loss: 0.6998754739761353\n",
            "Step [50/390]\t Loss: 0.6621241569519043\n",
            "Step [100/390]\t Loss: 0.8111603856086731\n",
            "Step [150/390]\t Loss: 0.7473137378692627\n",
            "Step [200/390]\t Loss: 0.6529366374015808\n",
            "Step [250/390]\t Loss: 0.77391517162323\n",
            "Step [300/390]\t Loss: 0.8472080826759338\n",
            "Step [350/390]\t Loss: 0.8244432210922241\n",
            "Train Epoch [5]\t Average loss: 0.7607699606663142\n",
            "Test Epoch [5]\t Accuracy: 72.45\t Best Accuracy: 72.88\n",
            "\n",
            "Step [0/390]\t Loss: 0.7701454758644104\n",
            "Step [50/390]\t Loss: 0.7733142971992493\n",
            "Step [100/390]\t Loss: 0.8027782440185547\n",
            "Step [150/390]\t Loss: 0.6759628653526306\n",
            "Step [200/390]\t Loss: 0.6924567818641663\n",
            "Step [250/390]\t Loss: 0.8066040277481079\n",
            "Step [300/390]\t Loss: 0.8626883625984192\n",
            "Step [350/390]\t Loss: 0.7084152102470398\n",
            "Train Epoch [6]\t Average loss: 0.758411173025767\n",
            "Test Epoch [6]\t Accuracy: 72.48\t Best Accuracy: 72.88\n",
            "\n",
            "Step [0/390]\t Loss: 0.7284170389175415\n",
            "Step [50/390]\t Loss: 0.6858532428741455\n",
            "Step [100/390]\t Loss: 0.6296696066856384\n",
            "Step [150/390]\t Loss: 0.7957608103752136\n",
            "Step [200/390]\t Loss: 0.7130327224731445\n",
            "Step [250/390]\t Loss: 0.797031819820404\n",
            "Step [300/390]\t Loss: 0.8309232592582703\n",
            "Step [350/390]\t Loss: 0.7945354580879211\n",
            "Train Epoch [7]\t Average loss: 0.7381264497836431\n",
            "Test Epoch [7]\t Accuracy: 70.53\t Best Accuracy: 72.88\n",
            "\n",
            "Step [0/390]\t Loss: 0.816526472568512\n",
            "Step [50/390]\t Loss: 0.8445644378662109\n",
            "Step [100/390]\t Loss: 0.8816223740577698\n",
            "Step [150/390]\t Loss: 0.714600145816803\n",
            "Step [200/390]\t Loss: 0.9973889589309692\n",
            "Step [250/390]\t Loss: 0.6720761656761169\n",
            "Step [300/390]\t Loss: 0.6932236552238464\n",
            "Step [350/390]\t Loss: 0.5243067145347595\n",
            "Train Epoch [8]\t Average loss: 0.759636534559421\n",
            "Test Epoch [8]\t Accuracy: 71.55\t Best Accuracy: 72.88\n",
            "\n",
            "Step [0/390]\t Loss: 0.7921230792999268\n",
            "Step [50/390]\t Loss: 0.756350576877594\n",
            "Step [100/390]\t Loss: 0.9232449531555176\n",
            "Step [150/390]\t Loss: 0.7099297046661377\n",
            "Step [200/390]\t Loss: 0.834015965461731\n",
            "Step [250/390]\t Loss: 0.6192680597305298\n",
            "Step [300/390]\t Loss: 0.7123451232910156\n",
            "Step [350/390]\t Loss: 0.6840533018112183\n",
            "Train Epoch [9]\t Average loss: 0.7446822467522743\n",
            "Test Epoch [9]\t Accuracy: 72.36\t Best Accuracy: 72.88\n",
            "\n",
            "Step [0/390]\t Loss: 1.0392802953720093\n",
            "Step [50/390]\t Loss: 0.7441668510437012\n",
            "Step [100/390]\t Loss: 0.6789475083351135\n",
            "Step [150/390]\t Loss: 0.635422945022583\n",
            "Step [200/390]\t Loss: 0.76064532995224\n",
            "Step [250/390]\t Loss: 0.6215319633483887\n",
            "Step [300/390]\t Loss: 0.683766782283783\n",
            "Step [350/390]\t Loss: 0.9166221022605896\n",
            "Train Epoch [10]\t Average loss: 0.7343493797840216\n",
            "Test Epoch [10]\t Accuracy: 73.57\t Best Accuracy: 73.57\n",
            "\n",
            "Step [0/390]\t Loss: 0.796061635017395\n",
            "Step [50/390]\t Loss: 0.598365068435669\n",
            "Step [100/390]\t Loss: 0.7008246779441833\n",
            "Step [150/390]\t Loss: 0.6206876635551453\n",
            "Step [200/390]\t Loss: 0.756608784198761\n",
            "Step [250/390]\t Loss: 0.6061768531799316\n",
            "Step [300/390]\t Loss: 0.7753953337669373\n",
            "Step [350/390]\t Loss: 0.6131061911582947\n",
            "Train Epoch [11]\t Average loss: 0.7411348505662038\n",
            "Test Epoch [11]\t Accuracy: 72.7\t Best Accuracy: 73.57\n",
            "\n",
            "Step [0/390]\t Loss: 0.8626245856285095\n",
            "Step [50/390]\t Loss: 0.7943132519721985\n",
            "Step [100/390]\t Loss: 0.6639408469200134\n",
            "Step [150/390]\t Loss: 0.8310696482658386\n",
            "Step [200/390]\t Loss: 0.6541739106178284\n",
            "Step [250/390]\t Loss: 0.759779155254364\n",
            "Step [300/390]\t Loss: 0.6749902963638306\n",
            "Step [350/390]\t Loss: 0.7740176916122437\n",
            "Train Epoch [12]\t Average loss: 0.7327156780621944\n",
            "Test Epoch [12]\t Accuracy: 73.07\t Best Accuracy: 73.57\n",
            "\n",
            "Step [0/390]\t Loss: 0.6495938897132874\n",
            "Step [50/390]\t Loss: 0.7788625359535217\n",
            "Step [100/390]\t Loss: 0.7073355913162231\n",
            "Step [150/390]\t Loss: 0.6816310286521912\n",
            "Step [200/390]\t Loss: 0.6545292139053345\n",
            "Step [250/390]\t Loss: 0.6004860997200012\n",
            "Step [300/390]\t Loss: 0.6206479072570801\n",
            "Step [350/390]\t Loss: 0.6850516200065613\n",
            "Train Epoch [13]\t Average loss: 0.7214901886689358\n",
            "Test Epoch [13]\t Accuracy: 73.63\t Best Accuracy: 73.63\n",
            "\n",
            "Step [0/390]\t Loss: 0.6388617157936096\n",
            "Step [50/390]\t Loss: 0.7116224765777588\n",
            "Step [100/390]\t Loss: 0.6024101376533508\n",
            "Step [150/390]\t Loss: 0.5889668464660645\n",
            "Step [200/390]\t Loss: 0.6059039235115051\n",
            "Step [250/390]\t Loss: 0.6256464719772339\n",
            "Step [300/390]\t Loss: 0.6880398988723755\n",
            "Step [350/390]\t Loss: 0.7965811491012573\n",
            "Train Epoch [14]\t Average loss: 0.7134554172173525\n",
            "Test Epoch [14]\t Accuracy: 72.31\t Best Accuracy: 73.63\n",
            "\n",
            "Step [0/390]\t Loss: 0.7801499366760254\n",
            "Step [50/390]\t Loss: 0.8862649202346802\n",
            "Step [100/390]\t Loss: 0.8524876832962036\n",
            "Step [150/390]\t Loss: 0.5736761093139648\n",
            "Step [200/390]\t Loss: 0.8538120985031128\n",
            "Step [250/390]\t Loss: 0.6911695003509521\n",
            "Step [300/390]\t Loss: 0.6226391196250916\n",
            "Step [350/390]\t Loss: 0.6399940848350525\n",
            "Train Epoch [15]\t Average loss: 0.7300297860915844\n",
            "Test Epoch [15]\t Accuracy: 73.55\t Best Accuracy: 73.63\n",
            "\n",
            "Step [0/390]\t Loss: 0.747988224029541\n",
            "Step [50/390]\t Loss: 0.6994850635528564\n",
            "Step [100/390]\t Loss: 0.7849533557891846\n",
            "Step [150/390]\t Loss: 0.6675781607627869\n",
            "Step [200/390]\t Loss: 0.6991153359413147\n",
            "Step [250/390]\t Loss: 0.751539945602417\n",
            "Step [300/390]\t Loss: 0.8092747926712036\n",
            "Step [350/390]\t Loss: 0.5362112522125244\n",
            "Train Epoch [16]\t Average loss: 0.7260472476482391\n",
            "Test Epoch [16]\t Accuracy: 73.87\t Best Accuracy: 73.87\n",
            "\n",
            "Step [0/390]\t Loss: 0.7218421101570129\n",
            "Step [50/390]\t Loss: 0.7260666489601135\n",
            "Step [100/390]\t Loss: 0.8003139495849609\n",
            "Step [150/390]\t Loss: 0.7024447321891785\n",
            "Step [200/390]\t Loss: 0.9226722717285156\n",
            "Step [250/390]\t Loss: 0.6472107172012329\n",
            "Step [300/390]\t Loss: 0.7755013704299927\n",
            "Step [350/390]\t Loss: 0.7961578369140625\n",
            "Train Epoch [17]\t Average loss: 0.7084627487720587\n",
            "Test Epoch [17]\t Accuracy: 74.85\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.6223111152648926\n",
            "Step [50/390]\t Loss: 0.7437801361083984\n",
            "Step [100/390]\t Loss: 0.8235649466514587\n",
            "Step [150/390]\t Loss: 0.7185581922531128\n",
            "Step [200/390]\t Loss: 0.6504080295562744\n",
            "Step [250/390]\t Loss: 0.720565915107727\n",
            "Step [300/390]\t Loss: 0.605168342590332\n",
            "Step [350/390]\t Loss: 0.718223512172699\n",
            "Train Epoch [18]\t Average loss: 0.7135427231207873\n",
            "Test Epoch [18]\t Accuracy: 73.77\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.77569979429245\n",
            "Step [50/390]\t Loss: 0.5878362655639648\n",
            "Step [100/390]\t Loss: 0.5607693195343018\n",
            "Step [150/390]\t Loss: 0.7765639424324036\n",
            "Step [200/390]\t Loss: 0.5695756077766418\n",
            "Step [250/390]\t Loss: 0.5740640759468079\n",
            "Step [300/390]\t Loss: 0.7588757872581482\n",
            "Step [350/390]\t Loss: 0.7087848782539368\n",
            "Train Epoch [19]\t Average loss: 0.7089987428524556\n",
            "Test Epoch [19]\t Accuracy: 72.27\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.6274954080581665\n",
            "Step [50/390]\t Loss: 0.7552801370620728\n",
            "Step [100/390]\t Loss: 0.5940791368484497\n",
            "Step [150/390]\t Loss: 1.0050103664398193\n",
            "Step [200/390]\t Loss: 0.693095326423645\n",
            "Step [250/390]\t Loss: 0.7737369537353516\n",
            "Step [300/390]\t Loss: 0.9329726099967957\n",
            "Step [350/390]\t Loss: 0.7730914354324341\n",
            "Train Epoch [20]\t Average loss: 0.7036495352402712\n",
            "Test Epoch [20]\t Accuracy: 72.85\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.7961494326591492\n",
            "Step [50/390]\t Loss: 0.6898829340934753\n",
            "Step [100/390]\t Loss: 0.6822466254234314\n",
            "Step [150/390]\t Loss: 0.5851894617080688\n",
            "Step [200/390]\t Loss: 0.7043142318725586\n",
            "Step [250/390]\t Loss: 0.7822756767272949\n",
            "Step [300/390]\t Loss: 0.6904089450836182\n",
            "Step [350/390]\t Loss: 0.7690659761428833\n",
            "Train Epoch [21]\t Average loss: 0.7100039270443794\n",
            "Test Epoch [21]\t Accuracy: 73.45\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.5602521300315857\n",
            "Step [50/390]\t Loss: 0.7449288368225098\n",
            "Step [100/390]\t Loss: 0.8532217144966125\n",
            "Step [150/390]\t Loss: 0.7372898459434509\n",
            "Step [200/390]\t Loss: 0.5859087109565735\n",
            "Step [250/390]\t Loss: 0.8604505062103271\n",
            "Step [300/390]\t Loss: 0.7090423703193665\n",
            "Step [350/390]\t Loss: 0.6244614124298096\n",
            "Train Epoch [22]\t Average loss: 0.7053291506492174\n",
            "Test Epoch [22]\t Accuracy: 73.54\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.7350118160247803\n",
            "Step [50/390]\t Loss: 0.7158987522125244\n",
            "Step [100/390]\t Loss: 0.7072190642356873\n",
            "Step [150/390]\t Loss: 0.6506370306015015\n",
            "Step [200/390]\t Loss: 0.8826265931129456\n",
            "Step [250/390]\t Loss: 0.7158896327018738\n",
            "Step [300/390]\t Loss: 0.6187570691108704\n",
            "Step [350/390]\t Loss: 0.8094583749771118\n",
            "Train Epoch [23]\t Average loss: 0.6951381269173744\n",
            "Test Epoch [23]\t Accuracy: 73.51\t Best Accuracy: 74.85\n",
            "\n",
            "Step [0/390]\t Loss: 0.6627385020256042\n",
            "Step [50/390]\t Loss: 0.7631752490997314\n",
            "Step [100/390]\t Loss: 0.6819614171981812\n",
            "Step [150/390]\t Loss: 0.8459383249282837\n",
            "Step [200/390]\t Loss: 0.6064561009407043\n",
            "Step [250/390]\t Loss: 0.7249353528022766\n",
            "Step [300/390]\t Loss: 0.7193808555603027\n",
            "Step [350/390]\t Loss: 0.7295119166374207\n",
            "Train Epoch [24]\t Average loss: 0.7094941178957621\n",
            "Test Epoch [24]\t Accuracy: 74.91\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.6052821278572083\n",
            "Step [50/390]\t Loss: 0.7251330018043518\n",
            "Step [100/390]\t Loss: 0.7935493588447571\n",
            "Step [150/390]\t Loss: 0.720291256904602\n",
            "Step [200/390]\t Loss: 0.7404885292053223\n",
            "Step [250/390]\t Loss: 0.8425933122634888\n",
            "Step [300/390]\t Loss: 0.8381016850471497\n",
            "Step [350/390]\t Loss: 0.7658289074897766\n",
            "Train Epoch [25]\t Average loss: 0.6968915533560973\n",
            "Test Epoch [25]\t Accuracy: 71.82\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.6700414419174194\n",
            "Step [50/390]\t Loss: 0.5662990808486938\n",
            "Step [100/390]\t Loss: 0.6082165241241455\n",
            "Step [150/390]\t Loss: 0.7457365393638611\n",
            "Step [200/390]\t Loss: 0.6920093894004822\n",
            "Step [250/390]\t Loss: 0.558620810508728\n",
            "Step [300/390]\t Loss: 0.5990062355995178\n",
            "Step [350/390]\t Loss: 0.7301623225212097\n",
            "Train Epoch [26]\t Average loss: 0.7068930403544352\n",
            "Test Epoch [26]\t Accuracy: 72.59\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.48357442021369934\n",
            "Step [50/390]\t Loss: 0.7139955163002014\n",
            "Step [100/390]\t Loss: 0.7141146659851074\n",
            "Step [150/390]\t Loss: 0.7212905287742615\n",
            "Step [200/390]\t Loss: 0.4839705228805542\n",
            "Step [250/390]\t Loss: 0.5577154755592346\n",
            "Step [300/390]\t Loss: 0.5480771660804749\n",
            "Step [350/390]\t Loss: 0.6397403478622437\n",
            "Train Epoch [27]\t Average loss: 0.705682783249097\n",
            "Test Epoch [27]\t Accuracy: 74.45\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.8478682637214661\n",
            "Step [50/390]\t Loss: 0.6114212274551392\n",
            "Step [100/390]\t Loss: 0.8022918701171875\n",
            "Step [150/390]\t Loss: 0.63133305311203\n",
            "Step [200/390]\t Loss: 0.9086964130401611\n",
            "Step [250/390]\t Loss: 0.8519220948219299\n",
            "Step [300/390]\t Loss: 0.7842115163803101\n",
            "Step [350/390]\t Loss: 0.7912104725837708\n",
            "Train Epoch [28]\t Average loss: 0.7060979671967336\n",
            "Test Epoch [28]\t Accuracy: 73.61\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.7619381546974182\n",
            "Step [50/390]\t Loss: 0.574617326259613\n",
            "Step [100/390]\t Loss: 0.7961536645889282\n",
            "Step [150/390]\t Loss: 0.725382924079895\n",
            "Step [200/390]\t Loss: 0.6388916373252869\n",
            "Step [250/390]\t Loss: 0.740984320640564\n",
            "Step [300/390]\t Loss: 0.6470665335655212\n",
            "Step [350/390]\t Loss: 0.7391775846481323\n",
            "Train Epoch [29]\t Average loss: 0.7015069730006731\n",
            "Test Epoch [29]\t Accuracy: 73.33\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.7339144349098206\n",
            "Step [50/390]\t Loss: 0.6648436784744263\n",
            "Step [100/390]\t Loss: 0.7200928926467896\n",
            "Step [150/390]\t Loss: 0.7066207528114319\n",
            "Step [200/390]\t Loss: 0.8104308843612671\n",
            "Step [250/390]\t Loss: 0.7404447793960571\n",
            "Step [300/390]\t Loss: 0.7446720600128174\n",
            "Step [350/390]\t Loss: 0.6251490712165833\n",
            "Train Epoch [30]\t Average loss: 0.6966565307134237\n",
            "Test Epoch [30]\t Accuracy: 72.65\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.8948198556900024\n",
            "Step [50/390]\t Loss: 0.7810627222061157\n",
            "Step [100/390]\t Loss: 0.619944155216217\n",
            "Step [150/390]\t Loss: 0.962660551071167\n",
            "Step [200/390]\t Loss: 0.6765216588973999\n",
            "Step [250/390]\t Loss: 0.6536979079246521\n",
            "Step [300/390]\t Loss: 0.6902496218681335\n",
            "Step [350/390]\t Loss: 0.5895514488220215\n",
            "Train Epoch [31]\t Average loss: 0.7020642996598513\n",
            "Test Epoch [31]\t Accuracy: 74.32\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.7097023129463196\n",
            "Step [50/390]\t Loss: 0.7109716534614563\n",
            "Step [100/390]\t Loss: 0.8561704158782959\n",
            "Step [150/390]\t Loss: 0.5855648517608643\n",
            "Step [200/390]\t Loss: 0.5113658905029297\n",
            "Step [250/390]\t Loss: 0.6311476230621338\n",
            "Step [300/390]\t Loss: 0.8254280686378479\n",
            "Step [350/390]\t Loss: 0.7584079504013062\n",
            "Train Epoch [32]\t Average loss: 0.7013160639084303\n",
            "Test Epoch [32]\t Accuracy: 74.32\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.5926576852798462\n",
            "Step [50/390]\t Loss: 0.7062990665435791\n",
            "Step [100/390]\t Loss: 0.5335368514060974\n",
            "Step [150/390]\t Loss: 0.7352307438850403\n",
            "Step [200/390]\t Loss: 0.6743035316467285\n",
            "Step [250/390]\t Loss: 0.4242285192012787\n",
            "Step [300/390]\t Loss: 0.575068473815918\n",
            "Step [350/390]\t Loss: 0.4526658356189728\n",
            "Train Epoch [33]\t Average loss: 0.6911573711878214\n",
            "Test Epoch [33]\t Accuracy: 72.39\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.5790793299674988\n",
            "Step [50/390]\t Loss: 0.7734761238098145\n",
            "Step [100/390]\t Loss: 0.5925633311271667\n",
            "Step [150/390]\t Loss: 0.7562589049339294\n",
            "Step [200/390]\t Loss: 0.6729152202606201\n",
            "Step [250/390]\t Loss: 0.5048665404319763\n",
            "Step [300/390]\t Loss: 0.6238870620727539\n",
            "Step [350/390]\t Loss: 0.6858409643173218\n",
            "Train Epoch [34]\t Average loss: 0.6906512945126264\n",
            "Test Epoch [34]\t Accuracy: 73.6\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.7020072340965271\n",
            "Step [50/390]\t Loss: 0.4916362762451172\n",
            "Step [100/390]\t Loss: 0.7140831351280212\n",
            "Step [150/390]\t Loss: 0.4879814386367798\n",
            "Step [200/390]\t Loss: 0.6830728054046631\n",
            "Step [250/390]\t Loss: 0.7169885635375977\n",
            "Step [300/390]\t Loss: 0.795628547668457\n",
            "Step [350/390]\t Loss: 0.5796403884887695\n",
            "Train Epoch [35]\t Average loss: 0.6831072178406593\n",
            "Test Epoch [35]\t Accuracy: 72.68\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.7249473333358765\n",
            "Step [50/390]\t Loss: 0.6837527751922607\n",
            "Step [100/390]\t Loss: 0.7708820104598999\n",
            "Step [150/390]\t Loss: 0.5678249597549438\n",
            "Step [200/390]\t Loss: 0.7419232726097107\n",
            "Step [250/390]\t Loss: 0.5901638269424438\n",
            "Step [300/390]\t Loss: 0.729091227054596\n",
            "Step [350/390]\t Loss: 0.7826969623565674\n",
            "Train Epoch [36]\t Average loss: 0.688808391109491\n",
            "Test Epoch [36]\t Accuracy: 74.07\t Best Accuracy: 74.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.6500192880630493\n",
            "Step [50/390]\t Loss: 0.7743130326271057\n",
            "Step [100/390]\t Loss: 0.7195690870285034\n",
            "Step [150/390]\t Loss: 0.7828271389007568\n",
            "Step [200/390]\t Loss: 0.49098989367485046\n",
            "Step [250/390]\t Loss: 0.6285806894302368\n",
            "Step [300/390]\t Loss: 0.688951849937439\n",
            "Step [350/390]\t Loss: 0.9036851525306702\n",
            "Train Epoch [37]\t Average loss: 0.6982771275899349\n",
            "Test Epoch [37]\t Accuracy: 75.2\t Best Accuracy: 75.2\n",
            "\n",
            "Step [0/390]\t Loss: 0.6518584489822388\n",
            "Step [50/390]\t Loss: 0.5708714127540588\n",
            "Step [100/390]\t Loss: 0.7767528891563416\n",
            "Step [150/390]\t Loss: 0.7960140109062195\n",
            "Step [200/390]\t Loss: 0.5137003064155579\n",
            "Step [250/390]\t Loss: 0.7837605476379395\n",
            "Step [300/390]\t Loss: 0.7780997157096863\n",
            "Step [350/390]\t Loss: 0.9568527340888977\n",
            "Train Epoch [38]\t Average loss: 0.6835183995656479\n",
            "Test Epoch [38]\t Accuracy: 74.7\t Best Accuracy: 75.2\n",
            "\n",
            "Step [0/390]\t Loss: 0.5466585159301758\n",
            "Step [50/390]\t Loss: 0.7954392433166504\n",
            "Step [100/390]\t Loss: 0.5780692100524902\n",
            "Step [150/390]\t Loss: 0.723146378993988\n",
            "Step [200/390]\t Loss: 0.7099276185035706\n",
            "Step [250/390]\t Loss: 0.641758918762207\n",
            "Step [300/390]\t Loss: 0.5512830018997192\n",
            "Step [350/390]\t Loss: 0.69434654712677\n",
            "Train Epoch [39]\t Average loss: 0.6955132924593412\n",
            "Test Epoch [39]\t Accuracy: 75.12\t Best Accuracy: 75.2\n",
            "\n",
            "Step [0/390]\t Loss: 0.5609264969825745\n",
            "Step [50/390]\t Loss: 0.7131207585334778\n",
            "Step [100/390]\t Loss: 0.6225783824920654\n",
            "Step [150/390]\t Loss: 0.694514274597168\n",
            "Step [200/390]\t Loss: 0.5520552396774292\n",
            "Step [250/390]\t Loss: 0.7439259886741638\n",
            "Step [300/390]\t Loss: 0.6331296563148499\n",
            "Step [350/390]\t Loss: 0.8167981505393982\n",
            "Train Epoch [40]\t Average loss: 0.6797884256411821\n",
            "Test Epoch [40]\t Accuracy: 75.53\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.7816529870033264\n",
            "Step [50/390]\t Loss: 0.7603418231010437\n",
            "Step [100/390]\t Loss: 0.6301316618919373\n",
            "Step [150/390]\t Loss: 0.7907053232192993\n",
            "Step [200/390]\t Loss: 0.7636496424674988\n",
            "Step [250/390]\t Loss: 0.6951180696487427\n",
            "Step [300/390]\t Loss: 0.7455915212631226\n",
            "Step [350/390]\t Loss: 0.6749477982521057\n",
            "Train Epoch [41]\t Average loss: 0.6685633930640343\n",
            "Test Epoch [41]\t Accuracy: 74.63\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6203565001487732\n",
            "Step [50/390]\t Loss: 0.7803928852081299\n",
            "Step [100/390]\t Loss: 0.5903278589248657\n",
            "Step [150/390]\t Loss: 0.7347950339317322\n",
            "Step [200/390]\t Loss: 0.592795193195343\n",
            "Step [250/390]\t Loss: 0.4404185116291046\n",
            "Step [300/390]\t Loss: 0.6912223100662231\n",
            "Step [350/390]\t Loss: 0.6305931210517883\n",
            "Train Epoch [42]\t Average loss: 0.6957141903730539\n",
            "Test Epoch [42]\t Accuracy: 71.85\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.7702124714851379\n",
            "Step [50/390]\t Loss: 0.5193694233894348\n",
            "Step [100/390]\t Loss: 0.6263188719749451\n",
            "Step [150/390]\t Loss: 0.6055135726928711\n",
            "Step [200/390]\t Loss: 0.8256729245185852\n",
            "Step [250/390]\t Loss: 0.7487563490867615\n",
            "Step [300/390]\t Loss: 0.5062388777732849\n",
            "Step [350/390]\t Loss: 0.7794262170791626\n",
            "Train Epoch [43]\t Average loss: 0.6797430018583933\n",
            "Test Epoch [43]\t Accuracy: 72.01\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.633284330368042\n",
            "Step [50/390]\t Loss: 0.8598655462265015\n",
            "Step [100/390]\t Loss: 0.717383861541748\n",
            "Step [150/390]\t Loss: 0.5261439681053162\n",
            "Step [200/390]\t Loss: 0.6407458782196045\n",
            "Step [250/390]\t Loss: 0.671859085559845\n",
            "Step [300/390]\t Loss: 0.4982287585735321\n",
            "Step [350/390]\t Loss: 0.5357409119606018\n",
            "Train Epoch [44]\t Average loss: 0.680652805704337\n",
            "Test Epoch [44]\t Accuracy: 75.11\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6001930832862854\n",
            "Step [50/390]\t Loss: 0.8625690340995789\n",
            "Step [100/390]\t Loss: 0.754145085811615\n",
            "Step [150/390]\t Loss: 0.49130678176879883\n",
            "Step [200/390]\t Loss: 0.6501691341400146\n",
            "Step [250/390]\t Loss: 0.5811346769332886\n",
            "Step [300/390]\t Loss: 0.5855523943901062\n",
            "Step [350/390]\t Loss: 0.6721575260162354\n",
            "Train Epoch [45]\t Average loss: 0.6769125855121857\n",
            "Test Epoch [45]\t Accuracy: 71.64\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.7301145792007446\n",
            "Step [50/390]\t Loss: 0.6147001385688782\n",
            "Step [100/390]\t Loss: 0.8756181001663208\n",
            "Step [150/390]\t Loss: 0.6069878935813904\n",
            "Step [200/390]\t Loss: 0.7332044839859009\n",
            "Step [250/390]\t Loss: 0.7126030921936035\n",
            "Step [300/390]\t Loss: 0.8235951066017151\n",
            "Step [350/390]\t Loss: 0.5876417756080627\n",
            "Train Epoch [46]\t Average loss: 0.6730828657364234\n",
            "Test Epoch [46]\t Accuracy: 74.92\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.5940837264060974\n",
            "Step [50/390]\t Loss: 0.7517521381378174\n",
            "Step [100/390]\t Loss: 0.5302172899246216\n",
            "Step [150/390]\t Loss: 0.6496196389198303\n",
            "Step [200/390]\t Loss: 0.7183345556259155\n",
            "Step [250/390]\t Loss: 0.7248309850692749\n",
            "Step [300/390]\t Loss: 0.8037927746772766\n",
            "Step [350/390]\t Loss: 0.5509004592895508\n",
            "Train Epoch [47]\t Average loss: 0.6782568720670846\n",
            "Test Epoch [47]\t Accuracy: 74.7\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.5149279832839966\n",
            "Step [50/390]\t Loss: 0.5007771849632263\n",
            "Step [100/390]\t Loss: 0.6271566152572632\n",
            "Step [150/390]\t Loss: 0.6934270262718201\n",
            "Step [200/390]\t Loss: 0.6624377965927124\n",
            "Step [250/390]\t Loss: 0.791961133480072\n",
            "Step [300/390]\t Loss: 0.6642518639564514\n",
            "Step [350/390]\t Loss: 0.6269568800926208\n",
            "Train Epoch [48]\t Average loss: 0.6711875996528528\n",
            "Test Epoch [48]\t Accuracy: 74.31\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.7746932506561279\n",
            "Step [50/390]\t Loss: 0.7313573956489563\n",
            "Step [100/390]\t Loss: 0.7107974886894226\n",
            "Step [150/390]\t Loss: 0.7066030502319336\n",
            "Step [200/390]\t Loss: 0.7437729835510254\n",
            "Step [250/390]\t Loss: 0.9100273251533508\n",
            "Step [300/390]\t Loss: 0.7098252177238464\n",
            "Step [350/390]\t Loss: 0.6051624417304993\n",
            "Train Epoch [49]\t Average loss: 0.6729498493365752\n",
            "Test Epoch [49]\t Accuracy: 75.02\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6664474606513977\n",
            "Step [50/390]\t Loss: 0.6960373520851135\n",
            "Step [100/390]\t Loss: 0.6397338509559631\n",
            "Step [150/390]\t Loss: 0.5825246572494507\n",
            "Step [200/390]\t Loss: 0.6904429197311401\n",
            "Step [250/390]\t Loss: 0.6397563815116882\n",
            "Step [300/390]\t Loss: 0.6907818913459778\n",
            "Step [350/390]\t Loss: 1.035328984260559\n",
            "Train Epoch [50]\t Average loss: 0.6852278981453334\n",
            "Test Epoch [50]\t Accuracy: 72.71\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.8156686425209045\n",
            "Step [50/390]\t Loss: 0.7111746668815613\n",
            "Step [100/390]\t Loss: 0.5435538291931152\n",
            "Step [150/390]\t Loss: 0.6439065337181091\n",
            "Step [200/390]\t Loss: 0.5422756671905518\n",
            "Step [250/390]\t Loss: 0.7288565039634705\n",
            "Step [300/390]\t Loss: 0.6501963138580322\n",
            "Step [350/390]\t Loss: 0.61090087890625\n",
            "Train Epoch [51]\t Average loss: 0.6732808870382798\n",
            "Test Epoch [51]\t Accuracy: 74.97\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6099274158477783\n",
            "Step [50/390]\t Loss: 0.9185214638710022\n",
            "Step [100/390]\t Loss: 0.8112248778343201\n",
            "Step [150/390]\t Loss: 0.5156709551811218\n",
            "Step [200/390]\t Loss: 0.7597904801368713\n",
            "Step [250/390]\t Loss: 0.691781759262085\n",
            "Step [300/390]\t Loss: 0.4800221025943756\n",
            "Step [350/390]\t Loss: 0.8287774324417114\n",
            "Train Epoch [52]\t Average loss: 0.6741188595692317\n",
            "Test Epoch [52]\t Accuracy: 73.08\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6844059824943542\n",
            "Step [50/390]\t Loss: 0.5264845490455627\n",
            "Step [100/390]\t Loss: 0.8493343591690063\n",
            "Step [150/390]\t Loss: 0.5285207629203796\n",
            "Step [200/390]\t Loss: 0.6638230681419373\n",
            "Step [250/390]\t Loss: 0.8277161717414856\n",
            "Step [300/390]\t Loss: 0.7724471092224121\n",
            "Step [350/390]\t Loss: 0.7505947947502136\n",
            "Train Epoch [53]\t Average loss: 0.6842936987296129\n",
            "Test Epoch [53]\t Accuracy: 74.99\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6476627588272095\n",
            "Step [50/390]\t Loss: 0.8638727068901062\n",
            "Step [100/390]\t Loss: 0.6831836104393005\n",
            "Step [150/390]\t Loss: 0.6481647491455078\n",
            "Step [200/390]\t Loss: 0.6580807566642761\n",
            "Step [250/390]\t Loss: 0.7480545043945312\n",
            "Step [300/390]\t Loss: 0.6343643665313721\n",
            "Step [350/390]\t Loss: 0.45135465264320374\n",
            "Train Epoch [54]\t Average loss: 0.6733172967647895\n",
            "Test Epoch [54]\t Accuracy: 75.36\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6690897941589355\n",
            "Step [50/390]\t Loss: 0.5820980072021484\n",
            "Step [100/390]\t Loss: 0.6719770431518555\n",
            "Step [150/390]\t Loss: 0.5575610995292664\n",
            "Step [200/390]\t Loss: 0.7894600629806519\n",
            "Step [250/390]\t Loss: 0.7220605611801147\n",
            "Step [300/390]\t Loss: 0.7606462836265564\n",
            "Step [350/390]\t Loss: 0.6306143403053284\n",
            "Train Epoch [55]\t Average loss: 0.6645667682855557\n",
            "Test Epoch [55]\t Accuracy: 75.24\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.7134832739830017\n",
            "Step [50/390]\t Loss: 0.6762005686759949\n",
            "Step [100/390]\t Loss: 0.6250506043434143\n",
            "Step [150/390]\t Loss: 0.562528669834137\n",
            "Step [200/390]\t Loss: 0.6718845367431641\n",
            "Step [250/390]\t Loss: 0.5379416346549988\n",
            "Step [300/390]\t Loss: 0.6952193975448608\n",
            "Step [350/390]\t Loss: 0.5433487892150879\n",
            "Train Epoch [56]\t Average loss: 0.6715409227670768\n",
            "Test Epoch [56]\t Accuracy: 73.45\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.5889725089073181\n",
            "Step [50/390]\t Loss: 0.7597948312759399\n",
            "Step [100/390]\t Loss: 0.5117979645729065\n",
            "Step [150/390]\t Loss: 0.6108834743499756\n",
            "Step [200/390]\t Loss: 0.7697117328643799\n",
            "Step [250/390]\t Loss: 0.7214927077293396\n",
            "Step [300/390]\t Loss: 0.7557859420776367\n",
            "Step [350/390]\t Loss: 0.5604214072227478\n",
            "Train Epoch [57]\t Average loss: 0.6915966283816558\n",
            "Test Epoch [57]\t Accuracy: 73.57\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.6704013347625732\n",
            "Step [50/390]\t Loss: 0.7153475284576416\n",
            "Step [100/390]\t Loss: 0.6305287480354309\n",
            "Step [150/390]\t Loss: 0.6210330128669739\n",
            "Step [200/390]\t Loss: 0.7371475100517273\n",
            "Step [250/390]\t Loss: 0.9567294120788574\n",
            "Step [300/390]\t Loss: 0.7889904379844666\n",
            "Step [350/390]\t Loss: 0.7948965430259705\n",
            "Train Epoch [58]\t Average loss: 0.6663507023682961\n",
            "Test Epoch [58]\t Accuracy: 74.07\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.8900476098060608\n",
            "Step [50/390]\t Loss: 0.6776840686798096\n",
            "Step [100/390]\t Loss: 0.6516802906990051\n",
            "Step [150/390]\t Loss: 0.6671213507652283\n",
            "Step [200/390]\t Loss: 0.8291547894477844\n",
            "Step [250/390]\t Loss: 0.5920335650444031\n",
            "Step [300/390]\t Loss: 0.5954460501670837\n",
            "Step [350/390]\t Loss: 0.6500107645988464\n",
            "Train Epoch [59]\t Average loss: 0.6588013463295423\n",
            "Test Epoch [59]\t Accuracy: 74.26\t Best Accuracy: 75.53\n",
            "\n",
            "Step [0/390]\t Loss: 0.5827572345733643\n",
            "Step [50/390]\t Loss: 0.4338670074939728\n",
            "Step [100/390]\t Loss: 0.640681803226471\n",
            "Step [150/390]\t Loss: 0.4372412860393524\n",
            "Step [200/390]\t Loss: 0.47951674461364746\n",
            "Step [250/390]\t Loss: 0.4667079448699951\n",
            "Step [300/390]\t Loss: 0.4693688154220581\n",
            "Step [350/390]\t Loss: 0.6873316764831543\n",
            "Train Epoch [60]\t Average loss: 0.6019809386669062\n",
            "Test Epoch [60]\t Accuracy: 76.64\t Best Accuracy: 76.64\n",
            "\n",
            "Step [0/390]\t Loss: 0.5330206155776978\n",
            "Step [50/390]\t Loss: 0.43010786175727844\n",
            "Step [100/390]\t Loss: 0.5653409361839294\n",
            "Step [150/390]\t Loss: 0.5837352871894836\n",
            "Step [200/390]\t Loss: 0.6463726758956909\n",
            "Step [250/390]\t Loss: 0.6068949103355408\n",
            "Step [300/390]\t Loss: 0.5659279823303223\n",
            "Step [350/390]\t Loss: 0.5458921790122986\n",
            "Train Epoch [61]\t Average loss: 0.5969927966594696\n",
            "Test Epoch [61]\t Accuracy: 76.26\t Best Accuracy: 76.64\n",
            "\n",
            "Step [0/390]\t Loss: 0.6522601842880249\n",
            "Step [50/390]\t Loss: 0.59273761510849\n",
            "Step [100/390]\t Loss: 0.5986058115959167\n",
            "Step [150/390]\t Loss: 0.5641909241676331\n",
            "Step [200/390]\t Loss: 0.7727514505386353\n",
            "Step [250/390]\t Loss: 0.6215941905975342\n",
            "Step [300/390]\t Loss: 0.4942144453525543\n",
            "Step [350/390]\t Loss: 0.5293264389038086\n",
            "Train Epoch [62]\t Average loss: 0.5981352352179014\n",
            "Test Epoch [62]\t Accuracy: 76.41\t Best Accuracy: 76.64\n",
            "\n",
            "Step [0/390]\t Loss: 0.6464869976043701\n",
            "Step [50/390]\t Loss: 0.5214606523513794\n",
            "Step [100/390]\t Loss: 0.4756954610347748\n",
            "Step [150/390]\t Loss: 0.624760091304779\n",
            "Step [200/390]\t Loss: 0.4884755611419678\n",
            "Step [250/390]\t Loss: 0.4781467318534851\n",
            "Step [300/390]\t Loss: 0.5856478214263916\n",
            "Step [350/390]\t Loss: 0.6398099064826965\n",
            "Train Epoch [63]\t Average loss: 0.5974706562665792\n",
            "Test Epoch [63]\t Accuracy: 76.35\t Best Accuracy: 76.64\n",
            "\n",
            "Step [0/390]\t Loss: 0.4568772614002228\n",
            "Step [50/390]\t Loss: 0.6375066637992859\n",
            "Step [100/390]\t Loss: 0.6402474641799927\n",
            "Step [150/390]\t Loss: 0.7274041175842285\n",
            "Step [200/390]\t Loss: 0.6931737065315247\n",
            "Step [250/390]\t Loss: 0.4425123333930969\n",
            "Step [300/390]\t Loss: 0.7041828632354736\n",
            "Step [350/390]\t Loss: 0.641083836555481\n",
            "Train Epoch [64]\t Average loss: 0.5965534836818011\n",
            "Test Epoch [64]\t Accuracy: 76.36\t Best Accuracy: 76.64\n",
            "\n",
            "Step [0/390]\t Loss: 0.5934244394302368\n",
            "Step [50/390]\t Loss: 0.5864036679267883\n",
            "Step [100/390]\t Loss: 0.49034208059310913\n",
            "Step [150/390]\t Loss: 0.5238251686096191\n",
            "Step [200/390]\t Loss: 0.591273307800293\n",
            "Step [250/390]\t Loss: 0.5978108048439026\n",
            "Step [300/390]\t Loss: 0.5159413814544678\n",
            "Step [350/390]\t Loss: 0.6631182432174683\n",
            "Train Epoch [65]\t Average loss: 0.5964492420355479\n",
            "Test Epoch [65]\t Accuracy: 76.72\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6763850450515747\n",
            "Step [50/390]\t Loss: 0.7687428593635559\n",
            "Step [100/390]\t Loss: 0.5465248227119446\n",
            "Step [150/390]\t Loss: 0.5373778343200684\n",
            "Step [200/390]\t Loss: 0.5923401117324829\n",
            "Step [250/390]\t Loss: 0.7487549185752869\n",
            "Step [300/390]\t Loss: 0.5360029339790344\n",
            "Step [350/390]\t Loss: 0.7200528383255005\n",
            "Train Epoch [66]\t Average loss: 0.5969398104227506\n",
            "Test Epoch [66]\t Accuracy: 76.34\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6117543578147888\n",
            "Step [50/390]\t Loss: 0.5172500014305115\n",
            "Step [100/390]\t Loss: 0.639075756072998\n",
            "Step [150/390]\t Loss: 0.6153540015220642\n",
            "Step [200/390]\t Loss: 0.5958155989646912\n",
            "Step [250/390]\t Loss: 0.5469918847084045\n",
            "Step [300/390]\t Loss: 0.6624892354011536\n",
            "Step [350/390]\t Loss: 0.6286126375198364\n",
            "Train Epoch [67]\t Average loss: 0.5971044084964654\n",
            "Test Epoch [67]\t Accuracy: 76.4\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5490372776985168\n",
            "Step [50/390]\t Loss: 0.6752652525901794\n",
            "Step [100/390]\t Loss: 0.5229547023773193\n",
            "Step [150/390]\t Loss: 0.44860580563545227\n",
            "Step [200/390]\t Loss: 0.45657870173454285\n",
            "Step [250/390]\t Loss: 0.5012390613555908\n",
            "Step [300/390]\t Loss: 0.5131711363792419\n",
            "Step [350/390]\t Loss: 0.48127710819244385\n",
            "Train Epoch [68]\t Average loss: 0.5962444426157536\n",
            "Test Epoch [68]\t Accuracy: 76.54\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5988350510597229\n",
            "Step [50/390]\t Loss: 0.5344669222831726\n",
            "Step [100/390]\t Loss: 0.4992437958717346\n",
            "Step [150/390]\t Loss: 0.5163611173629761\n",
            "Step [200/390]\t Loss: 0.5783683657646179\n",
            "Step [250/390]\t Loss: 0.48703861236572266\n",
            "Step [300/390]\t Loss: 0.5884487628936768\n",
            "Step [350/390]\t Loss: 0.7288343906402588\n",
            "Train Epoch [69]\t Average loss: 0.5972805127883569\n",
            "Test Epoch [69]\t Accuracy: 76.43\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5256972312927246\n",
            "Step [50/390]\t Loss: 0.6195985078811646\n",
            "Step [100/390]\t Loss: 0.6336824893951416\n",
            "Step [150/390]\t Loss: 0.5429565906524658\n",
            "Step [200/390]\t Loss: 0.49570661783218384\n",
            "Step [250/390]\t Loss: 0.6393148303031921\n",
            "Step [300/390]\t Loss: 0.6632984280586243\n",
            "Step [350/390]\t Loss: 0.4557993710041046\n",
            "Train Epoch [70]\t Average loss: 0.596540513405433\n",
            "Test Epoch [70]\t Accuracy: 76.41\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5085253119468689\n",
            "Step [50/390]\t Loss: 0.6344996094703674\n",
            "Step [100/390]\t Loss: 0.577233076095581\n",
            "Step [150/390]\t Loss: 0.7434414029121399\n",
            "Step [200/390]\t Loss: 0.5529314875602722\n",
            "Step [250/390]\t Loss: 0.6398128867149353\n",
            "Step [300/390]\t Loss: 0.443246454000473\n",
            "Step [350/390]\t Loss: 0.49700498580932617\n",
            "Train Epoch [71]\t Average loss: 0.5958379027171012\n",
            "Test Epoch [71]\t Accuracy: 76.33\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6513667106628418\n",
            "Step [50/390]\t Loss: 0.7753881812095642\n",
            "Step [100/390]\t Loss: 0.7023901343345642\n",
            "Step [150/390]\t Loss: 0.5478667616844177\n",
            "Step [200/390]\t Loss: 0.5780735611915588\n",
            "Step [250/390]\t Loss: 0.5801183581352234\n",
            "Step [300/390]\t Loss: 0.6716403365135193\n",
            "Step [350/390]\t Loss: 0.5251738429069519\n",
            "Train Epoch [72]\t Average loss: 0.5976411159221943\n",
            "Test Epoch [72]\t Accuracy: 76.42\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6352933049201965\n",
            "Step [50/390]\t Loss: 0.6687486171722412\n",
            "Step [100/390]\t Loss: 0.6008569598197937\n",
            "Step [150/390]\t Loss: 0.7337915301322937\n",
            "Step [200/390]\t Loss: 0.45019015669822693\n",
            "Step [250/390]\t Loss: 0.6205275654792786\n",
            "Step [300/390]\t Loss: 0.6432487368583679\n",
            "Step [350/390]\t Loss: 0.5722448825836182\n",
            "Train Epoch [73]\t Average loss: 0.5956489164859821\n",
            "Test Epoch [73]\t Accuracy: 76.47\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6045158505439758\n",
            "Step [50/390]\t Loss: 0.5676740407943726\n",
            "Step [100/390]\t Loss: 0.7742656469345093\n",
            "Step [150/390]\t Loss: 0.5056589841842651\n",
            "Step [200/390]\t Loss: 0.5087132453918457\n",
            "Step [250/390]\t Loss: 0.5179582834243774\n",
            "Step [300/390]\t Loss: 0.5943226218223572\n",
            "Step [350/390]\t Loss: 0.5038237571716309\n",
            "Train Epoch [74]\t Average loss: 0.5961935302385917\n",
            "Test Epoch [74]\t Accuracy: 76.63\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.633060872554779\n",
            "Step [50/390]\t Loss: 0.6642484068870544\n",
            "Step [100/390]\t Loss: 0.6818523406982422\n",
            "Step [150/390]\t Loss: 0.5765309929847717\n",
            "Step [200/390]\t Loss: 0.4591542184352875\n",
            "Step [250/390]\t Loss: 0.5876913666725159\n",
            "Step [300/390]\t Loss: 0.6318144202232361\n",
            "Step [350/390]\t Loss: 0.6463367938995361\n",
            "Train Epoch [75]\t Average loss: 0.5959746646575439\n",
            "Test Epoch [75]\t Accuracy: 76.39\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6581035256385803\n",
            "Step [50/390]\t Loss: 0.44909217953681946\n",
            "Step [100/390]\t Loss: 0.5225846171379089\n",
            "Step [150/390]\t Loss: 0.6091313362121582\n",
            "Step [200/390]\t Loss: 0.6613547205924988\n",
            "Step [250/390]\t Loss: 0.7700396180152893\n",
            "Step [300/390]\t Loss: 0.5482080578804016\n",
            "Step [350/390]\t Loss: 0.4403609335422516\n",
            "Train Epoch [76]\t Average loss: 0.5957066358664097\n",
            "Test Epoch [76]\t Accuracy: 76.23\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5867106318473816\n",
            "Step [50/390]\t Loss: 0.511031448841095\n",
            "Step [100/390]\t Loss: 0.6037819981575012\n",
            "Step [150/390]\t Loss: 0.48899325728416443\n",
            "Step [200/390]\t Loss: 0.6232497692108154\n",
            "Step [250/390]\t Loss: 0.6928772330284119\n",
            "Step [300/390]\t Loss: 0.5001418590545654\n",
            "Step [350/390]\t Loss: 0.505073606967926\n",
            "Train Epoch [77]\t Average loss: 0.5967311609249849\n",
            "Test Epoch [77]\t Accuracy: 76.62\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5433689951896667\n",
            "Step [50/390]\t Loss: 0.6778916716575623\n",
            "Step [100/390]\t Loss: 0.5760797262191772\n",
            "Step [150/390]\t Loss: 0.5024810433387756\n",
            "Step [200/390]\t Loss: 0.6057553291320801\n",
            "Step [250/390]\t Loss: 0.48876237869262695\n",
            "Step [300/390]\t Loss: 0.49941912293434143\n",
            "Step [350/390]\t Loss: 0.5352492332458496\n",
            "Train Epoch [78]\t Average loss: 0.595250128247799\n",
            "Test Epoch [78]\t Accuracy: 76.48\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.5465039610862732\n",
            "Step [50/390]\t Loss: 0.47992199659347534\n",
            "Step [100/390]\t Loss: 0.4826795756816864\n",
            "Step [150/390]\t Loss: 0.527127206325531\n",
            "Step [200/390]\t Loss: 0.6893215179443359\n",
            "Step [250/390]\t Loss: 0.703260600566864\n",
            "Step [300/390]\t Loss: 0.6261642575263977\n",
            "Step [350/390]\t Loss: 0.6810677647590637\n",
            "Train Epoch [79]\t Average loss: 0.5957860747973124\n",
            "Test Epoch [79]\t Accuracy: 76.22\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6150681972503662\n",
            "Step [50/390]\t Loss: 0.5835264921188354\n",
            "Step [100/390]\t Loss: 0.5499886274337769\n",
            "Step [150/390]\t Loss: 0.7652861475944519\n",
            "Step [200/390]\t Loss: 0.611319899559021\n",
            "Step [250/390]\t Loss: 0.4903007447719574\n",
            "Step [300/390]\t Loss: 0.6315920948982239\n",
            "Step [350/390]\t Loss: 0.6560511589050293\n",
            "Train Epoch [80]\t Average loss: 0.5910748713291608\n",
            "Test Epoch [80]\t Accuracy: 76.54\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.6554808616638184\n",
            "Step [50/390]\t Loss: 0.6567143797874451\n",
            "Step [100/390]\t Loss: 0.6176850199699402\n",
            "Step [150/390]\t Loss: 0.7337504029273987\n",
            "Step [200/390]\t Loss: 0.6243062615394592\n",
            "Step [250/390]\t Loss: 0.4536034166812897\n",
            "Step [300/390]\t Loss: 0.5868022441864014\n",
            "Step [350/390]\t Loss: 0.44024932384490967\n",
            "Train Epoch [81]\t Average loss: 0.5897607425084481\n",
            "Test Epoch [81]\t Accuracy: 76.59\t Best Accuracy: 76.72\n",
            "\n",
            "Step [0/390]\t Loss: 0.75734943151474\n",
            "Step [50/390]\t Loss: 0.5594198107719421\n",
            "Step [100/390]\t Loss: 0.5920742750167847\n",
            "Step [150/390]\t Loss: 0.5570871829986572\n",
            "Step [200/390]\t Loss: 0.5680694580078125\n",
            "Step [250/390]\t Loss: 0.5643036365509033\n",
            "Step [300/390]\t Loss: 0.5742664933204651\n",
            "Step [350/390]\t Loss: 0.5444002151489258\n",
            "Train Epoch [82]\t Average loss: 0.589488695982175\n",
            "Test Epoch [82]\t Accuracy: 76.81\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.5676361322402954\n",
            "Step [50/390]\t Loss: 0.654108464717865\n",
            "Step [100/390]\t Loss: 0.5776183009147644\n",
            "Step [150/390]\t Loss: 0.6609622836112976\n",
            "Step [200/390]\t Loss: 0.5561428666114807\n",
            "Step [250/390]\t Loss: 0.7094780206680298\n",
            "Step [300/390]\t Loss: 0.6527418494224548\n",
            "Step [350/390]\t Loss: 0.7174040079116821\n",
            "Train Epoch [83]\t Average loss: 0.5895208978500122\n",
            "Test Epoch [83]\t Accuracy: 76.66\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.5837588310241699\n",
            "Step [50/390]\t Loss: 0.523815929889679\n",
            "Step [100/390]\t Loss: 0.628432035446167\n",
            "Step [150/390]\t Loss: 0.5989399552345276\n",
            "Step [200/390]\t Loss: 0.6263843178749084\n",
            "Step [250/390]\t Loss: 0.7015272974967957\n",
            "Step [300/390]\t Loss: 0.5667322874069214\n",
            "Step [350/390]\t Loss: 0.49040335416793823\n",
            "Train Epoch [84]\t Average loss: 0.5895688633124033\n",
            "Test Epoch [84]\t Accuracy: 76.55\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.5536993741989136\n",
            "Step [50/390]\t Loss: 0.6021267771720886\n",
            "Step [100/390]\t Loss: 0.6566773653030396\n",
            "Step [150/390]\t Loss: 0.6447917222976685\n",
            "Step [200/390]\t Loss: 0.616313099861145\n",
            "Step [250/390]\t Loss: 0.6263723373413086\n",
            "Step [300/390]\t Loss: 0.5793323516845703\n",
            "Step [350/390]\t Loss: 0.594376802444458\n",
            "Train Epoch [85]\t Average loss: 0.5893808047740887\n",
            "Test Epoch [85]\t Accuracy: 76.72\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.46430277824401855\n",
            "Step [50/390]\t Loss: 0.6239637136459351\n",
            "Step [100/390]\t Loss: 0.6696171760559082\n",
            "Step [150/390]\t Loss: 0.6829316020011902\n",
            "Step [200/390]\t Loss: 0.7097480297088623\n",
            "Step [250/390]\t Loss: 0.5235865712165833\n",
            "Step [300/390]\t Loss: 0.5636693239212036\n",
            "Step [350/390]\t Loss: 0.6430769562721252\n",
            "Train Epoch [86]\t Average loss: 0.5892624071775339\n",
            "Test Epoch [86]\t Accuracy: 76.74\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.6533119082450867\n",
            "Step [50/390]\t Loss: 0.526686429977417\n",
            "Step [100/390]\t Loss: 0.691324770450592\n",
            "Step [150/390]\t Loss: 0.5616356134414673\n",
            "Step [200/390]\t Loss: 0.5641897320747375\n",
            "Step [250/390]\t Loss: 0.39963456988334656\n",
            "Step [300/390]\t Loss: 0.5221526026725769\n",
            "Step [350/390]\t Loss: 0.5843926668167114\n",
            "Train Epoch [87]\t Average loss: 0.5893817614286374\n",
            "Test Epoch [87]\t Accuracy: 76.73\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.5156103372573853\n",
            "Step [50/390]\t Loss: 0.5843901038169861\n",
            "Step [100/390]\t Loss: 0.7838330268859863\n",
            "Step [150/390]\t Loss: 0.6176444888114929\n",
            "Step [200/390]\t Loss: 0.45509785413742065\n",
            "Step [250/390]\t Loss: 0.514262318611145\n",
            "Step [300/390]\t Loss: 0.7134262323379517\n",
            "Step [350/390]\t Loss: 0.688506543636322\n",
            "Train Epoch [88]\t Average loss: 0.5893876768075503\n",
            "Test Epoch [88]\t Accuracy: 76.58\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.45627591013908386\n",
            "Step [50/390]\t Loss: 0.5489979982376099\n",
            "Step [100/390]\t Loss: 0.5849988460540771\n",
            "Step [150/390]\t Loss: 0.5830041766166687\n",
            "Step [200/390]\t Loss: 0.5803692936897278\n",
            "Step [250/390]\t Loss: 0.5517842769622803\n",
            "Step [300/390]\t Loss: 0.4784044325351715\n",
            "Step [350/390]\t Loss: 0.5172672271728516\n",
            "Train Epoch [89]\t Average loss: 0.5895669305171722\n",
            "Test Epoch [89]\t Accuracy: 76.71\t Best Accuracy: 76.81\n",
            "\n",
            "Step [0/390]\t Loss: 0.5363240242004395\n",
            "Step [50/390]\t Loss: 0.6919686794281006\n",
            "Step [100/390]\t Loss: 0.548119068145752\n",
            "Step [150/390]\t Loss: 0.6063926815986633\n",
            "Step [200/390]\t Loss: 0.6143783330917358\n",
            "Step [250/390]\t Loss: 0.6725094318389893\n",
            "Step [300/390]\t Loss: 0.568649411201477\n",
            "Step [350/390]\t Loss: 0.7244951128959656\n",
            "Train Epoch [90]\t Average loss: 0.5895601742542707\n",
            "Test Epoch [90]\t Accuracy: 76.86\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.6542734503746033\n",
            "Step [50/390]\t Loss: 0.640774130821228\n",
            "Step [100/390]\t Loss: 0.5311067700386047\n",
            "Step [150/390]\t Loss: 0.6424540281295776\n",
            "Step [200/390]\t Loss: 0.6826491355895996\n",
            "Step [250/390]\t Loss: 0.46349844336509705\n",
            "Step [300/390]\t Loss: 0.5367924571037292\n",
            "Step [350/390]\t Loss: 0.5987370610237122\n",
            "Train Epoch [91]\t Average loss: 0.5894498205337769\n",
            "Test Epoch [91]\t Accuracy: 76.67\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.5536301136016846\n",
            "Step [50/390]\t Loss: 0.5946767926216125\n",
            "Step [100/390]\t Loss: 0.5028148293495178\n",
            "Step [150/390]\t Loss: 0.6579766273498535\n",
            "Step [200/390]\t Loss: 0.6244349479675293\n",
            "Step [250/390]\t Loss: 0.6963824033737183\n",
            "Step [300/390]\t Loss: 0.5626207590103149\n",
            "Step [350/390]\t Loss: 0.7453648447990417\n",
            "Train Epoch [92]\t Average loss: 0.5894736327421971\n",
            "Test Epoch [92]\t Accuracy: 76.72\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.604418158531189\n",
            "Step [50/390]\t Loss: 0.5566442012786865\n",
            "Step [100/390]\t Loss: 0.557109534740448\n",
            "Step [150/390]\t Loss: 0.5490322113037109\n",
            "Step [200/390]\t Loss: 0.5865542888641357\n",
            "Step [250/390]\t Loss: 0.6152137517929077\n",
            "Step [300/390]\t Loss: 0.4715788662433624\n",
            "Step [350/390]\t Loss: 0.6730924844741821\n",
            "Train Epoch [93]\t Average loss: 0.5895178795625002\n",
            "Test Epoch [93]\t Accuracy: 76.62\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.5886583924293518\n",
            "Step [50/390]\t Loss: 0.5526406168937683\n",
            "Step [100/390]\t Loss: 0.604586660861969\n",
            "Step [150/390]\t Loss: 0.4820663332939148\n",
            "Step [200/390]\t Loss: 0.5952167510986328\n",
            "Step [250/390]\t Loss: 0.5046940445899963\n",
            "Step [300/390]\t Loss: 0.5134879350662231\n",
            "Step [350/390]\t Loss: 0.4380488991737366\n",
            "Train Epoch [94]\t Average loss: 0.5894262688282209\n",
            "Test Epoch [94]\t Accuracy: 76.73\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.5241267085075378\n",
            "Step [50/390]\t Loss: 0.6328041553497314\n",
            "Step [100/390]\t Loss: 0.5831398367881775\n",
            "Step [150/390]\t Loss: 0.5769459009170532\n",
            "Step [200/390]\t Loss: 0.7042166590690613\n",
            "Step [250/390]\t Loss: 0.6407613754272461\n",
            "Step [300/390]\t Loss: 0.5842617154121399\n",
            "Step [350/390]\t Loss: 0.6749643087387085\n",
            "Train Epoch [95]\t Average loss: 0.5896395053618994\n",
            "Test Epoch [95]\t Accuracy: 76.74\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.6259623169898987\n",
            "Step [50/390]\t Loss: 0.5214112997055054\n",
            "Step [100/390]\t Loss: 0.529519259929657\n",
            "Step [150/390]\t Loss: 0.5508737564086914\n",
            "Step [200/390]\t Loss: 0.6799787282943726\n",
            "Step [250/390]\t Loss: 0.6726745367050171\n",
            "Step [300/390]\t Loss: 0.5387298464775085\n",
            "Step [350/390]\t Loss: 0.553026556968689\n",
            "Train Epoch [96]\t Average loss: 0.5895243618732844\n",
            "Test Epoch [96]\t Accuracy: 76.65\t Best Accuracy: 76.86\n",
            "\n",
            "Step [0/390]\t Loss: 0.6873167753219604\n",
            "Step [50/390]\t Loss: 0.82159823179245\n",
            "Step [100/390]\t Loss: 0.6287875771522522\n",
            "Step [150/390]\t Loss: 0.5932126045227051\n",
            "Step [200/390]\t Loss: 0.524861752986908\n",
            "Step [250/390]\t Loss: 0.5632654428482056\n",
            "Step [300/390]\t Loss: 0.47349897027015686\n",
            "Step [350/390]\t Loss: 0.8470627665519714\n",
            "Train Epoch [97]\t Average loss: 0.5894535545355234\n",
            "Test Epoch [97]\t Accuracy: 76.91\t Best Accuracy: 76.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.6334785223007202\n",
            "Step [50/390]\t Loss: 0.5655378699302673\n",
            "Step [100/390]\t Loss: 0.5306366682052612\n",
            "Step [150/390]\t Loss: 0.5699227452278137\n",
            "Step [200/390]\t Loss: 0.47291719913482666\n",
            "Step [250/390]\t Loss: 0.45913663506507874\n",
            "Step [300/390]\t Loss: 0.595633327960968\n",
            "Step [350/390]\t Loss: 0.4531134068965912\n",
            "Train Epoch [98]\t Average loss: 0.5895075215743139\n",
            "Test Epoch [98]\t Accuracy: 76.59\t Best Accuracy: 76.91\n",
            "\n",
            "Step [0/390]\t Loss: 0.8110067844390869\n",
            "Step [50/390]\t Loss: 0.5989535450935364\n",
            "Step [100/390]\t Loss: 0.5435576438903809\n",
            "Step [150/390]\t Loss: 0.5139948725700378\n",
            "Step [200/390]\t Loss: 0.4827497601509094\n",
            "Step [250/390]\t Loss: 0.6031028032302856\n",
            "Step [300/390]\t Loss: 0.696416974067688\n",
            "Step [350/390]\t Loss: 0.6118746399879456\n",
            "Train Epoch [99]\t Average loss: 0.5893077090000495\n",
            "Test Epoch [99]\t Accuracy: 76.77\t Best Accuracy: 76.91\n",
            "\n",
            "Final Best Accuracy: 76.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.argv=['']\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='SimCLR')\n",
        "parser.add_argument('--model', default=\"RC\", type=str, help=\"orig/RC/LBE/IP/MIB\")\n",
        "parser.add_argument('--batch_size', default=256, type=int, metavar='B', help='training batch size')\n",
        "parser.add_argument('--workers', default=2, type=int, help='workers')\n",
        "parser.add_argument('--epochs', default=30, type=int, help='epochs')\n",
        "parser.add_argument('--save_freq', default=20, type=int, help='save frequency')\n",
        "parser.add_argument('--resnet', default=\"resnet18\", type=str, help=\"resnet18/resnet34/resnet50/resnet101/resnet152\")\n",
        "parser.add_argument('--normalize', default=True, action='store_true', help='normalize')\n",
        "parser.add_argument('--projection_dim', default=128, type=int, help='projection_dim')\n",
        "parser.add_argument('--lamb', default=1., type=float, help='weight of regularization term')\n",
        "parser.add_argument('--zeta', default=0.1, type=float, help='variance')\n",
        "parser.add_argument('--optimizer', default=\"Adam\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--lr', default=3e-4, type=float, help='lr')\n",
        "parser.add_argument('--weight_decay', default=1e-6, type=float, help='weight_decay')\n",
        "parser.add_argument('--temperature', default=0.5, type=float, help='temperature')\n",
        "parser.add_argument('--gpus', default=8, type=int, help='number of gpu')\n",
        "parser.add_argument('--model_dir', default='output/checkpoint/', type=str, help='model save path')\n",
        "parser.add_argument('--dataset', default='CIFAR10', help='[CIFAR10, CIFAR100, ImageNet, STL-10]')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def train(train_loader, model, recon, criterion, optimizer):\n",
        "    loss_epoch = 0.\n",
        "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "        x_i, x_j = x_i.cuda(), x_j.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        if args.model == 'orig':\n",
        "            _, z_i = model(x_i)\n",
        "            _, z_j = model(x_j)\n",
        "            loss = criterion(z_i, z_j)\n",
        "        elif args.model == 'RC':\n",
        "            h_i, z_i = model(x_i)\n",
        "            h_j, z_j = model(x_j)\n",
        "            recon_loss = F.mse_loss(recon(h_i), x_i) + F.mse_loss(recon(h_j), x_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * recon_loss\n",
        "        elif args.model == 'LBE':\n",
        "            mu2_i, mu3_i, mu4_i, h2_i, h3_i, h4_i, z_i = model(x_i)\n",
        "            mu2_j, mu3_j, mu4_j, h2_j, h3_j, h4_j, z_j = model(x_j)\n",
        "            mu2, h2 = torch.cat([mu2_i, mu2_j], dim=0), torch.cat([h2_i, h2_j], dim=0)\n",
        "            mu3, h3 = torch.cat([mu3_i, mu3_j], dim=0), torch.cat([h3_i, h3_j], dim=0)\n",
        "            mu4, h4 = torch.cat([mu4_i, mu4_j], dim=0), torch.cat([h4_i, h4_j], dim=0)\n",
        "            if args.dataset == \"ImageNet\":\n",
        "                MI_estimitor = InfoNCE(mu4, h4)\n",
        "            else:\n",
        "                MI_estimitor = 0.25 * InfoNCE(mu2, h2) + 0.50 * InfoNCE(mu3, h3) + InfoNCE(mu4, h4)\n",
        "            loss = criterion(z_i, z_j) - args.lamb * MI_estimitor\n",
        "        elif args.model == 'IP':\n",
        "            h_i, z_i = model(x_i)\n",
        "            h_j, z_j = model(x_j)\n",
        "            IP = F.mse_loss(h_i, h_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * IP\n",
        "        elif args.model == 'MIB':\n",
        "            mu_i, _, z_i = model(x_i)\n",
        "            mu_j, _, z_j = model(x_j)\n",
        "            MIB = F.mse_loss(mu_i, mu_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * MIB\n",
        "        else:\n",
        "            assert False\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
        "        loss_epoch += loss.item()\n",
        "    return loss_epoch\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = 'non_imagenet'\n",
        "    root = \"datasets\"\n",
        "    if args.dataset == \"CIFAR10\":\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root, download=True, transform=Transforms(32))\n",
        "    elif args.dataset == \"CIFAR100\":\n",
        "        train_dataset = torchvision.datasets.CIFAR100(root, download=True, transform=Transforms(32))\n",
        "    elif args.dataset == \"STL-10\":\n",
        "        train_dataset = torchvision.datasets.STL10(root, split='unlabeled', download=True, transform=Transforms(64))\n",
        "    elif args.dataset == \"ImageNet\":\n",
        "        traindir = os.path.join(root, 'ImageNet/train')\n",
        "        train_dataset = torchvision.datasets.ImageFolder(traindir, Transforms_imagenet(size=224))\n",
        "        data = 'imagenet'\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.workers,\n",
        "        sampler=None)\n",
        "\n",
        "    log_dir = \"output/log/\" + args.dataset + '_%s/'%args.model\n",
        "    if not os.path.isdir(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    suffix = args.dataset + '_{}_batch_{}'.format(args.resnet, args.batch_size)\n",
        "    suffix = suffix + '_proj_dim_{}'.format(args.projection_dim)\n",
        "    test_log_file = open(log_dir + suffix + '.txt', \"w\")\n",
        "\n",
        "    model, recon, optimizer, scheduler = load_model(args, data=data)\n",
        "    if args.dataset=='ImageNet':\n",
        "        model = torch.nn.DataParallel(model, device_ids=list(range(args.gpus)))\n",
        "    args.model_dir = args.model_dir + args.dataset + '_%s/'%args.model\n",
        "    if not os.path.isdir(args.model_dir):\n",
        "        os.makedirs(args.model_dir)\n",
        "            \n",
        "    mask = mask_correlated_samples(args.batch_size)\n",
        "    criterion = NT_Xent(args.batch_size, args.temperature, mask)\n",
        "    for epoch in range(args.epochs):\n",
        "        loss_epoch = train(train_loader, model, recon, criterion, optimizer)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        if (epoch+1) % args.save_freq == 0:\n",
        "            save_model(args.model_dir+suffix, model, epoch+1)\n",
        "\n",
        "        print('Epoch {} loss: {}\\n'.format(epoch, loss_epoch / len(train_loader)))\n",
        "        print('Epoch {} loss: {}'.format(epoch, loss_epoch/len(train_loader)), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "\n",
        "    save_model(args.model_dir+suffix, model, args.epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8O6UYbWaeem7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd64c4fe-4d14-4481-c006-03676a8191be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Step [0/195]\t Loss: 14.195569038391113\n",
            "Step [50/195]\t Loss: 6.427123069763184\n",
            "Step [100/195]\t Loss: 6.017059326171875\n",
            "Step [150/195]\t Loss: 5.786752223968506\n",
            "Epoch 0 loss: 6.272327550252279\n",
            "\n",
            "Step [0/195]\t Loss: 5.779562473297119\n",
            "Step [50/195]\t Loss: 5.709253311157227\n",
            "Step [100/195]\t Loss: 5.617240905761719\n",
            "Step [150/195]\t Loss: 5.553275108337402\n",
            "Epoch 1 loss: 5.621863714853922\n",
            "\n",
            "Step [0/195]\t Loss: 5.462712287902832\n",
            "Step [50/195]\t Loss: 5.436542987823486\n",
            "Step [100/195]\t Loss: 5.484389305114746\n",
            "Step [150/195]\t Loss: 5.381921291351318\n",
            "Epoch 2 loss: 5.447281529353215\n",
            "\n",
            "Step [0/195]\t Loss: 5.358564853668213\n",
            "Step [50/195]\t Loss: 5.336451530456543\n",
            "Step [100/195]\t Loss: 5.297582626342773\n",
            "Step [150/195]\t Loss: 5.3031086921691895\n",
            "Epoch 3 loss: 5.3363340989137305\n",
            "\n",
            "Step [0/195]\t Loss: 5.346673488616943\n",
            "Step [50/195]\t Loss: 5.231128215789795\n",
            "Step [100/195]\t Loss: 5.211153030395508\n",
            "Step [150/195]\t Loss: 5.279646873474121\n",
            "Epoch 4 loss: 5.273751158592028\n",
            "\n",
            "Step [0/195]\t Loss: 5.285140514373779\n",
            "Step [50/195]\t Loss: 5.280045509338379\n",
            "Step [100/195]\t Loss: 5.206981658935547\n",
            "Step [150/195]\t Loss: 5.142021656036377\n",
            "Epoch 5 loss: 5.220987363962027\n",
            "\n",
            "Step [0/195]\t Loss: 5.138848304748535\n",
            "Step [50/195]\t Loss: 5.244357109069824\n",
            "Step [100/195]\t Loss: 5.153312683105469\n",
            "Step [150/195]\t Loss: 5.12645959854126\n",
            "Epoch 6 loss: 5.178464366228153\n",
            "\n",
            "Step [0/195]\t Loss: 5.114477157592773\n",
            "Step [50/195]\t Loss: 5.2213521003723145\n",
            "Step [100/195]\t Loss: 5.170894145965576\n",
            "Step [150/195]\t Loss: 5.144347667694092\n",
            "Epoch 7 loss: 5.147619782961332\n",
            "\n",
            "Step [0/195]\t Loss: 5.117980003356934\n",
            "Step [50/195]\t Loss: 5.167557239532471\n",
            "Step [100/195]\t Loss: 5.188151836395264\n",
            "Step [150/195]\t Loss: 5.1383538246154785\n",
            "Epoch 8 loss: 5.124963833735539\n",
            "\n",
            "Step [0/195]\t Loss: 5.088986396789551\n",
            "Step [50/195]\t Loss: 5.089460849761963\n",
            "Step [100/195]\t Loss: 5.05831241607666\n",
            "Step [150/195]\t Loss: 5.162691593170166\n",
            "Epoch 9 loss: 5.1034412677471455\n",
            "\n",
            "Step [0/195]\t Loss: 5.088475704193115\n",
            "Step [50/195]\t Loss: 5.068645477294922\n",
            "Step [100/195]\t Loss: 5.102735996246338\n",
            "Step [150/195]\t Loss: 5.03842306137085\n",
            "Epoch 10 loss: 5.084142709389711\n",
            "\n",
            "Step [0/195]\t Loss: 5.112415313720703\n",
            "Step [50/195]\t Loss: 5.036533832550049\n",
            "Step [100/195]\t Loss: 5.085102558135986\n",
            "Step [150/195]\t Loss: 5.0531535148620605\n",
            "Epoch 11 loss: 5.07044622470171\n",
            "\n",
            "Step [0/195]\t Loss: 5.114285945892334\n",
            "Step [50/195]\t Loss: 5.062839508056641\n",
            "Step [100/195]\t Loss: 5.047807693481445\n",
            "Step [150/195]\t Loss: 5.0753493309021\n",
            "Epoch 12 loss: 5.052771257742857\n",
            "\n",
            "Step [0/195]\t Loss: 5.02145528793335\n",
            "Step [50/195]\t Loss: 4.977784633636475\n",
            "Step [100/195]\t Loss: 4.986318588256836\n",
            "Step [150/195]\t Loss: 5.025811672210693\n",
            "Epoch 13 loss: 5.041964433132073\n",
            "\n",
            "Step [0/195]\t Loss: 5.04205322265625\n",
            "Step [50/195]\t Loss: 5.027892112731934\n",
            "Step [100/195]\t Loss: 5.029505252838135\n",
            "Step [150/195]\t Loss: 5.065061092376709\n",
            "Epoch 14 loss: 5.027755984281883\n",
            "\n",
            "Step [0/195]\t Loss: 4.9568939208984375\n",
            "Step [50/195]\t Loss: 5.00700569152832\n",
            "Step [100/195]\t Loss: 5.0195183753967285\n",
            "Step [150/195]\t Loss: 5.0326642990112305\n",
            "Epoch 15 loss: 5.01721997872377\n",
            "\n",
            "Step [0/195]\t Loss: 4.988030910491943\n",
            "Step [50/195]\t Loss: 5.0508599281311035\n",
            "Step [100/195]\t Loss: 5.032516002655029\n",
            "Step [150/195]\t Loss: 4.996872425079346\n",
            "Epoch 16 loss: 5.013136528699826\n",
            "\n",
            "Step [0/195]\t Loss: 5.033740043640137\n",
            "Step [50/195]\t Loss: 5.030969142913818\n",
            "Step [100/195]\t Loss: 4.9732232093811035\n",
            "Step [150/195]\t Loss: 4.942826271057129\n",
            "Epoch 17 loss: 5.004942267980331\n",
            "\n",
            "Step [0/195]\t Loss: 5.043178081512451\n",
            "Step [50/195]\t Loss: 4.959558010101318\n",
            "Step [100/195]\t Loss: 4.998013973236084\n",
            "Step [150/195]\t Loss: 4.9810380935668945\n",
            "Epoch 18 loss: 4.991191744193053\n",
            "\n",
            "Step [0/195]\t Loss: 4.993529319763184\n",
            "Step [50/195]\t Loss: 4.97589111328125\n",
            "Step [100/195]\t Loss: 4.932163715362549\n",
            "Step [150/195]\t Loss: 4.960017681121826\n",
            "Epoch 19 loss: 4.98416594236325\n",
            "\n",
            "Step [0/195]\t Loss: 4.965169906616211\n",
            "Step [50/195]\t Loss: 4.9685773849487305\n",
            "Step [100/195]\t Loss: 5.016506671905518\n",
            "Step [150/195]\t Loss: 4.942685127258301\n",
            "Epoch 20 loss: 4.975762367248535\n",
            "\n",
            "Step [0/195]\t Loss: 4.956470966339111\n",
            "Step [50/195]\t Loss: 4.968203067779541\n",
            "Step [100/195]\t Loss: 4.9525980949401855\n",
            "Step [150/195]\t Loss: 4.95880651473999\n",
            "Epoch 21 loss: 4.9693294378427355\n",
            "\n",
            "Step [0/195]\t Loss: 4.93389368057251\n",
            "Step [50/195]\t Loss: 4.972876071929932\n",
            "Step [100/195]\t Loss: 4.9077372550964355\n",
            "Step [150/195]\t Loss: 4.981179237365723\n",
            "Epoch 22 loss: 4.965312434465457\n",
            "\n",
            "Step [0/195]\t Loss: 4.938472747802734\n",
            "Step [50/195]\t Loss: 4.979705810546875\n",
            "Step [100/195]\t Loss: 4.951186656951904\n",
            "Step [150/195]\t Loss: 4.989525318145752\n",
            "Epoch 23 loss: 4.957261332487449\n",
            "\n",
            "Step [0/195]\t Loss: 4.989230632781982\n",
            "Step [50/195]\t Loss: 4.942346096038818\n",
            "Step [100/195]\t Loss: 4.955425262451172\n",
            "Step [150/195]\t Loss: 4.894583225250244\n",
            "Epoch 24 loss: 4.9509335346710985\n",
            "\n",
            "Step [0/195]\t Loss: 4.905701160430908\n",
            "Step [50/195]\t Loss: 4.95409631729126\n",
            "Step [100/195]\t Loss: 4.921967506408691\n",
            "Step [150/195]\t Loss: 4.962794780731201\n",
            "Epoch 25 loss: 4.943993898538443\n",
            "\n",
            "Step [0/195]\t Loss: 4.909534931182861\n",
            "Step [50/195]\t Loss: 4.943410396575928\n",
            "Step [100/195]\t Loss: 4.96630334854126\n",
            "Step [150/195]\t Loss: 4.933066368103027\n",
            "Epoch 26 loss: 4.940752508701422\n",
            "\n",
            "Step [0/195]\t Loss: 4.946369647979736\n",
            "Step [50/195]\t Loss: 4.922682762145996\n",
            "Step [100/195]\t Loss: 4.883431434631348\n",
            "Step [150/195]\t Loss: 4.959228038787842\n",
            "Epoch 27 loss: 4.93480636645586\n",
            "\n",
            "Step [0/195]\t Loss: 4.978270530700684\n",
            "Step [50/195]\t Loss: 4.912802219390869\n",
            "Step [100/195]\t Loss: 4.881874084472656\n",
            "Step [150/195]\t Loss: 4.94386100769043\n",
            "Epoch 28 loss: 4.927824722192226\n",
            "\n",
            "Step [0/195]\t Loss: 4.925095081329346\n",
            "Step [50/195]\t Loss: 4.9437737464904785\n",
            "Step [100/195]\t Loss: 4.966033458709717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5AJn2CfIk9aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNFxbA8Xk-TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='linear Evaluation')\n",
        "parser.add_argument('--model', default=\"RC\", type=str, help=\"orig/RC/LBE/IP/MIB\")\n",
        "parser.add_argument('--logistic_batch_size', default=128, type=int, metavar='B', help='logistic_batch_size batch size')\n",
        "parser.add_argument('--logistic_epochs', default=100, type=int, help='logistic_epochs')\n",
        "parser.add_argument('--batch_size', default=256, type=int, metavar='B', help='training batch size')\n",
        "parser.add_argument('--workers', default=2, type=int, help='workers')\n",
        "parser.add_argument('--epochs', default=30, type=int, help='epochs')\n",
        "parser.add_argument('--resnet', default=\"resnet18\", type=str, help=\"resnet18/resnet34/resnet50/resnet101/resnet152\")\n",
        "parser.add_argument('--normalize', default=True, action='store_true', help='normalize')\n",
        "parser.add_argument('--projection_dim', default=128, type=int,help='projection_dim')\n",
        "parser.add_argument('--lamb', default=1., type=float, help='weight of regularization term')\n",
        "parser.add_argument('--optimizer', default=\"Adam\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--weight_decay', default=1e-6, type=float, help='weight_decay')\n",
        "parser.add_argument('--lr', default=3e-4, type=float, help='lr')\n",
        "parser.add_argument('--temperature', default=0.5, type=float, help='temperature')\n",
        "parser.add_argument('--model_dir', default='output/checkpoint/', type=str, help='model save path')\n",
        "parser.add_argument('--root', default=\"../datasets\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--dataset', default='CIFAR10', help='[CIFAR10, CIFAR100, STL-10]')\n",
        "parser.add_argument('--testset', default='CIFAR10', help='[CIFAR10, CIFAR100, STL-10, aircraft, cu_birds, dtd, fashionmnist, mnist, traffic_sign, vgg_flower]')\n",
        "args = parser.parse_args()\n",
        "\n",
        "def train(loader, simclr_model, model, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    model.train()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            if args.model == 'LBE':\n",
        "                _, _, h, _, _, _, _ = simclr_model(x.cuda())\n",
        "            elif args.model == 'MIB':\n",
        "                h, _, _ = simclr_model(x.cuda())\n",
        "            else:\n",
        "                h, _ = simclr_model(x.cuda())\n",
        "        output = model(h)\n",
        "        loss = criterion(output, y.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()}\")\n",
        "    return loss_epoch\n",
        "\n",
        "def test(loader, simclr_model, model):\n",
        "    right_num = 0\n",
        "    all_num = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "            if args.model == 'LBE':\n",
        "                _, _, h, _, _, _, _ = simclr_model(x)\n",
        "            elif args.model == 'MIB':\n",
        "                h, _, _ = simclr_model(x)\n",
        "            else:\n",
        "                h, _ = simclr_model(x)\n",
        "            output = model(h)\n",
        "\n",
        "            predicted = output.argmax(1)\n",
        "            right_num += (predicted == y).sum().item()\n",
        "            all_num += y.size(0)\n",
        "    accuracy = right_num*100./all_num\n",
        "    return accuracy\n",
        "\n",
        "def load_transform(dataset, size=32):\n",
        "    mean, std = get_data_mean_and_stdev(dataset)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)])\n",
        "    return transform\n",
        "\n",
        "def main():\n",
        "    data = 'non_imagenet'\n",
        "    if args.testset == \"CIFAR10\":\n",
        "        train_dataset = torchvision.datasets.CIFAR10(args.root, train=True, download=True, transform=load_transform('CIFAR10', 32))\n",
        "        test_dataset = torchvision.datasets.CIFAR10(args.root, train=False, download=True, transform=load_transform('CIFAR10', 32))\n",
        "    elif args.testset == \"CIFAR100\":\n",
        "        train_dataset = torchvision.datasets.CIFAR100(args.root, train=True, download=True, transform=load_transform('CIFAR100', 32))\n",
        "        test_dataset = torchvision.datasets.CIFAR100(args.root, train=False, download=True, transform=load_transform('CIFAR100', 32))\n",
        "    elif args.testset == \"STL-10\":\n",
        "        train_dataset = torchvision.datasets.STL10(args.root, split='train', download=True, transform=load_transform('STL-10', 96))\n",
        "        test_dataset = torchvision.datasets.STL10(args.root, split='test', download=True, transform=load_transform('STL-10', 96))\n",
        "    else:\n",
        "        if args.dataset=='STL-10':\n",
        "            train_dataset = DATASET[args.testset](train=True, image_transforms=load_transform(args.testset, 64))\n",
        "            test_dataset = DATASET[args.testset](train=False, image_transforms=load_transform(args.testset, 64))\n",
        "        else:\n",
        "            train_dataset = DATASET[args.testset](train=True, image_transforms=load_transform(args.testset, 32))\n",
        "            test_dataset = DATASET[args.testset](train=False, image_transforms=load_transform(args.testset, 32))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.logistic_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.logistic_batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=args.workers)\n",
        "\n",
        "    log_dir = \"output/log/\" + args.testset + '_%s/'%args.model\n",
        "    if not os.path.isdir(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    suffix = args.dataset + '_{}_batch_{}'.format(args.resnet, args.batch_size)\n",
        "    suffix = suffix + '_proj_dim_{}'.format(args.projection_dim) + '_epoch_%d'%args.epochs\n",
        "    args.model_dir = args.model_dir + args.dataset + '_%s/'%args.model\n",
        "    epoch_dir = args.model_dir + suffix + '.pt'\n",
        "    print(\"Loading {}\".format(epoch_dir))\n",
        "    \n",
        "    simclr_model, _, _, _ = load_model(args, reload_model=True, load_path=epoch_dir, data=data)\n",
        "    simclr_model = simclr_model.cuda()\n",
        "    simclr_model.eval()\n",
        "\n",
        "    # Logistic Regression\n",
        "    n_classes = get_data_nclass(args.testset)\n",
        "    model = LogisticRegression(simclr_model.n_features, n_classes).cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [60, 80], gamma=0.1)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.\n",
        "    test_log_file = open(log_dir + suffix + '_LR.txt', \"w\")\n",
        "    for epoch in range(args.logistic_epochs):\n",
        "        loss_epoch = train(train_loader, simclr_model, model, criterion, optimizer)\n",
        "        print(\"Train Epoch [{}]\\t Average loss: {}\".format(epoch, loss_epoch/len(train_loader)))\n",
        "        print(\"Train Epoch [{}]\\t Average loss: {}\".format(epoch, loss_epoch/len(train_loader)), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "\n",
        "        # final testing\n",
        "        test_current_acc = test(test_loader, simclr_model, model)\n",
        "        if test_current_acc > best_acc:\n",
        "            best_acc = test_current_acc\n",
        "        print(\"Test Epoch [{}]\\t Accuracy: {}\\t Best Accuracy: {}\\n\".format(epoch, test_current_acc, best_acc))\n",
        "        print(\"Test Epoch [{}]\\t Accuracy: {}\\t Best Accuracy: {}\\n\".format(epoch, test_current_acc, best_acc), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Final Best Accuracy: {}\".format(best_acc))\n",
        "    print(\"Final Best Accuracy: {}\".format(best_acc), file=test_log_file)\n",
        "    test_log_file.flush()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TSVGDGya9kv",
        "outputId": "07813349-2dc2-4fa5-a968-2c24adb8d8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Loading output/checkpoint/CIFAR10_RC/CIFAR10_resnet18_batch_256_proj_dim_128_epoch_30.pt\n",
            "Step [0/390]\t Loss: 2.621952772140503\n",
            "Step [50/390]\t Loss: 0.8526278734207153\n",
            "Step [100/390]\t Loss: 0.9318972229957581\n",
            "Step [150/390]\t Loss: 0.6395511031150818\n",
            "Step [200/390]\t Loss: 0.8330305218696594\n",
            "Step [250/390]\t Loss: 0.8762748837471008\n",
            "Step [300/390]\t Loss: 0.8193378448486328\n",
            "Step [350/390]\t Loss: 0.997359573841095\n",
            "Train Epoch [0]\t Average loss: 0.9155357454067622\n",
            "Test Epoch [0]\t Accuracy: 69.84\t Best Accuracy: 69.84\n",
            "\n",
            "Step [0/390]\t Loss: 0.7429522275924683\n",
            "Step [50/390]\t Loss: 0.7126017212867737\n",
            "Step [100/390]\t Loss: 0.7909466624259949\n",
            "Step [150/390]\t Loss: 0.6902105212211609\n",
            "Step [200/390]\t Loss: 0.7695375084877014\n",
            "Step [250/390]\t Loss: 0.577748715877533\n",
            "Step [300/390]\t Loss: 0.719490647315979\n",
            "Step [350/390]\t Loss: 0.8091475367546082\n",
            "Train Epoch [1]\t Average loss: 0.8128020792435378\n",
            "Test Epoch [1]\t Accuracy: 70.43\t Best Accuracy: 70.43\n",
            "\n",
            "Step [0/390]\t Loss: 0.9748851656913757\n",
            "Step [50/390]\t Loss: 0.7429062128067017\n",
            "Step [100/390]\t Loss: 0.7100218534469604\n",
            "Step [150/390]\t Loss: 1.0413498878479004\n",
            "Step [200/390]\t Loss: 0.8922514319419861\n",
            "Step [250/390]\t Loss: 0.8019378185272217\n",
            "Step [300/390]\t Loss: 0.705535888671875\n",
            "Step [350/390]\t Loss: 0.7726454734802246\n",
            "Train Epoch [2]\t Average loss: 0.7922837997858341\n",
            "Test Epoch [2]\t Accuracy: 72.83\t Best Accuracy: 72.83\n",
            "\n",
            "Step [0/390]\t Loss: 0.6493756771087646\n",
            "Step [50/390]\t Loss: 0.6762661337852478\n",
            "Step [100/390]\t Loss: 0.7963254451751709\n",
            "Step [150/390]\t Loss: 0.7856993079185486\n",
            "Step [200/390]\t Loss: 0.5470960736274719\n",
            "Step [250/390]\t Loss: 0.6223505139350891\n",
            "Step [300/390]\t Loss: 0.853710949420929\n",
            "Step [350/390]\t Loss: 0.6510278582572937\n",
            "Train Epoch [3]\t Average loss: 0.7631732981174421\n",
            "Test Epoch [3]\t Accuracy: 72.25\t Best Accuracy: 72.83\n",
            "\n",
            "Step [0/390]\t Loss: 0.7381103038787842\n",
            "Step [50/390]\t Loss: 0.7576185464859009\n",
            "Step [100/390]\t Loss: 0.6358156800270081\n",
            "Step [150/390]\t Loss: 0.7805315256118774\n",
            "Step [200/390]\t Loss: 0.8037729859352112\n",
            "Step [250/390]\t Loss: 0.763166069984436\n",
            "Step [300/390]\t Loss: 0.7890595197677612\n",
            "Step [350/390]\t Loss: 0.5187748670578003\n",
            "Train Epoch [4]\t Average loss: 0.7528750320275625\n",
            "Test Epoch [4]\t Accuracy: 73.59\t Best Accuracy: 73.59\n",
            "\n",
            "Step [0/390]\t Loss: 0.6806467771530151\n",
            "Step [50/390]\t Loss: 0.6733901500701904\n",
            "Step [100/390]\t Loss: 0.8569909930229187\n",
            "Step [150/390]\t Loss: 0.6837022304534912\n",
            "Step [200/390]\t Loss: 0.6940098404884338\n",
            "Step [250/390]\t Loss: 0.7741763591766357\n",
            "Step [300/390]\t Loss: 1.0110745429992676\n",
            "Step [350/390]\t Loss: 0.8635122179985046\n",
            "Train Epoch [5]\t Average loss: 0.7478062689304352\n",
            "Test Epoch [5]\t Accuracy: 72.27\t Best Accuracy: 73.59\n",
            "\n",
            "Step [0/390]\t Loss: 0.7175694108009338\n",
            "Step [50/390]\t Loss: 0.6809574961662292\n",
            "Step [100/390]\t Loss: 0.6072990894317627\n",
            "Step [150/390]\t Loss: 0.8588343262672424\n",
            "Step [200/390]\t Loss: 0.7384761571884155\n",
            "Step [250/390]\t Loss: 0.684935450553894\n",
            "Step [300/390]\t Loss: 0.8130274415016174\n",
            "Step [350/390]\t Loss: 0.7451810836791992\n",
            "Train Epoch [6]\t Average loss: 0.7444759462888424\n",
            "Test Epoch [6]\t Accuracy: 71.46\t Best Accuracy: 73.59\n",
            "\n",
            "Step [0/390]\t Loss: 0.5886794924736023\n",
            "Step [50/390]\t Loss: 0.6609113812446594\n",
            "Step [100/390]\t Loss: 1.0329087972640991\n",
            "Step [150/390]\t Loss: 0.9663178324699402\n",
            "Step [200/390]\t Loss: 0.6265919208526611\n",
            "Step [250/390]\t Loss: 0.8147394061088562\n",
            "Step [300/390]\t Loss: 0.6594499349594116\n",
            "Step [350/390]\t Loss: 0.7230746746063232\n",
            "Train Epoch [7]\t Average loss: 0.7197721256659582\n",
            "Test Epoch [7]\t Accuracy: 73.51\t Best Accuracy: 73.59\n",
            "\n",
            "Step [0/390]\t Loss: 0.6219154596328735\n",
            "Step [50/390]\t Loss: 0.5770535469055176\n",
            "Step [100/390]\t Loss: 0.5866448283195496\n",
            "Step [150/390]\t Loss: 0.8085751533508301\n",
            "Step [200/390]\t Loss: 0.6140814423561096\n",
            "Step [250/390]\t Loss: 0.6736623644828796\n",
            "Step [300/390]\t Loss: 0.6510573625564575\n",
            "Step [350/390]\t Loss: 0.6756711006164551\n",
            "Train Epoch [8]\t Average loss: 0.7216851469033804\n",
            "Test Epoch [8]\t Accuracy: 74.0\t Best Accuracy: 74.0\n",
            "\n",
            "Step [0/390]\t Loss: 0.566231906414032\n",
            "Step [50/390]\t Loss: 0.8222253918647766\n",
            "Step [100/390]\t Loss: 0.6512714624404907\n",
            "Step [150/390]\t Loss: 0.6192488670349121\n",
            "Step [200/390]\t Loss: 0.6659629940986633\n",
            "Step [250/390]\t Loss: 0.7416089773178101\n",
            "Step [300/390]\t Loss: 0.7433164715766907\n",
            "Step [350/390]\t Loss: 0.5374106168746948\n",
            "Train Epoch [9]\t Average loss: 0.7120964725812277\n",
            "Test Epoch [9]\t Accuracy: 72.68\t Best Accuracy: 74.0\n",
            "\n",
            "Step [0/390]\t Loss: 0.5798184871673584\n",
            "Step [50/390]\t Loss: 0.643830418586731\n",
            "Step [100/390]\t Loss: 0.7359316349029541\n",
            "Step [150/390]\t Loss: 0.6848559975624084\n",
            "Step [200/390]\t Loss: 0.77093505859375\n",
            "Step [250/390]\t Loss: 0.7030627727508545\n",
            "Step [300/390]\t Loss: 0.585028350353241\n",
            "Step [350/390]\t Loss: 0.7822527885437012\n",
            "Train Epoch [10]\t Average loss: 0.7146655259224085\n",
            "Test Epoch [10]\t Accuracy: 73.22\t Best Accuracy: 74.0\n",
            "\n",
            "Step [0/390]\t Loss: 0.7948353886604309\n",
            "Step [50/390]\t Loss: 0.6294673681259155\n",
            "Step [100/390]\t Loss: 0.630415678024292\n",
            "Step [150/390]\t Loss: 0.7644817233085632\n",
            "Step [200/390]\t Loss: 0.6344066858291626\n",
            "Step [250/390]\t Loss: 0.8419469594955444\n",
            "Step [300/390]\t Loss: 0.7952023148536682\n",
            "Step [350/390]\t Loss: 0.7029553651809692\n",
            "Train Epoch [11]\t Average loss: 0.7076808052949416\n",
            "Test Epoch [11]\t Accuracy: 73.21\t Best Accuracy: 74.0\n",
            "\n",
            "Step [0/390]\t Loss: 0.6655698418617249\n",
            "Step [50/390]\t Loss: 0.6739285588264465\n",
            "Step [100/390]\t Loss: 0.7007172107696533\n",
            "Step [150/390]\t Loss: 0.726653516292572\n",
            "Step [200/390]\t Loss: 0.6600950956344604\n",
            "Step [250/390]\t Loss: 0.7147828340530396\n",
            "Step [300/390]\t Loss: 0.6226824522018433\n",
            "Step [350/390]\t Loss: 0.7333232164382935\n",
            "Train Epoch [12]\t Average loss: 0.7007498470636514\n",
            "Test Epoch [12]\t Accuracy: 74.06\t Best Accuracy: 74.06\n",
            "\n",
            "Step [0/390]\t Loss: 0.670373797416687\n",
            "Step [50/390]\t Loss: 0.7652652263641357\n",
            "Step [100/390]\t Loss: 0.7855162620544434\n",
            "Step [150/390]\t Loss: 0.7574320435523987\n",
            "Step [200/390]\t Loss: 0.7127621173858643\n",
            "Step [250/390]\t Loss: 0.6654030084609985\n",
            "Step [300/390]\t Loss: 0.6433281302452087\n",
            "Step [350/390]\t Loss: 0.6319122910499573\n",
            "Train Epoch [13]\t Average loss: 0.69183199489728\n",
            "Test Epoch [13]\t Accuracy: 74.44\t Best Accuracy: 74.44\n",
            "\n",
            "Step [0/390]\t Loss: 0.6287350058555603\n",
            "Step [50/390]\t Loss: 0.6671872735023499\n",
            "Step [100/390]\t Loss: 0.8673866391181946\n",
            "Step [150/390]\t Loss: 0.8105005025863647\n",
            "Step [200/390]\t Loss: 0.7064524292945862\n",
            "Step [250/390]\t Loss: 0.6907103061676025\n",
            "Step [300/390]\t Loss: 0.6710290908813477\n",
            "Step [350/390]\t Loss: 0.7460252046585083\n",
            "Train Epoch [14]\t Average loss: 0.6972103364192522\n",
            "Test Epoch [14]\t Accuracy: 71.34\t Best Accuracy: 74.44\n",
            "\n",
            "Step [0/390]\t Loss: 0.7531291842460632\n",
            "Step [50/390]\t Loss: 0.7912768721580505\n",
            "Step [100/390]\t Loss: 0.7162684798240662\n",
            "Step [150/390]\t Loss: 0.6925569772720337\n",
            "Step [200/390]\t Loss: 0.7736390233039856\n",
            "Step [250/390]\t Loss: 0.7498111724853516\n",
            "Step [300/390]\t Loss: 0.6852044463157654\n",
            "Step [350/390]\t Loss: 0.6659340858459473\n",
            "Train Epoch [15]\t Average loss: 0.691464555187103\n",
            "Test Epoch [15]\t Accuracy: 73.81\t Best Accuracy: 74.44\n",
            "\n",
            "Step [0/390]\t Loss: 0.6716611385345459\n",
            "Step [50/390]\t Loss: 0.801572322845459\n",
            "Step [100/390]\t Loss: 0.8149107694625854\n",
            "Step [150/390]\t Loss: 0.6212457418441772\n",
            "Step [200/390]\t Loss: 0.7517408132553101\n",
            "Step [250/390]\t Loss: 0.8315682411193848\n",
            "Step [300/390]\t Loss: 0.7259959578514099\n",
            "Step [350/390]\t Loss: 0.7450982332229614\n",
            "Train Epoch [16]\t Average loss: 0.6990295122067134\n",
            "Test Epoch [16]\t Accuracy: 75.1\t Best Accuracy: 75.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.6089117527008057\n",
            "Step [50/390]\t Loss: 0.6659960150718689\n",
            "Step [100/390]\t Loss: 0.7088276743888855\n",
            "Step [150/390]\t Loss: 0.5052487850189209\n",
            "Step [200/390]\t Loss: 0.4755600392818451\n",
            "Step [250/390]\t Loss: 0.5644955039024353\n",
            "Step [300/390]\t Loss: 0.5827921628952026\n",
            "Step [350/390]\t Loss: 0.662031888961792\n",
            "Train Epoch [17]\t Average loss: 0.6850773253501989\n",
            "Test Epoch [17]\t Accuracy: 73.96\t Best Accuracy: 75.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.6419268846511841\n",
            "Step [50/390]\t Loss: 0.6594780683517456\n",
            "Step [100/390]\t Loss: 0.689410924911499\n",
            "Step [150/390]\t Loss: 0.6585162281990051\n",
            "Step [200/390]\t Loss: 0.6636189222335815\n",
            "Step [250/390]\t Loss: 0.6652712821960449\n",
            "Step [300/390]\t Loss: 0.8745995759963989\n",
            "Step [350/390]\t Loss: 0.8503130078315735\n",
            "Train Epoch [18]\t Average loss: 0.7014153001400141\n",
            "Test Epoch [18]\t Accuracy: 73.91\t Best Accuracy: 75.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.6773169040679932\n",
            "Step [50/390]\t Loss: 0.5281975865364075\n",
            "Step [100/390]\t Loss: 0.5507688522338867\n",
            "Step [150/390]\t Loss: 0.6188099384307861\n",
            "Step [200/390]\t Loss: 0.7413118481636047\n",
            "Step [250/390]\t Loss: 0.6182637214660645\n",
            "Step [300/390]\t Loss: 0.7748170495033264\n",
            "Step [350/390]\t Loss: 0.821315586566925\n",
            "Train Epoch [19]\t Average loss: 0.6844768449282035\n",
            "Test Epoch [19]\t Accuracy: 75.1\t Best Accuracy: 75.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.644111156463623\n",
            "Step [50/390]\t Loss: 0.887706458568573\n",
            "Step [100/390]\t Loss: 0.7248384356498718\n",
            "Step [150/390]\t Loss: 0.6637074947357178\n",
            "Step [200/390]\t Loss: 0.49654993414878845\n",
            "Step [250/390]\t Loss: 0.6272066235542297\n",
            "Step [300/390]\t Loss: 0.7345409989356995\n",
            "Step [350/390]\t Loss: 0.6824370622634888\n",
            "Train Epoch [20]\t Average loss: 0.6862714454913751\n",
            "Test Epoch [20]\t Accuracy: 73.06\t Best Accuracy: 75.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.7406017780303955\n",
            "Step [50/390]\t Loss: 0.688676655292511\n",
            "Step [100/390]\t Loss: 0.798181414604187\n",
            "Step [150/390]\t Loss: 0.6258265376091003\n",
            "Step [200/390]\t Loss: 0.5499598979949951\n",
            "Step [250/390]\t Loss: 0.6433050632476807\n",
            "Step [300/390]\t Loss: 0.6599217653274536\n",
            "Step [350/390]\t Loss: 0.677655041217804\n",
            "Train Epoch [21]\t Average loss: 0.6738309948872298\n",
            "Test Epoch [21]\t Accuracy: 73.79\t Best Accuracy: 75.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.7643394470214844\n",
            "Step [50/390]\t Loss: 0.8033947944641113\n",
            "Step [100/390]\t Loss: 0.7701568007469177\n",
            "Step [150/390]\t Loss: 0.8702568411827087\n",
            "Step [200/390]\t Loss: 0.8091078996658325\n",
            "Step [250/390]\t Loss: 0.6697161197662354\n",
            "Step [300/390]\t Loss: 0.7451689839363098\n",
            "Step [350/390]\t Loss: 0.68415367603302\n",
            "Train Epoch [22]\t Average loss: 0.6833582580089569\n",
            "Test Epoch [22]\t Accuracy: 75.31\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.7797406315803528\n",
            "Step [50/390]\t Loss: 0.6891120076179504\n",
            "Step [100/390]\t Loss: 0.7271573543548584\n",
            "Step [150/390]\t Loss: 0.6187297105789185\n",
            "Step [200/390]\t Loss: 0.8089974522590637\n",
            "Step [250/390]\t Loss: 0.6542777419090271\n",
            "Step [300/390]\t Loss: 0.5940883755683899\n",
            "Step [350/390]\t Loss: 0.7901122570037842\n",
            "Train Epoch [23]\t Average loss: 0.6804427057504654\n",
            "Test Epoch [23]\t Accuracy: 74.81\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5029658675193787\n",
            "Step [50/390]\t Loss: 0.9015515446662903\n",
            "Step [100/390]\t Loss: 0.7710998058319092\n",
            "Step [150/390]\t Loss: 0.7469021677970886\n",
            "Step [200/390]\t Loss: 0.7235186100006104\n",
            "Step [250/390]\t Loss: 0.6703075170516968\n",
            "Step [300/390]\t Loss: 0.6052452921867371\n",
            "Step [350/390]\t Loss: 0.6295210123062134\n",
            "Train Epoch [24]\t Average loss: 0.6774907576732146\n",
            "Test Epoch [24]\t Accuracy: 74.44\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.8071211576461792\n",
            "Step [50/390]\t Loss: 0.6475405693054199\n",
            "Step [100/390]\t Loss: 0.6657456159591675\n",
            "Step [150/390]\t Loss: 0.636033296585083\n",
            "Step [200/390]\t Loss: 0.6111934185028076\n",
            "Step [250/390]\t Loss: 0.5875695943832397\n",
            "Step [300/390]\t Loss: 0.6616108417510986\n",
            "Step [350/390]\t Loss: 0.6591829061508179\n",
            "Train Epoch [25]\t Average loss: 0.6811744414843046\n",
            "Test Epoch [25]\t Accuracy: 74.53\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.7046880125999451\n",
            "Step [50/390]\t Loss: 0.5875929594039917\n",
            "Step [100/390]\t Loss: 0.7778952121734619\n",
            "Step [150/390]\t Loss: 0.5388321280479431\n",
            "Step [200/390]\t Loss: 0.6558309197425842\n",
            "Step [250/390]\t Loss: 0.6687881350517273\n",
            "Step [300/390]\t Loss: 0.6101868748664856\n",
            "Step [350/390]\t Loss: 0.48632028698921204\n",
            "Train Epoch [26]\t Average loss: 0.675956286298923\n",
            "Test Epoch [26]\t Accuracy: 73.59\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6444154977798462\n",
            "Step [50/390]\t Loss: 0.5291135907173157\n",
            "Step [100/390]\t Loss: 0.3941773772239685\n",
            "Step [150/390]\t Loss: 0.7042039632797241\n",
            "Step [200/390]\t Loss: 0.5750662088394165\n",
            "Step [250/390]\t Loss: 0.8303056955337524\n",
            "Step [300/390]\t Loss: 0.7367163300514221\n",
            "Step [350/390]\t Loss: 0.5890015959739685\n",
            "Train Epoch [27]\t Average loss: 0.6630912083845872\n",
            "Test Epoch [27]\t Accuracy: 74.07\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.725005030632019\n",
            "Step [50/390]\t Loss: 0.7874636054039001\n",
            "Step [100/390]\t Loss: 0.6029518842697144\n",
            "Step [150/390]\t Loss: 0.7034229636192322\n",
            "Step [200/390]\t Loss: 0.6144862771034241\n",
            "Step [250/390]\t Loss: 0.5991387963294983\n",
            "Step [300/390]\t Loss: 0.6418051719665527\n",
            "Step [350/390]\t Loss: 0.6540383696556091\n",
            "Train Epoch [28]\t Average loss: 0.6598045909251922\n",
            "Test Epoch [28]\t Accuracy: 74.85\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6791087985038757\n",
            "Step [50/390]\t Loss: 0.6448941230773926\n",
            "Step [100/390]\t Loss: 0.7760418653488159\n",
            "Step [150/390]\t Loss: 0.7138215899467468\n",
            "Step [200/390]\t Loss: 0.5448076128959656\n",
            "Step [250/390]\t Loss: 0.7939386367797852\n",
            "Step [300/390]\t Loss: 0.6418069005012512\n",
            "Step [350/390]\t Loss: 0.6447175741195679\n",
            "Train Epoch [29]\t Average loss: 0.6734449994105559\n",
            "Test Epoch [29]\t Accuracy: 74.65\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.7443982362747192\n",
            "Step [50/390]\t Loss: 0.8449984192848206\n",
            "Step [100/390]\t Loss: 0.6273348927497864\n",
            "Step [150/390]\t Loss: 0.7625473141670227\n",
            "Step [200/390]\t Loss: 0.7139197587966919\n",
            "Step [250/390]\t Loss: 0.5534062385559082\n",
            "Step [300/390]\t Loss: 0.571579098701477\n",
            "Step [350/390]\t Loss: 0.69105064868927\n",
            "Train Epoch [30]\t Average loss: 0.6688232820767622\n",
            "Test Epoch [30]\t Accuracy: 74.89\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6376557350158691\n",
            "Step [50/390]\t Loss: 0.7267969250679016\n",
            "Step [100/390]\t Loss: 0.6340590119361877\n",
            "Step [150/390]\t Loss: 0.601118803024292\n",
            "Step [200/390]\t Loss: 0.6106157302856445\n",
            "Step [250/390]\t Loss: 0.6220923662185669\n",
            "Step [300/390]\t Loss: 0.8280104398727417\n",
            "Step [350/390]\t Loss: 0.5440369248390198\n",
            "Train Epoch [31]\t Average loss: 0.6673681741341566\n",
            "Test Epoch [31]\t Accuracy: 74.43\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5782314538955688\n",
            "Step [50/390]\t Loss: 0.542633056640625\n",
            "Step [100/390]\t Loss: 0.6701206564903259\n",
            "Step [150/390]\t Loss: 0.581205427646637\n",
            "Step [200/390]\t Loss: 0.4485127627849579\n",
            "Step [250/390]\t Loss: 0.6223965287208557\n",
            "Step [300/390]\t Loss: 0.8688868284225464\n",
            "Step [350/390]\t Loss: 0.9777910113334656\n",
            "Train Epoch [32]\t Average loss: 0.6694316319166086\n",
            "Test Epoch [32]\t Accuracy: 75.21\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.7465714812278748\n",
            "Step [50/390]\t Loss: 0.5765034556388855\n",
            "Step [100/390]\t Loss: 0.6416088342666626\n",
            "Step [150/390]\t Loss: 0.6528037786483765\n",
            "Step [200/390]\t Loss: 0.5438148975372314\n",
            "Step [250/390]\t Loss: 0.7322202920913696\n",
            "Step [300/390]\t Loss: 0.5614578723907471\n",
            "Step [350/390]\t Loss: 0.6344958543777466\n",
            "Train Epoch [33]\t Average loss: 0.6734308637105502\n",
            "Test Epoch [33]\t Accuracy: 73.98\t Best Accuracy: 75.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5901151895523071\n",
            "Step [50/390]\t Loss: 0.612565815448761\n",
            "Step [100/390]\t Loss: 0.584209680557251\n",
            "Step [150/390]\t Loss: 0.6796391010284424\n",
            "Step [200/390]\t Loss: 0.6105195879936218\n",
            "Step [250/390]\t Loss: 0.5604907274246216\n",
            "Step [300/390]\t Loss: 0.5176164507865906\n",
            "Step [350/390]\t Loss: 0.6641528606414795\n",
            "Train Epoch [34]\t Average loss: 0.66775978352779\n",
            "Test Epoch [34]\t Accuracy: 75.95\t Best Accuracy: 75.95\n",
            "\n",
            "Step [0/390]\t Loss: 0.5256052017211914\n",
            "Step [50/390]\t Loss: 0.5315728783607483\n",
            "Step [100/390]\t Loss: 0.6501873731613159\n",
            "Step [150/390]\t Loss: 0.6795651316642761\n",
            "Step [200/390]\t Loss: 0.6597182154655457\n",
            "Step [250/390]\t Loss: 0.6763163208961487\n",
            "Step [300/390]\t Loss: 0.6409266591072083\n",
            "Step [350/390]\t Loss: 0.7071954011917114\n",
            "Train Epoch [35]\t Average loss: 0.670458745268675\n",
            "Test Epoch [35]\t Accuracy: 76.26\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.6107624173164368\n",
            "Step [50/390]\t Loss: 0.6568860411643982\n",
            "Step [100/390]\t Loss: 0.5526508092880249\n",
            "Step [150/390]\t Loss: 0.6751648783683777\n",
            "Step [200/390]\t Loss: 0.5804409980773926\n",
            "Step [250/390]\t Loss: 0.4627642035484314\n",
            "Step [300/390]\t Loss: 0.7913206219673157\n",
            "Step [350/390]\t Loss: 0.5921438932418823\n",
            "Train Epoch [36]\t Average loss: 0.6600568725512578\n",
            "Test Epoch [36]\t Accuracy: 74.68\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.5971004366874695\n",
            "Step [50/390]\t Loss: 0.6021061539649963\n",
            "Step [100/390]\t Loss: 0.7572742700576782\n",
            "Step [150/390]\t Loss: 0.5988836884498596\n",
            "Step [200/390]\t Loss: 0.5467047095298767\n",
            "Step [250/390]\t Loss: 0.7168974876403809\n",
            "Step [300/390]\t Loss: 0.545723021030426\n",
            "Step [350/390]\t Loss: 0.622592031955719\n",
            "Train Epoch [37]\t Average loss: 0.6624580627068495\n",
            "Test Epoch [37]\t Accuracy: 74.8\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.7490060925483704\n",
            "Step [50/390]\t Loss: 0.722687840461731\n",
            "Step [100/390]\t Loss: 0.7369604706764221\n",
            "Step [150/390]\t Loss: 0.7768945693969727\n",
            "Step [200/390]\t Loss: 0.7339892983436584\n",
            "Step [250/390]\t Loss: 0.7755705714225769\n",
            "Step [300/390]\t Loss: 0.6364244818687439\n",
            "Step [350/390]\t Loss: 0.7536986470222473\n",
            "Train Epoch [38]\t Average loss: 0.6696611745235247\n",
            "Test Epoch [38]\t Accuracy: 73.71\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.6218311190605164\n",
            "Step [50/390]\t Loss: 0.5884917378425598\n",
            "Step [100/390]\t Loss: 0.742058277130127\n",
            "Step [150/390]\t Loss: 0.6379153728485107\n",
            "Step [200/390]\t Loss: 0.6743302345275879\n",
            "Step [250/390]\t Loss: 0.7160041332244873\n",
            "Step [300/390]\t Loss: 0.8667711019515991\n",
            "Step [350/390]\t Loss: 0.6433636546134949\n",
            "Train Epoch [39]\t Average loss: 0.6604881511284755\n",
            "Test Epoch [39]\t Accuracy: 75.08\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.6968044638633728\n",
            "Step [50/390]\t Loss: 0.5722702741622925\n",
            "Step [100/390]\t Loss: 0.5143057703971863\n",
            "Step [150/390]\t Loss: 0.6154303550720215\n",
            "Step [200/390]\t Loss: 0.4785934090614319\n",
            "Step [250/390]\t Loss: 0.7085684537887573\n",
            "Step [300/390]\t Loss: 0.5855342745780945\n",
            "Step [350/390]\t Loss: 0.6576148867607117\n",
            "Train Epoch [40]\t Average loss: 0.6590029729482455\n",
            "Test Epoch [40]\t Accuracy: 75.12\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.5263184905052185\n",
            "Step [50/390]\t Loss: 0.7209601402282715\n",
            "Step [100/390]\t Loss: 0.5106176137924194\n",
            "Step [150/390]\t Loss: 0.7579425573348999\n",
            "Step [200/390]\t Loss: 0.6086706519126892\n",
            "Step [250/390]\t Loss: 0.7362082004547119\n",
            "Step [300/390]\t Loss: 0.8450697064399719\n",
            "Step [350/390]\t Loss: 0.6909276247024536\n",
            "Train Epoch [41]\t Average loss: 0.6601869601469773\n",
            "Test Epoch [41]\t Accuracy: 76.15\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.7372488975524902\n",
            "Step [50/390]\t Loss: 0.4861955940723419\n",
            "Step [100/390]\t Loss: 0.47416195273399353\n",
            "Step [150/390]\t Loss: 0.6511867642402649\n",
            "Step [200/390]\t Loss: 0.7350308895111084\n",
            "Step [250/390]\t Loss: 0.4674175977706909\n",
            "Step [300/390]\t Loss: 0.7318058609962463\n",
            "Step [350/390]\t Loss: 0.7047232985496521\n",
            "Train Epoch [42]\t Average loss: 0.6562614200206903\n",
            "Test Epoch [42]\t Accuracy: 75.58\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.49342605471611023\n",
            "Step [50/390]\t Loss: 0.6235104203224182\n",
            "Step [100/390]\t Loss: 0.4844138026237488\n",
            "Step [150/390]\t Loss: 0.6437318325042725\n",
            "Step [200/390]\t Loss: 0.5849136710166931\n",
            "Step [250/390]\t Loss: 0.5723211765289307\n",
            "Step [300/390]\t Loss: 0.6792745590209961\n",
            "Step [350/390]\t Loss: 0.8032437562942505\n",
            "Train Epoch [43]\t Average loss: 0.6560946271969722\n",
            "Test Epoch [43]\t Accuracy: 71.38\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.8888512253761292\n",
            "Step [50/390]\t Loss: 0.597200870513916\n",
            "Step [100/390]\t Loss: 0.7212148308753967\n",
            "Step [150/390]\t Loss: 0.7563809156417847\n",
            "Step [200/390]\t Loss: 0.7592513561248779\n",
            "Step [250/390]\t Loss: 0.6379560232162476\n",
            "Step [300/390]\t Loss: 0.577643871307373\n",
            "Step [350/390]\t Loss: 0.4945594072341919\n",
            "Train Epoch [44]\t Average loss: 0.655990473811443\n",
            "Test Epoch [44]\t Accuracy: 74.44\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.6221290230751038\n",
            "Step [50/390]\t Loss: 0.5768188238143921\n",
            "Step [100/390]\t Loss: 0.6174314022064209\n",
            "Step [150/390]\t Loss: 0.8065998554229736\n",
            "Step [200/390]\t Loss: 0.5886942744255066\n",
            "Step [250/390]\t Loss: 0.6756486296653748\n",
            "Step [300/390]\t Loss: 0.581316351890564\n",
            "Step [350/390]\t Loss: 0.5754386186599731\n",
            "Train Epoch [45]\t Average loss: 0.6663778551113911\n",
            "Test Epoch [45]\t Accuracy: 74.7\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.5491046905517578\n",
            "Step [50/390]\t Loss: 0.8448559045791626\n",
            "Step [100/390]\t Loss: 0.589111328125\n",
            "Step [150/390]\t Loss: 0.6715383529663086\n",
            "Step [200/390]\t Loss: 0.4686824083328247\n",
            "Step [250/390]\t Loss: 0.5888874530792236\n",
            "Step [300/390]\t Loss: 0.7734439969062805\n",
            "Step [350/390]\t Loss: 0.6728207468986511\n",
            "Train Epoch [46]\t Average loss: 0.6487694027332159\n",
            "Test Epoch [46]\t Accuracy: 74.29\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.7424171566963196\n",
            "Step [50/390]\t Loss: 0.6065343022346497\n",
            "Step [100/390]\t Loss: 0.713936984539032\n",
            "Step [150/390]\t Loss: 0.6224585175514221\n",
            "Step [200/390]\t Loss: 0.7519543766975403\n",
            "Step [250/390]\t Loss: 0.5708997845649719\n",
            "Step [300/390]\t Loss: 0.6064106822013855\n",
            "Step [350/390]\t Loss: 0.8115878701210022\n",
            "Train Epoch [47]\t Average loss: 0.6519381547585512\n",
            "Test Epoch [47]\t Accuracy: 75.85\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.6356053352355957\n",
            "Step [50/390]\t Loss: 0.7444337010383606\n",
            "Step [100/390]\t Loss: 0.7454469203948975\n",
            "Step [150/390]\t Loss: 0.6313977837562561\n",
            "Step [200/390]\t Loss: 0.526116669178009\n",
            "Step [250/390]\t Loss: 0.6967495679855347\n",
            "Step [300/390]\t Loss: 0.7011038661003113\n",
            "Step [350/390]\t Loss: 0.6272815465927124\n",
            "Train Epoch [48]\t Average loss: 0.6476607930965913\n",
            "Test Epoch [48]\t Accuracy: 76.23\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.7779986262321472\n",
            "Step [50/390]\t Loss: 0.737079381942749\n",
            "Step [100/390]\t Loss: 0.5617766976356506\n",
            "Step [150/390]\t Loss: 0.723371148109436\n",
            "Step [200/390]\t Loss: 0.5249398350715637\n",
            "Step [250/390]\t Loss: 0.6441560387611389\n",
            "Step [300/390]\t Loss: 0.6792011857032776\n",
            "Step [350/390]\t Loss: 0.6686519384384155\n",
            "Train Epoch [49]\t Average loss: 0.6482568191412168\n",
            "Test Epoch [49]\t Accuracy: 72.39\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.7662891745567322\n",
            "Step [50/390]\t Loss: 0.7701624035835266\n",
            "Step [100/390]\t Loss: 0.5819751620292664\n",
            "Step [150/390]\t Loss: 0.5308992266654968\n",
            "Step [200/390]\t Loss: 0.5555614233016968\n",
            "Step [250/390]\t Loss: 0.5229707360267639\n",
            "Step [300/390]\t Loss: 0.5812593698501587\n",
            "Step [350/390]\t Loss: 0.8003159761428833\n",
            "Train Epoch [50]\t Average loss: 0.6538563849070134\n",
            "Test Epoch [50]\t Accuracy: 75.92\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.606881320476532\n",
            "Step [50/390]\t Loss: 0.5597822666168213\n",
            "Step [100/390]\t Loss: 0.5967540144920349\n",
            "Step [150/390]\t Loss: 0.6882970333099365\n",
            "Step [200/390]\t Loss: 0.6255037784576416\n",
            "Step [250/390]\t Loss: 0.6478649377822876\n",
            "Step [300/390]\t Loss: 0.6212218403816223\n",
            "Step [350/390]\t Loss: 0.7049917578697205\n",
            "Train Epoch [51]\t Average loss: 0.6494406723059141\n",
            "Test Epoch [51]\t Accuracy: 75.45\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.632972240447998\n",
            "Step [50/390]\t Loss: 0.8497076630592346\n",
            "Step [100/390]\t Loss: 0.7831711173057556\n",
            "Step [150/390]\t Loss: 0.572253942489624\n",
            "Step [200/390]\t Loss: 0.7078282833099365\n",
            "Step [250/390]\t Loss: 0.8840122222900391\n",
            "Step [300/390]\t Loss: 0.6292641162872314\n",
            "Step [350/390]\t Loss: 0.6447394490242004\n",
            "Train Epoch [52]\t Average loss: 0.644651600642082\n",
            "Test Epoch [52]\t Accuracy: 74.33\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.7175189256668091\n",
            "Step [50/390]\t Loss: 0.7216742038726807\n",
            "Step [100/390]\t Loss: 0.6373021602630615\n",
            "Step [150/390]\t Loss: 0.6042883396148682\n",
            "Step [200/390]\t Loss: 0.6042835116386414\n",
            "Step [250/390]\t Loss: 0.7652068734169006\n",
            "Step [300/390]\t Loss: 0.46818026900291443\n",
            "Step [350/390]\t Loss: 0.6513143181800842\n",
            "Train Epoch [53]\t Average loss: 0.6471791263574209\n",
            "Test Epoch [53]\t Accuracy: 75.85\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.6043960452079773\n",
            "Step [50/390]\t Loss: 0.6394113302230835\n",
            "Step [100/390]\t Loss: 0.6693860292434692\n",
            "Step [150/390]\t Loss: 0.5385068655014038\n",
            "Step [200/390]\t Loss: 0.6036049127578735\n",
            "Step [250/390]\t Loss: 0.7720096111297607\n",
            "Step [300/390]\t Loss: 0.6384819149971008\n",
            "Step [350/390]\t Loss: 0.6207278966903687\n",
            "Train Epoch [54]\t Average loss: 0.6396827133802268\n",
            "Test Epoch [54]\t Accuracy: 75.06\t Best Accuracy: 76.26\n",
            "\n",
            "Step [0/390]\t Loss: 0.480042040348053\n",
            "Step [50/390]\t Loss: 0.5440769791603088\n",
            "Step [100/390]\t Loss: 0.7084968686103821\n",
            "Step [150/390]\t Loss: 0.5783495306968689\n",
            "Step [200/390]\t Loss: 0.7349236607551575\n",
            "Step [250/390]\t Loss: 0.6451906561851501\n",
            "Step [300/390]\t Loss: 0.776023805141449\n",
            "Step [350/390]\t Loss: 0.7360971570014954\n",
            "Train Epoch [55]\t Average loss: 0.6451576109115894\n",
            "Test Epoch [55]\t Accuracy: 76.34\t Best Accuracy: 76.34\n",
            "\n",
            "Step [0/390]\t Loss: 0.7088344097137451\n",
            "Step [50/390]\t Loss: 0.5794367790222168\n",
            "Step [100/390]\t Loss: 0.506237268447876\n",
            "Step [150/390]\t Loss: 0.5264358520507812\n",
            "Step [200/390]\t Loss: 0.7076056599617004\n",
            "Step [250/390]\t Loss: 0.7581489086151123\n",
            "Step [300/390]\t Loss: 0.6450227499008179\n",
            "Step [350/390]\t Loss: 0.65705806016922\n",
            "Train Epoch [56]\t Average loss: 0.6470158024476125\n",
            "Test Epoch [56]\t Accuracy: 75.21\t Best Accuracy: 76.34\n",
            "\n",
            "Step [0/390]\t Loss: 0.7290504574775696\n",
            "Step [50/390]\t Loss: 0.6458113193511963\n",
            "Step [100/390]\t Loss: 0.7983812689781189\n",
            "Step [150/390]\t Loss: 0.481239378452301\n",
            "Step [200/390]\t Loss: 0.5807121992111206\n",
            "Step [250/390]\t Loss: 0.7043248414993286\n",
            "Step [300/390]\t Loss: 0.48115968704223633\n",
            "Step [350/390]\t Loss: 0.4942060708999634\n",
            "Train Epoch [57]\t Average loss: 0.6478411079981388\n",
            "Test Epoch [57]\t Accuracy: 74.68\t Best Accuracy: 76.34\n",
            "\n",
            "Step [0/390]\t Loss: 0.7357972860336304\n",
            "Step [50/390]\t Loss: 0.5889030694961548\n",
            "Step [100/390]\t Loss: 0.8334153294563293\n",
            "Step [150/390]\t Loss: 0.7964591979980469\n",
            "Step [200/390]\t Loss: 0.7690904140472412\n",
            "Step [250/390]\t Loss: 0.7782828211784363\n",
            "Step [300/390]\t Loss: 0.6622897982597351\n",
            "Step [350/390]\t Loss: 0.7314296960830688\n",
            "Train Epoch [58]\t Average loss: 0.6539005786944658\n",
            "Test Epoch [58]\t Accuracy: 74.43\t Best Accuracy: 76.34\n",
            "\n",
            "Step [0/390]\t Loss: 0.6932367086410522\n",
            "Step [50/390]\t Loss: 0.7314828634262085\n",
            "Step [100/390]\t Loss: 0.5383532047271729\n",
            "Step [150/390]\t Loss: 0.607867419719696\n",
            "Step [200/390]\t Loss: 0.6457647681236267\n",
            "Step [250/390]\t Loss: 0.6308550238609314\n",
            "Step [300/390]\t Loss: 0.7127351760864258\n",
            "Step [350/390]\t Loss: 0.7026235461235046\n",
            "Train Epoch [59]\t Average loss: 0.6459514097525523\n",
            "Test Epoch [59]\t Accuracy: 74.63\t Best Accuracy: 76.34\n",
            "\n",
            "Step [0/390]\t Loss: 0.6303378343582153\n",
            "Step [50/390]\t Loss: 0.5581986308097839\n",
            "Step [100/390]\t Loss: 0.6219937801361084\n",
            "Step [150/390]\t Loss: 0.6602184176445007\n",
            "Step [200/390]\t Loss: 0.6035661101341248\n",
            "Step [250/390]\t Loss: 0.6094139218330383\n",
            "Step [300/390]\t Loss: 0.5547963380813599\n",
            "Step [350/390]\t Loss: 0.6189719438552856\n",
            "Train Epoch [60]\t Average loss: 0.5888892230315086\n",
            "Test Epoch [60]\t Accuracy: 76.79\t Best Accuracy: 76.79\n",
            "\n",
            "Step [0/390]\t Loss: 0.5008851289749146\n",
            "Step [50/390]\t Loss: 0.574236273765564\n",
            "Step [100/390]\t Loss: 0.44816136360168457\n",
            "Step [150/390]\t Loss: 0.6413865685462952\n",
            "Step [200/390]\t Loss: 0.642091691493988\n",
            "Step [250/390]\t Loss: 0.7276797294616699\n",
            "Step [300/390]\t Loss: 0.5056801438331604\n",
            "Step [350/390]\t Loss: 0.5419762134552002\n",
            "Train Epoch [61]\t Average loss: 0.5841124727175786\n",
            "Test Epoch [61]\t Accuracy: 76.74\t Best Accuracy: 76.79\n",
            "\n",
            "Step [0/390]\t Loss: 0.5104217529296875\n",
            "Step [50/390]\t Loss: 0.6557887196540833\n",
            "Step [100/390]\t Loss: 0.6623749136924744\n",
            "Step [150/390]\t Loss: 0.6710973381996155\n",
            "Step [200/390]\t Loss: 0.6509236097335815\n",
            "Step [250/390]\t Loss: 0.5027387142181396\n",
            "Step [300/390]\t Loss: 0.4861554503440857\n",
            "Step [350/390]\t Loss: 0.4729270935058594\n",
            "Train Epoch [62]\t Average loss: 0.5834841923835951\n",
            "Test Epoch [62]\t Accuracy: 77.1\t Best Accuracy: 77.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.6882814168930054\n",
            "Step [50/390]\t Loss: 0.5448623895645142\n",
            "Step [100/390]\t Loss: 0.4790191948413849\n",
            "Step [150/390]\t Loss: 0.5090689063072205\n",
            "Step [200/390]\t Loss: 0.47055479884147644\n",
            "Step [250/390]\t Loss: 0.8241078853607178\n",
            "Step [300/390]\t Loss: 0.5271888375282288\n",
            "Step [350/390]\t Loss: 0.5580660104751587\n",
            "Train Epoch [63]\t Average loss: 0.5828726924382723\n",
            "Test Epoch [63]\t Accuracy: 76.96\t Best Accuracy: 77.1\n",
            "\n",
            "Step [0/390]\t Loss: 0.4167138934135437\n",
            "Step [50/390]\t Loss: 0.6508987545967102\n",
            "Step [100/390]\t Loss: 0.5713223218917847\n",
            "Step [150/390]\t Loss: 0.6567947864532471\n",
            "Step [200/390]\t Loss: 0.5912871360778809\n",
            "Step [250/390]\t Loss: 0.4829256534576416\n",
            "Step [300/390]\t Loss: 0.6654852628707886\n",
            "Step [350/390]\t Loss: 0.6645250916481018\n",
            "Train Epoch [64]\t Average loss: 0.5833863567847472\n",
            "Test Epoch [64]\t Accuracy: 77.17\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.6190847158432007\n",
            "Step [50/390]\t Loss: 0.5714477300643921\n",
            "Step [100/390]\t Loss: 0.5597410798072815\n",
            "Step [150/390]\t Loss: 0.5429331064224243\n",
            "Step [200/390]\t Loss: 0.6113713979721069\n",
            "Step [250/390]\t Loss: 0.5150860548019409\n",
            "Step [300/390]\t Loss: 0.5040812492370605\n",
            "Step [350/390]\t Loss: 0.5901072025299072\n",
            "Train Epoch [65]\t Average loss: 0.5836388101944556\n",
            "Test Epoch [65]\t Accuracy: 76.97\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.5961657762527466\n",
            "Step [50/390]\t Loss: 0.5525538921356201\n",
            "Step [100/390]\t Loss: 0.7777405977249146\n",
            "Step [150/390]\t Loss: 0.5092414617538452\n",
            "Step [200/390]\t Loss: 0.5064992904663086\n",
            "Step [250/390]\t Loss: 0.49021807312965393\n",
            "Step [300/390]\t Loss: 0.5276741981506348\n",
            "Step [350/390]\t Loss: 0.5726291537284851\n",
            "Train Epoch [66]\t Average loss: 0.5832916550147228\n",
            "Test Epoch [66]\t Accuracy: 76.8\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.6280313730239868\n",
            "Step [50/390]\t Loss: 0.47891995310783386\n",
            "Step [100/390]\t Loss: 0.5806102752685547\n",
            "Step [150/390]\t Loss: 0.47026631236076355\n",
            "Step [200/390]\t Loss: 0.6141790747642517\n",
            "Step [250/390]\t Loss: 0.6153605580329895\n",
            "Step [300/390]\t Loss: 0.40565225481987\n",
            "Step [350/390]\t Loss: 0.6652483344078064\n",
            "Train Epoch [67]\t Average loss: 0.5835355132818222\n",
            "Test Epoch [67]\t Accuracy: 76.78\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.5148500204086304\n",
            "Step [50/390]\t Loss: 0.6031171083450317\n",
            "Step [100/390]\t Loss: 0.5894335508346558\n",
            "Step [150/390]\t Loss: 0.6102666854858398\n",
            "Step [200/390]\t Loss: 0.6553314924240112\n",
            "Step [250/390]\t Loss: 0.4934447407722473\n",
            "Step [300/390]\t Loss: 0.44678398966789246\n",
            "Step [350/390]\t Loss: 0.6024127006530762\n",
            "Train Epoch [68]\t Average loss: 0.5821951542909328\n",
            "Test Epoch [68]\t Accuracy: 77.03\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.6910650134086609\n",
            "Step [50/390]\t Loss: 0.5884798765182495\n",
            "Step [100/390]\t Loss: 0.59206223487854\n",
            "Step [150/390]\t Loss: 0.4850558638572693\n",
            "Step [200/390]\t Loss: 0.5366799235343933\n",
            "Step [250/390]\t Loss: 0.548668622970581\n",
            "Step [300/390]\t Loss: 0.5168941617012024\n",
            "Step [350/390]\t Loss: 0.5744723677635193\n",
            "Train Epoch [69]\t Average loss: 0.5829459616006949\n",
            "Test Epoch [69]\t Accuracy: 76.78\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.601416826248169\n",
            "Step [50/390]\t Loss: 0.5542269945144653\n",
            "Step [100/390]\t Loss: 0.5882773399353027\n",
            "Step [150/390]\t Loss: 0.5707643628120422\n",
            "Step [200/390]\t Loss: 0.6503574252128601\n",
            "Step [250/390]\t Loss: 0.5650458931922913\n",
            "Step [300/390]\t Loss: 0.5313594341278076\n",
            "Step [350/390]\t Loss: 0.6514572501182556\n",
            "Train Epoch [70]\t Average loss: 0.5827315949476682\n",
            "Test Epoch [70]\t Accuracy: 77.07\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.4727686941623688\n",
            "Step [50/390]\t Loss: 0.7068641185760498\n",
            "Step [100/390]\t Loss: 0.6777347326278687\n",
            "Step [150/390]\t Loss: 0.5047687292098999\n",
            "Step [200/390]\t Loss: 0.6262236833572388\n",
            "Step [250/390]\t Loss: 0.58128422498703\n",
            "Step [300/390]\t Loss: 0.5777122974395752\n",
            "Step [350/390]\t Loss: 0.45027366280555725\n",
            "Train Epoch [71]\t Average loss: 0.5824624141821495\n",
            "Test Epoch [71]\t Accuracy: 76.89\t Best Accuracy: 77.17\n",
            "\n",
            "Step [0/390]\t Loss: 0.6095964312553406\n",
            "Step [50/390]\t Loss: 0.5797404646873474\n",
            "Step [100/390]\t Loss: 0.48432695865631104\n",
            "Step [150/390]\t Loss: 0.45592620968818665\n",
            "Step [200/390]\t Loss: 0.5554155707359314\n",
            "Step [250/390]\t Loss: 0.5042904019355774\n",
            "Step [300/390]\t Loss: 0.5524742603302002\n",
            "Step [350/390]\t Loss: 0.49537667632102966\n",
            "Train Epoch [72]\t Average loss: 0.5820439337155758\n",
            "Test Epoch [72]\t Accuracy: 77.31\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.591908872127533\n",
            "Step [50/390]\t Loss: 0.5627176761627197\n",
            "Step [100/390]\t Loss: 0.4338487386703491\n",
            "Step [150/390]\t Loss: 0.5711960196495056\n",
            "Step [200/390]\t Loss: 0.5094470381736755\n",
            "Step [250/390]\t Loss: 0.7072109580039978\n",
            "Step [300/390]\t Loss: 0.678240954875946\n",
            "Step [350/390]\t Loss: 0.6422938704490662\n",
            "Train Epoch [73]\t Average loss: 0.5823124615045694\n",
            "Test Epoch [73]\t Accuracy: 77.04\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5093945860862732\n",
            "Step [50/390]\t Loss: 0.36533322930336\n",
            "Step [100/390]\t Loss: 0.4741929769515991\n",
            "Step [150/390]\t Loss: 0.5022786855697632\n",
            "Step [200/390]\t Loss: 0.5724892616271973\n",
            "Step [250/390]\t Loss: 0.5312793254852295\n",
            "Step [300/390]\t Loss: 0.6546034216880798\n",
            "Step [350/390]\t Loss: 0.6704162955284119\n",
            "Train Epoch [74]\t Average loss: 0.5828349090539492\n",
            "Test Epoch [74]\t Accuracy: 77.06\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6462039947509766\n",
            "Step [50/390]\t Loss: 0.4705325663089752\n",
            "Step [100/390]\t Loss: 0.4703092575073242\n",
            "Step [150/390]\t Loss: 0.6023532152175903\n",
            "Step [200/390]\t Loss: 0.6944792866706848\n",
            "Step [250/390]\t Loss: 0.6830055713653564\n",
            "Step [300/390]\t Loss: 0.6731075048446655\n",
            "Step [350/390]\t Loss: 0.5854039788246155\n",
            "Train Epoch [75]\t Average loss: 0.5824288773231018\n",
            "Test Epoch [75]\t Accuracy: 76.7\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6729065775871277\n",
            "Step [50/390]\t Loss: 0.5501965284347534\n",
            "Step [100/390]\t Loss: 0.5385560393333435\n",
            "Step [150/390]\t Loss: 0.48095783591270447\n",
            "Step [200/390]\t Loss: 0.5428951382637024\n",
            "Step [250/390]\t Loss: 0.582175612449646\n",
            "Step [300/390]\t Loss: 0.4475386142730713\n",
            "Step [350/390]\t Loss: 0.6537593603134155\n",
            "Train Epoch [76]\t Average loss: 0.5823790843670185\n",
            "Test Epoch [76]\t Accuracy: 77.15\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6157195568084717\n",
            "Step [50/390]\t Loss: 0.5071669220924377\n",
            "Step [100/390]\t Loss: 0.462914377450943\n",
            "Step [150/390]\t Loss: 0.4541472792625427\n",
            "Step [200/390]\t Loss: 0.46269792318344116\n",
            "Step [250/390]\t Loss: 0.5340157747268677\n",
            "Step [300/390]\t Loss: 0.39780983328819275\n",
            "Step [350/390]\t Loss: 0.48473668098449707\n",
            "Train Epoch [77]\t Average loss: 0.5827492169080637\n",
            "Test Epoch [77]\t Accuracy: 76.9\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.49093908071517944\n",
            "Step [50/390]\t Loss: 0.5461172461509705\n",
            "Step [100/390]\t Loss: 0.5659534931182861\n",
            "Step [150/390]\t Loss: 0.6018131971359253\n",
            "Step [200/390]\t Loss: 0.6508742570877075\n",
            "Step [250/390]\t Loss: 0.5516719818115234\n",
            "Step [300/390]\t Loss: 0.4845752418041229\n",
            "Step [350/390]\t Loss: 0.62137371301651\n",
            "Train Epoch [78]\t Average loss: 0.5819951123916186\n",
            "Test Epoch [78]\t Accuracy: 77.12\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6667510271072388\n",
            "Step [50/390]\t Loss: 0.6733994483947754\n",
            "Step [100/390]\t Loss: 0.5904680490493774\n",
            "Step [150/390]\t Loss: 0.514544665813446\n",
            "Step [200/390]\t Loss: 0.5912604331970215\n",
            "Step [250/390]\t Loss: 0.6461778879165649\n",
            "Step [300/390]\t Loss: 0.6017248630523682\n",
            "Step [350/390]\t Loss: 0.632077693939209\n",
            "Train Epoch [79]\t Average loss: 0.5826322818413759\n",
            "Test Epoch [79]\t Accuracy: 76.9\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6849080920219421\n",
            "Step [50/390]\t Loss: 0.6569782495498657\n",
            "Step [100/390]\t Loss: 0.5754806995391846\n",
            "Step [150/390]\t Loss: 0.5019986033439636\n",
            "Step [200/390]\t Loss: 0.6321957111358643\n",
            "Step [250/390]\t Loss: 0.6667307615280151\n",
            "Step [300/390]\t Loss: 0.5888002514839172\n",
            "Step [350/390]\t Loss: 0.47395697236061096\n",
            "Train Epoch [80]\t Average loss: 0.5768999628531627\n",
            "Test Epoch [80]\t Accuracy: 77.04\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6252664923667908\n",
            "Step [50/390]\t Loss: 0.7130029201507568\n",
            "Step [100/390]\t Loss: 0.4663577079772949\n",
            "Step [150/390]\t Loss: 0.5466406941413879\n",
            "Step [200/390]\t Loss: 0.7623398303985596\n",
            "Step [250/390]\t Loss: 0.6986013054847717\n",
            "Step [300/390]\t Loss: 0.4799913465976715\n",
            "Step [350/390]\t Loss: 0.5443827509880066\n",
            "Train Epoch [81]\t Average loss: 0.5768723961634513\n",
            "Test Epoch [81]\t Accuracy: 77.08\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.492765337228775\n",
            "Step [50/390]\t Loss: 0.561985969543457\n",
            "Step [100/390]\t Loss: 0.5429843664169312\n",
            "Step [150/390]\t Loss: 0.6415378451347351\n",
            "Step [200/390]\t Loss: 0.6011695265769958\n",
            "Step [250/390]\t Loss: 0.5620043277740479\n",
            "Step [300/390]\t Loss: 0.5826746225357056\n",
            "Step [350/390]\t Loss: 0.5146918892860413\n",
            "Train Epoch [82]\t Average loss: 0.5768999662918922\n",
            "Test Epoch [82]\t Accuracy: 77.11\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.46157386898994446\n",
            "Step [50/390]\t Loss: 0.5310069918632507\n",
            "Step [100/390]\t Loss: 0.44598671793937683\n",
            "Step [150/390]\t Loss: 0.5126460790634155\n",
            "Step [200/390]\t Loss: 0.7348933219909668\n",
            "Step [250/390]\t Loss: 0.6035123467445374\n",
            "Step [300/390]\t Loss: 0.6856392621994019\n",
            "Step [350/390]\t Loss: 0.6671526432037354\n",
            "Train Epoch [83]\t Average loss: 0.5766957024733226\n",
            "Test Epoch [83]\t Accuracy: 77.15\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5731950998306274\n",
            "Step [50/390]\t Loss: 0.6093735694885254\n",
            "Step [100/390]\t Loss: 0.5595925450325012\n",
            "Step [150/390]\t Loss: 0.6089699268341064\n",
            "Step [200/390]\t Loss: 0.6024307608604431\n",
            "Step [250/390]\t Loss: 0.5175445675849915\n",
            "Step [300/390]\t Loss: 0.5914800763130188\n",
            "Step [350/390]\t Loss: 0.6650059819221497\n",
            "Train Epoch [84]\t Average loss: 0.5765504596325067\n",
            "Test Epoch [84]\t Accuracy: 77.09\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6029166579246521\n",
            "Step [50/390]\t Loss: 0.6317340135574341\n",
            "Step [100/390]\t Loss: 0.5305875539779663\n",
            "Step [150/390]\t Loss: 0.4846126139163971\n",
            "Step [200/390]\t Loss: 0.5743421316146851\n",
            "Step [250/390]\t Loss: 0.6175073981285095\n",
            "Step [300/390]\t Loss: 0.5170717239379883\n",
            "Step [350/390]\t Loss: 0.6262297034263611\n",
            "Train Epoch [85]\t Average loss: 0.5768455111827606\n",
            "Test Epoch [85]\t Accuracy: 77.08\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.7086377143859863\n",
            "Step [50/390]\t Loss: 0.5785197615623474\n",
            "Step [100/390]\t Loss: 0.6043379306793213\n",
            "Step [150/390]\t Loss: 0.6081549525260925\n",
            "Step [200/390]\t Loss: 0.6241284012794495\n",
            "Step [250/390]\t Loss: 0.6808247566223145\n",
            "Step [300/390]\t Loss: 0.4623068571090698\n",
            "Step [350/390]\t Loss: 0.6185427308082581\n",
            "Train Epoch [86]\t Average loss: 0.5767828044983057\n",
            "Test Epoch [86]\t Accuracy: 77.04\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5903337001800537\n",
            "Step [50/390]\t Loss: 0.6456568241119385\n",
            "Step [100/390]\t Loss: 0.5803604125976562\n",
            "Step [150/390]\t Loss: 0.5120871663093567\n",
            "Step [200/390]\t Loss: 0.458964079618454\n",
            "Step [250/390]\t Loss: 0.6094397902488708\n",
            "Step [300/390]\t Loss: 0.5827552080154419\n",
            "Step [350/390]\t Loss: 0.574015200138092\n",
            "Train Epoch [87]\t Average loss: 0.5768430325465325\n",
            "Test Epoch [87]\t Accuracy: 77.09\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6246321201324463\n",
            "Step [50/390]\t Loss: 0.6921050548553467\n",
            "Step [100/390]\t Loss: 0.5581506490707397\n",
            "Step [150/390]\t Loss: 0.5512044429779053\n",
            "Step [200/390]\t Loss: 0.5037603974342346\n",
            "Step [250/390]\t Loss: 0.6604171991348267\n",
            "Step [300/390]\t Loss: 0.47831183671951294\n",
            "Step [350/390]\t Loss: 0.6161010265350342\n",
            "Train Epoch [88]\t Average loss: 0.5764819923119667\n",
            "Test Epoch [88]\t Accuracy: 77.03\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6796874403953552\n",
            "Step [50/390]\t Loss: 0.4612236022949219\n",
            "Step [100/390]\t Loss: 0.6823680400848389\n",
            "Step [150/390]\t Loss: 0.6946536302566528\n",
            "Step [200/390]\t Loss: 0.4645187556743622\n",
            "Step [250/390]\t Loss: 0.6139400601387024\n",
            "Step [300/390]\t Loss: 0.5461569428443909\n",
            "Step [350/390]\t Loss: 0.4619535505771637\n",
            "Train Epoch [89]\t Average loss: 0.576663278005062\n",
            "Test Epoch [89]\t Accuracy: 77.01\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5243178009986877\n",
            "Step [50/390]\t Loss: 0.5519757866859436\n",
            "Step [100/390]\t Loss: 0.616796612739563\n",
            "Step [150/390]\t Loss: 0.5402401089668274\n",
            "Step [200/390]\t Loss: 0.6502397060394287\n",
            "Step [250/390]\t Loss: 0.6283573508262634\n",
            "Step [300/390]\t Loss: 0.6070687770843506\n",
            "Step [350/390]\t Loss: 0.7591261863708496\n",
            "Train Epoch [90]\t Average loss: 0.5771678957419518\n",
            "Test Epoch [90]\t Accuracy: 77.15\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6658499836921692\n",
            "Step [50/390]\t Loss: 0.5461239218711853\n",
            "Step [100/390]\t Loss: 0.598280131816864\n",
            "Step [150/390]\t Loss: 0.5755879282951355\n",
            "Step [200/390]\t Loss: 0.5749872326850891\n",
            "Step [250/390]\t Loss: 0.5066735744476318\n",
            "Step [300/390]\t Loss: 0.49584174156188965\n",
            "Step [350/390]\t Loss: 0.5004424452781677\n",
            "Train Epoch [91]\t Average loss: 0.5764142626371139\n",
            "Test Epoch [91]\t Accuracy: 77.05\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5125983357429504\n",
            "Step [50/390]\t Loss: 0.5914566516876221\n",
            "Step [100/390]\t Loss: 0.5510191321372986\n",
            "Step [150/390]\t Loss: 0.4566282033920288\n",
            "Step [200/390]\t Loss: 0.6361309885978699\n",
            "Step [250/390]\t Loss: 0.45397254824638367\n",
            "Step [300/390]\t Loss: 0.6043063402175903\n",
            "Step [350/390]\t Loss: 0.6326286196708679\n",
            "Train Epoch [92]\t Average loss: 0.5764832232242976\n",
            "Test Epoch [92]\t Accuracy: 77.04\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5237309336662292\n",
            "Step [50/390]\t Loss: 0.5392377972602844\n",
            "Step [100/390]\t Loss: 0.5658230781555176\n",
            "Step [150/390]\t Loss: 0.548597514629364\n",
            "Step [200/390]\t Loss: 0.5397660136222839\n",
            "Step [250/390]\t Loss: 0.4428640604019165\n",
            "Step [300/390]\t Loss: 0.602073609828949\n",
            "Step [350/390]\t Loss: 0.5826888680458069\n",
            "Train Epoch [93]\t Average loss: 0.5768334566782682\n",
            "Test Epoch [93]\t Accuracy: 77.15\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.48385700583457947\n",
            "Step [50/390]\t Loss: 0.49376851320266724\n",
            "Step [100/390]\t Loss: 0.6284871101379395\n",
            "Step [150/390]\t Loss: 0.6823374629020691\n",
            "Step [200/390]\t Loss: 0.5542531609535217\n",
            "Step [250/390]\t Loss: 0.6943463087081909\n",
            "Step [300/390]\t Loss: 0.7754610776901245\n",
            "Step [350/390]\t Loss: 0.5984063148498535\n",
            "Train Epoch [94]\t Average loss: 0.5766129597639427\n",
            "Test Epoch [94]\t Accuracy: 77.09\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.4928799867630005\n",
            "Step [50/390]\t Loss: 0.6211426258087158\n",
            "Step [100/390]\t Loss: 0.6780943274497986\n",
            "Step [150/390]\t Loss: 0.5880041122436523\n",
            "Step [200/390]\t Loss: 0.7377266883850098\n",
            "Step [250/390]\t Loss: 0.5088644027709961\n",
            "Step [300/390]\t Loss: 0.5094605684280396\n",
            "Step [350/390]\t Loss: 0.5478491187095642\n",
            "Train Epoch [95]\t Average loss: 0.5767453539829988\n",
            "Test Epoch [95]\t Accuracy: 77.06\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.688653826713562\n",
            "Step [50/390]\t Loss: 0.6140217185020447\n",
            "Step [100/390]\t Loss: 0.5758126378059387\n",
            "Step [150/390]\t Loss: 0.6101386547088623\n",
            "Step [200/390]\t Loss: 0.531275749206543\n",
            "Step [250/390]\t Loss: 0.6241434216499329\n",
            "Step [300/390]\t Loss: 0.49452611804008484\n",
            "Step [350/390]\t Loss: 0.5212002992630005\n",
            "Train Epoch [96]\t Average loss: 0.57682477862407\n",
            "Test Epoch [96]\t Accuracy: 77.1\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5729398131370544\n",
            "Step [50/390]\t Loss: 0.6093152761459351\n",
            "Step [100/390]\t Loss: 0.6650910973548889\n",
            "Step [150/390]\t Loss: 0.6716182827949524\n",
            "Step [200/390]\t Loss: 0.45152363181114197\n",
            "Step [250/390]\t Loss: 0.4909241199493408\n",
            "Step [300/390]\t Loss: 0.6151872873306274\n",
            "Step [350/390]\t Loss: 0.566730797290802\n",
            "Train Epoch [97]\t Average loss: 0.5767949814979847\n",
            "Test Epoch [97]\t Accuracy: 77.08\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.6880089044570923\n",
            "Step [50/390]\t Loss: 0.5206404328346252\n",
            "Step [100/390]\t Loss: 0.601453423500061\n",
            "Step [150/390]\t Loss: 0.7433124780654907\n",
            "Step [200/390]\t Loss: 0.689536452293396\n",
            "Step [250/390]\t Loss: 0.68578040599823\n",
            "Step [300/390]\t Loss: 0.48135828971862793\n",
            "Step [350/390]\t Loss: 0.5382387638092041\n",
            "Train Epoch [98]\t Average loss: 0.576690137768403\n",
            "Test Epoch [98]\t Accuracy: 77.06\t Best Accuracy: 77.31\n",
            "\n",
            "Step [0/390]\t Loss: 0.5084487795829773\n",
            "Step [50/390]\t Loss: 0.6711837649345398\n",
            "Step [100/390]\t Loss: 0.5454731583595276\n",
            "Step [150/390]\t Loss: 0.5575136542320251\n",
            "Step [200/390]\t Loss: 0.652669370174408\n",
            "Step [250/390]\t Loss: 0.6169090867042542\n",
            "Step [300/390]\t Loss: 0.4134306311607361\n",
            "Step [350/390]\t Loss: 0.3571169078350067\n",
            "Train Epoch [99]\t Average loss: 0.5767972133098505\n",
            "Test Epoch [99]\t Accuracy: 76.99\t Best Accuracy: 77.31\n",
            "\n",
            "Final Best Accuracy: 77.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.argv=['']\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='SimCLR')\n",
        "parser.add_argument('--model', default=\"LBE\", type=str, help=\"orig/RC/LBE/IP/MIB\")\n",
        "parser.add_argument('--batch_size', default=256, type=int, metavar='B', help='training batch size')\n",
        "parser.add_argument('--workers', default=2, type=int, help='workers')\n",
        "parser.add_argument('--epochs', default=30, type=int, help='epochs')\n",
        "parser.add_argument('--save_freq', default=20, type=int, help='save frequency')\n",
        "parser.add_argument('--resnet', default=\"resnet18\", type=str, help=\"resnet18/resnet34/resnet50/resnet101/resnet152\")\n",
        "parser.add_argument('--normalize', default=True, action='store_true', help='normalize')\n",
        "parser.add_argument('--projection_dim', default=128, type=int, help='projection_dim')\n",
        "parser.add_argument('--lamb', default=1., type=float, help='weight of regularization term')\n",
        "parser.add_argument('--zeta', default=0.1, type=float, help='variance')\n",
        "parser.add_argument('--optimizer', default=\"Adam\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--lr', default=3e-4, type=float, help='lr')\n",
        "parser.add_argument('--weight_decay', default=1e-6, type=float, help='weight_decay')\n",
        "parser.add_argument('--temperature', default=0.5, type=float, help='temperature')\n",
        "parser.add_argument('--gpus', default=8, type=int, help='number of gpu')\n",
        "parser.add_argument('--model_dir', default='output/checkpoint/', type=str, help='model save path')\n",
        "parser.add_argument('--dataset', default='CIFAR10', help='[CIFAR10, CIFAR100, ImageNet, STL-10]')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def train(train_loader, model, recon, criterion, optimizer):\n",
        "    loss_epoch = 0.\n",
        "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "        x_i, x_j = x_i.cuda(), x_j.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        if args.model == 'orig':\n",
        "            _, z_i = model(x_i)\n",
        "            _, z_j = model(x_j)\n",
        "            loss = criterion(z_i, z_j)\n",
        "        elif args.model == 'RC':\n",
        "            h_i, z_i = model(x_i)\n",
        "            h_j, z_j = model(x_j)\n",
        "            recon_loss = F.mse_loss(recon(h_i), x_i) + F.mse_loss(recon(h_j), x_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * recon_loss\n",
        "        elif args.model == 'LBE':\n",
        "            mu2_i, mu3_i, mu4_i, h2_i, h3_i, h4_i, z_i = model(x_i)\n",
        "            mu2_j, mu3_j, mu4_j, h2_j, h3_j, h4_j, z_j = model(x_j)\n",
        "            mu2, h2 = torch.cat([mu2_i, mu2_j], dim=0), torch.cat([h2_i, h2_j], dim=0)\n",
        "            mu3, h3 = torch.cat([mu3_i, mu3_j], dim=0), torch.cat([h3_i, h3_j], dim=0)\n",
        "            mu4, h4 = torch.cat([mu4_i, mu4_j], dim=0), torch.cat([h4_i, h4_j], dim=0)\n",
        "            if args.dataset == \"ImageNet\":\n",
        "                MI_estimitor = InfoNCE(mu4, h4)\n",
        "            else:\n",
        "                MI_estimitor = 0.25 * InfoNCE(mu2, h2) + 0.50 * InfoNCE(mu3, h3) + InfoNCE(mu4, h4)\n",
        "            loss = criterion(z_i, z_j) - args.lamb * MI_estimitor\n",
        "        elif args.model == 'IP':\n",
        "            h_i, z_i = model(x_i)\n",
        "            h_j, z_j = model(x_j)\n",
        "            IP = F.mse_loss(h_i, h_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * IP\n",
        "        elif args.model == 'MIB':\n",
        "            mu_i, _, z_i = model(x_i)\n",
        "            mu_j, _, z_j = model(x_j)\n",
        "            MIB = F.mse_loss(mu_i, mu_j)\n",
        "            loss = criterion(z_i, z_j) + args.lamb * MIB\n",
        "        else:\n",
        "            assert False\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
        "        loss_epoch += loss.item()\n",
        "    return loss_epoch\n",
        "\n",
        "\n",
        "def main():\n",
        "    data = 'non_imagenet'\n",
        "    root = \"datasets\"\n",
        "    if args.dataset == \"CIFAR10\":\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root, download=True, transform=Transforms(32))\n",
        "    elif args.dataset == \"CIFAR100\":\n",
        "        train_dataset = torchvision.datasets.CIFAR100(root, download=True, transform=Transforms(32))\n",
        "    elif args.dataset == \"STL-10\":\n",
        "        train_dataset = torchvision.datasets.STL10(root, split='unlabeled', download=True, transform=Transforms(64))\n",
        "    elif args.dataset == \"ImageNet\":\n",
        "        traindir = os.path.join(root, 'ImageNet/train')\n",
        "        train_dataset = torchvision.datasets.ImageFolder(traindir, Transforms_imagenet(size=224))\n",
        "        data = 'imagenet'\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.workers,\n",
        "        sampler=None)\n",
        "\n",
        "    log_dir = \"output/log/\" + args.dataset + '_%s/'%args.model\n",
        "    if not os.path.isdir(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    suffix = args.dataset + '_{}_batch_{}'.format(args.resnet, args.batch_size)\n",
        "    suffix = suffix + '_proj_dim_{}'.format(args.projection_dim)\n",
        "    test_log_file = open(log_dir + suffix + '.txt', \"w\")\n",
        "\n",
        "    model, recon, optimizer, scheduler = load_model(args, data=data)\n",
        "    if args.dataset=='ImageNet':\n",
        "        model = torch.nn.DataParallel(model, device_ids=list(range(args.gpus)))\n",
        "    args.model_dir = args.model_dir + args.dataset + '_%s/'%args.model\n",
        "    if not os.path.isdir(args.model_dir):\n",
        "        os.makedirs(args.model_dir)\n",
        "            \n",
        "    mask = mask_correlated_samples(args.batch_size)\n",
        "    criterion = NT_Xent(args.batch_size, args.temperature, mask)\n",
        "    for epoch in range(args.epochs):\n",
        "        loss_epoch = train(train_loader, model, recon, criterion, optimizer)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        if (epoch+1) % args.save_freq == 0:\n",
        "            save_model(args.model_dir+suffix, model, epoch+1)\n",
        "\n",
        "        print('Epoch {} loss: {}\\n'.format(epoch, loss_epoch / len(train_loader)))\n",
        "        print('Epoch {} loss: {}'.format(epoch, loss_epoch/len(train_loader)), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "\n",
        "    save_model(args.model_dir+suffix, model, args.epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdwfxFslk_bC",
        "outputId": "476f83ed-8b31-47e1-cfa3-75ea4d536f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Step [0/195]\t Loss: 12.30868911743164\n",
            "Step [50/195]\t Loss: 5.741456031799316\n",
            "Step [100/195]\t Loss: 5.541835784912109\n",
            "Step [150/195]\t Loss: 5.551767826080322\n",
            "Epoch 0 loss: 5.747309775230212\n",
            "\n",
            "Step [0/195]\t Loss: 5.5019850730896\n",
            "Step [50/195]\t Loss: 5.403343200683594\n",
            "Step [100/195]\t Loss: 5.38482666015625\n",
            "Step [150/195]\t Loss: 5.28580904006958\n",
            "Epoch 1 loss: 5.361246272845146\n",
            "\n",
            "Step [0/195]\t Loss: 5.316155433654785\n",
            "Step [50/195]\t Loss: 5.199192047119141\n",
            "Step [100/195]\t Loss: 5.160252094268799\n",
            "Step [150/195]\t Loss: 5.155027866363525\n",
            "Epoch 2 loss: 5.225735326913687\n",
            "\n",
            "Step [0/195]\t Loss: 5.175335884094238\n",
            "Step [50/195]\t Loss: 5.157167911529541\n",
            "Step [100/195]\t Loss: 5.055595397949219\n",
            "Step [150/195]\t Loss: 5.087771415710449\n",
            "Epoch 3 loss: 5.1335507466242865\n",
            "\n",
            "Step [0/195]\t Loss: 5.138099670410156\n",
            "Step [50/195]\t Loss: 5.099242687225342\n",
            "Step [100/195]\t Loss: 5.052754878997803\n",
            "Step [150/195]\t Loss: 5.076033115386963\n",
            "Epoch 4 loss: 5.060245284056053\n",
            "\n",
            "Step [0/195]\t Loss: 4.991260051727295\n",
            "Step [50/195]\t Loss: 5.004289627075195\n",
            "Step [100/195]\t Loss: 5.0265302658081055\n",
            "Step [150/195]\t Loss: 4.990043640136719\n",
            "Epoch 5 loss: 5.014018733684833\n",
            "\n",
            "Step [0/195]\t Loss: 4.960601329803467\n",
            "Step [50/195]\t Loss: 4.897682189941406\n",
            "Step [100/195]\t Loss: 4.959585189819336\n",
            "Step [150/195]\t Loss: 4.955836296081543\n",
            "Epoch 6 loss: 4.968863389430902\n",
            "\n",
            "Step [0/195]\t Loss: 4.9882707595825195\n",
            "Step [50/195]\t Loss: 4.924270153045654\n",
            "Step [100/195]\t Loss: 4.923570156097412\n",
            "Step [150/195]\t Loss: 4.951026916503906\n",
            "Epoch 7 loss: 4.929931669968825\n",
            "\n",
            "Step [0/195]\t Loss: 4.9040374755859375\n",
            "Step [50/195]\t Loss: 4.8943681716918945\n",
            "Step [100/195]\t Loss: 4.925676345825195\n",
            "Step [150/195]\t Loss: 4.899507999420166\n",
            "Epoch 8 loss: 4.891392641801101\n",
            "\n",
            "Step [0/195]\t Loss: 4.803279399871826\n",
            "Step [50/195]\t Loss: 4.870975971221924\n",
            "Step [100/195]\t Loss: 4.819791316986084\n",
            "Step [150/195]\t Loss: 4.8888421058654785\n",
            "Epoch 9 loss: 4.864063030634171\n",
            "\n",
            "Step [0/195]\t Loss: 4.868975639343262\n",
            "Step [50/195]\t Loss: 4.86067533493042\n",
            "Step [100/195]\t Loss: 4.871380805969238\n",
            "Step [150/195]\t Loss: 4.851067543029785\n",
            "Epoch 10 loss: 4.828742971175756\n",
            "\n",
            "Step [0/195]\t Loss: 4.8659443855285645\n",
            "Step [50/195]\t Loss: 4.77786922454834\n",
            "Step [100/195]\t Loss: 4.830113410949707\n",
            "Step [150/195]\t Loss: 4.737790584564209\n",
            "Epoch 11 loss: 4.793576367696127\n",
            "\n",
            "Step [0/195]\t Loss: 4.830216884613037\n",
            "Step [50/195]\t Loss: 4.802389621734619\n",
            "Step [100/195]\t Loss: 4.818874835968018\n",
            "Step [150/195]\t Loss: 4.776508808135986\n",
            "Epoch 12 loss: 4.7743206977844235\n",
            "\n",
            "Step [0/195]\t Loss: 4.821619510650635\n",
            "Step [50/195]\t Loss: 4.711780071258545\n",
            "Step [100/195]\t Loss: 4.72802209854126\n",
            "Step [150/195]\t Loss: 4.811762809753418\n",
            "Epoch 13 loss: 4.747930954664182\n",
            "\n",
            "Step [0/195]\t Loss: 4.711146831512451\n",
            "Step [50/195]\t Loss: 4.693404674530029\n",
            "Step [100/195]\t Loss: 4.715215682983398\n",
            "Step [150/195]\t Loss: 4.688631534576416\n",
            "Epoch 14 loss: 4.7289455731709795\n",
            "\n",
            "Step [0/195]\t Loss: 4.698251724243164\n",
            "Step [50/195]\t Loss: 4.6729207038879395\n",
            "Step [100/195]\t Loss: 4.672980308532715\n",
            "Step [150/195]\t Loss: 4.704647064208984\n",
            "Epoch 15 loss: 4.710396250700339\n",
            "\n",
            "Step [0/195]\t Loss: 4.721626281738281\n",
            "Step [50/195]\t Loss: 4.702298164367676\n",
            "Step [100/195]\t Loss: 4.748869895935059\n",
            "Step [150/195]\t Loss: 4.698863506317139\n",
            "Epoch 16 loss: 4.701737313392835\n",
            "\n",
            "Step [0/195]\t Loss: 4.668122291564941\n",
            "Step [50/195]\t Loss: 4.699117183685303\n",
            "Step [100/195]\t Loss: 4.678449630737305\n",
            "Step [150/195]\t Loss: 4.656341552734375\n",
            "Epoch 17 loss: 4.683246074578701\n",
            "\n",
            "Step [0/195]\t Loss: 4.655993461608887\n",
            "Step [50/195]\t Loss: 4.702347755432129\n",
            "Step [100/195]\t Loss: 4.704501628875732\n",
            "Step [150/195]\t Loss: 4.646096229553223\n",
            "Epoch 18 loss: 4.671169973031068\n",
            "\n",
            "Step [0/195]\t Loss: 4.629058361053467\n",
            "Step [50/195]\t Loss: 4.69591760635376\n",
            "Step [100/195]\t Loss: 4.685019016265869\n",
            "Step [150/195]\t Loss: 4.664042949676514\n",
            "Epoch 19 loss: 4.665059659419915\n",
            "\n",
            "Step [0/195]\t Loss: 4.610903739929199\n",
            "Step [50/195]\t Loss: 4.620277404785156\n",
            "Step [100/195]\t Loss: 4.652894973754883\n",
            "Step [150/195]\t Loss: 4.68342924118042\n",
            "Epoch 20 loss: 4.64995560768323\n",
            "\n",
            "Step [0/195]\t Loss: 4.602230072021484\n",
            "Step [50/195]\t Loss: 4.636779308319092\n",
            "Step [100/195]\t Loss: 4.664353370666504\n",
            "Step [150/195]\t Loss: 4.625133514404297\n",
            "Epoch 21 loss: 4.641148021893623\n",
            "\n",
            "Step [0/195]\t Loss: 4.6013665199279785\n",
            "Step [50/195]\t Loss: 4.60784387588501\n",
            "Step [100/195]\t Loss: 4.646276473999023\n",
            "Step [150/195]\t Loss: 4.6510910987854\n",
            "Epoch 22 loss: 4.634004475520207\n",
            "\n",
            "Step [0/195]\t Loss: 4.584392547607422\n",
            "Step [50/195]\t Loss: 4.624014377593994\n",
            "Step [100/195]\t Loss: 4.611083984375\n",
            "Step [150/195]\t Loss: 4.592772006988525\n",
            "Epoch 23 loss: 4.624818770090739\n",
            "\n",
            "Step [0/195]\t Loss: 4.627260684967041\n",
            "Step [50/195]\t Loss: 4.545713901519775\n",
            "Step [100/195]\t Loss: 4.634815216064453\n",
            "Step [150/195]\t Loss: 4.595039367675781\n",
            "Epoch 24 loss: 4.621244545471974\n",
            "\n",
            "Step [0/195]\t Loss: 4.642514228820801\n",
            "Step [50/195]\t Loss: 4.6019182205200195\n",
            "Step [100/195]\t Loss: 4.6243367195129395\n",
            "Step [150/195]\t Loss: 4.620789051055908\n",
            "Epoch 25 loss: 4.613050294533754\n",
            "\n",
            "Step [0/195]\t Loss: 4.582038879394531\n",
            "Step [50/195]\t Loss: 4.631709098815918\n",
            "Step [100/195]\t Loss: 4.666585445404053\n",
            "Step [150/195]\t Loss: 4.64366340637207\n",
            "Epoch 26 loss: 4.60966236897004\n",
            "\n",
            "Step [0/195]\t Loss: 4.570106506347656\n",
            "Step [50/195]\t Loss: 4.587253570556641\n",
            "Step [100/195]\t Loss: 4.6417951583862305\n",
            "Step [150/195]\t Loss: 4.6711039543151855\n",
            "Epoch 27 loss: 4.600691531254695\n",
            "\n",
            "Step [0/195]\t Loss: 4.606775283813477\n",
            "Step [50/195]\t Loss: 4.5908660888671875\n",
            "Step [100/195]\t Loss: 4.573576927185059\n",
            "Step [150/195]\t Loss: 4.636419773101807\n",
            "Epoch 28 loss: 4.593987222818228\n",
            "\n",
            "Step [0/195]\t Loss: 4.623598575592041\n",
            "Step [50/195]\t Loss: 4.568750381469727\n",
            "Step [100/195]\t Loss: 4.592498779296875\n",
            "Step [150/195]\t Loss: 4.576846122741699\n",
            "Epoch 29 loss: 4.591921644944411\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "parser = argparse.ArgumentParser(description='linear Evaluation')\n",
        "parser.add_argument('--model', default=\"orig\", type=str, help=\"orig/RC/LBE/IP/MIB\")\n",
        "parser.add_argument('--logistic_batch_size', default=128, type=int, metavar='B', help='logistic_batch_size batch size')\n",
        "parser.add_argument('--logistic_epochs', default=100, type=int, help='logistic_epochs')\n",
        "parser.add_argument('--batch_size', default=256, type=int, metavar='B', help='training batch size')\n",
        "parser.add_argument('--workers', default=8, type=int, help='workers')\n",
        "parser.add_argument('--epochs', default=30, type=int, help='epochs')\n",
        "parser.add_argument('--resnet', default=\"resnet18\", type=str, help=\"resnet18/resnet34/resnet50/resnet101/resnet152\")\n",
        "parser.add_argument('--normalize', default=True, action='store_true', help='normalize')\n",
        "parser.add_argument('--projection_dim', default=128, type=int,help='projection_dim')\n",
        "parser.add_argument('--lamb', default=1., type=float, help='weight of regularization term')\n",
        "parser.add_argument('--optimizer', default=\"Adam\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--weight_decay', default=1e-6, type=float, help='weight_decay')\n",
        "parser.add_argument('--lr', default=3e-4, type=float, help='lr')\n",
        "parser.add_argument('--temperature', default=0.5, type=float, help='temperature')\n",
        "parser.add_argument('--model_dir', default='output/checkpoint/', type=str, help='model save path')\n",
        "parser.add_argument('--root', default=\"../datasets\", type=str, help=\"optimizer\")\n",
        "parser.add_argument('--dataset', default='CIFAR10', help='[CIFAR10, CIFAR100, STL-10]')\n",
        "parser.add_argument('--testset', default='CIFAR10', help='[CIFAR10, CIFAR100, STL-10, aircraft, cu_birds, dtd, fashionmnist, mnist, traffic_sign, vgg_flower]')\n",
        "args = parser.parse_args()\n",
        "\n",
        "def train(loader, simclr_model, model, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    model.train()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            if args.model == 'LBE':\n",
        "                _, _, h, _, _, _, _ = simclr_model(x.cuda())\n",
        "            elif args.model == 'MIB':\n",
        "                h, _, _ = simclr_model(x.cuda())\n",
        "            else:\n",
        "                h, _ = simclr_model(x.cuda())\n",
        "        output = model(h)\n",
        "        loss = criterion(output, y.cuda())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()}\")\n",
        "    return loss_epoch\n",
        "\n",
        "def test(loader, simclr_model, model):\n",
        "    right_num = 0\n",
        "    all_num = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "            if args.model == 'LBE':\n",
        "                _, _, h, _, _, _, _ = simclr_model(x)\n",
        "            elif args.model == 'MIB':\n",
        "                h, _, _ = simclr_model(x)\n",
        "            else:\n",
        "                h, _ = simclr_model(x)\n",
        "            output = model(h)\n",
        "\n",
        "            predicted = output.argmax(1)\n",
        "            right_num += (predicted == y).sum().item()\n",
        "            all_num += y.size(0)\n",
        "    accuracy = right_num*100./all_num\n",
        "    return accuracy\n",
        "\n",
        "def load_transform(dataset, size=32):\n",
        "    mean, std = get_data_mean_and_stdev(dataset)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)])\n",
        "    return transform\n",
        "\n",
        "def main():\n",
        "    data = 'non_imagenet'\n",
        "    if args.testset == \"CIFAR10\":\n",
        "        train_dataset = torchvision.datasets.CIFAR10(args.root, train=True, download=True, transform=load_transform('CIFAR10', 32))\n",
        "        test_dataset = torchvision.datasets.CIFAR10(args.root, train=False, download=True, transform=load_transform('CIFAR10', 32))\n",
        "    elif args.testset == \"CIFAR100\":\n",
        "        train_dataset = torchvision.datasets.CIFAR100(args.root, train=True, download=True, transform=load_transform('CIFAR100', 32))\n",
        "        test_dataset = torchvision.datasets.CIFAR100(args.root, train=False, download=True, transform=load_transform('CIFAR100', 32))\n",
        "    elif args.testset == \"STL-10\":\n",
        "        train_dataset = torchvision.datasets.STL10(args.root, split='train', download=True, transform=load_transform('STL-10', 96))\n",
        "        test_dataset = torchvision.datasets.STL10(args.root, split='test', download=True, transform=load_transform('STL-10', 96))\n",
        "    else:\n",
        "        if args.dataset=='STL-10':\n",
        "            train_dataset = DATASET[args.testset](train=True, image_transforms=load_transform(args.testset, 64))\n",
        "            test_dataset = DATASET[args.testset](train=False, image_transforms=load_transform(args.testset, 64))\n",
        "        else:\n",
        "            train_dataset = DATASET[args.testset](train=True, image_transforms=load_transform(args.testset, 32))\n",
        "            test_dataset = DATASET[args.testset](train=False, image_transforms=load_transform(args.testset, 32))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.logistic_batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.workers)\n",
        "    print(\"1\")\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.logistic_batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=args.workers)\n",
        "    print(\"2\")\n",
        "    log_dir = \"output/log/\" + args.testset + '_%s/'%args.model\n",
        "    if not os.path.isdir(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    suffix = args.dataset + '_{}_batch_{}'.format(args.resnet, args.batch_size)\n",
        "    suffix = suffix + '_proj_dim_{}'.format(args.projection_dim) + '_epoch_%d'%args.epochs\n",
        "    args.model_dir = args.model_dir + args.dataset + '_%s/'%args.model\n",
        "    epoch_dir = args.model_dir + suffix + '.pt'\n",
        "    print(\"Loading {}\".format(epoch_dir))\n",
        "    \n",
        "    \n",
        "    simclr_model, _, _, _ = load_model(args, reload_model=True, load_path=epoch_dir, data=data)\n",
        "    simclr_model = simclr_model.cuda()\n",
        "    simclr_model.eval()\n",
        "\n",
        "    print(\"3\")\n",
        "    # Logistic Regression\n",
        "    n_classes = get_data_nclass(args.testset)\n",
        "    model = LogisticRegression(simclr_model.n_features, n_classes).cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [60, 80], gamma=0.1)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    print(\"4\")\n",
        "    best_acc = 0.\n",
        "    test_log_file = open(log_dir + suffix + '_LR.txt', \"w\")\n",
        "    for epoch in range(args.logistic_epochs):\n",
        "        print(epoch, \" -> \", args.logistic_epochs)\n",
        "        \n",
        "        loss_epoch = train(train_loader, simclr_model, model, criterion, optimizer)\n",
        "        print(\"Train Epoch [{}]\\t Average loss: {}\".format(epoch, loss_epoch/len(train_loader)))\n",
        "        print(\"Train Epoch [{}]\\t Average loss: {}\".format(epoch, loss_epoch/len(train_loader)), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "\n",
        "        # final testing\n",
        "        test_current_acc = test(test_loader, simclr_model, model)\n",
        "        if test_current_acc > best_acc:\n",
        "            best_acc = test_current_acc\n",
        "        print(\"Test Epoch [{}]\\t Accuracy: {}\\t Best Accuracy: {}\\n\".format(epoch, test_current_acc, best_acc))\n",
        "        print(\"Test Epoch [{}]\\t Accuracy: {}\\t Best Accuracy: {}\\n\".format(epoch, test_current_acc, best_acc), file=test_log_file)\n",
        "        test_log_file.flush()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Final Best Accuracy: {}\".format(best_acc))\n",
        "    print(\"Final Best Accuracy: {}\".format(best_acc), file=test_log_file)\n",
        "    test_log_file.flush()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApPSPg5B0Dq5",
        "outputId": "e141ae25-3d46-449e-f3ed-1da349392279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "Loading output/checkpoint/CIFAR10_orig/CIFAR10_resnet18_batch_256_proj_dim_128_epoch_30.pt\n",
            "3\n",
            "4\n",
            "0  ->  100\n",
            "Step [0/390]\t Loss: 2.5344059467315674\n",
            "Step [50/390]\t Loss: 0.9289258718490601\n",
            "Step [100/390]\t Loss: 0.8767004609107971\n",
            "Step [150/390]\t Loss: 0.8008785247802734\n",
            "Step [200/390]\t Loss: 1.17722487449646\n",
            "Step [250/390]\t Loss: 0.8149793744087219\n",
            "Step [300/390]\t Loss: 0.9343677163124084\n",
            "Step [350/390]\t Loss: 0.9740603566169739\n",
            "Train Epoch [0]\t Average loss: 0.9254412767214653\n",
            "Test Epoch [0]\t Accuracy: 68.83\t Best Accuracy: 68.83\n",
            "\n",
            "1  ->  100\n",
            "Step [0/390]\t Loss: 0.8179484605789185\n",
            "Step [50/390]\t Loss: 0.801536500453949\n",
            "Step [100/390]\t Loss: 0.7711110711097717\n",
            "Step [150/390]\t Loss: 0.7958533763885498\n",
            "Step [200/390]\t Loss: 0.8303967118263245\n",
            "Step [250/390]\t Loss: 0.7836562395095825\n",
            "Step [300/390]\t Loss: 1.0504257678985596\n",
            "Step [350/390]\t Loss: 0.7790979146957397\n",
            "Train Epoch [1]\t Average loss: 0.8387197581621316\n",
            "Test Epoch [1]\t Accuracy: 70.57\t Best Accuracy: 70.57\n",
            "\n",
            "2  ->  100\n",
            "Step [0/390]\t Loss: 0.8241272568702698\n",
            "Step [50/390]\t Loss: 0.945694625377655\n",
            "Step [100/390]\t Loss: 0.6216844320297241\n",
            "Step [150/390]\t Loss: 0.7509012222290039\n",
            "Step [200/390]\t Loss: 0.608971893787384\n",
            "Step [250/390]\t Loss: 0.8425702452659607\n",
            "Step [300/390]\t Loss: 0.7816411256790161\n",
            "Step [350/390]\t Loss: 0.7590353488922119\n",
            "Train Epoch [2]\t Average loss: 0.8110708349026167\n",
            "Test Epoch [2]\t Accuracy: 70.76\t Best Accuracy: 70.76\n",
            "\n",
            "3  ->  100\n",
            "Step [0/390]\t Loss: 0.589758574962616\n",
            "Step [50/390]\t Loss: 0.9238775968551636\n",
            "Step [100/390]\t Loss: 0.8040505647659302\n",
            "Step [150/390]\t Loss: 0.8956804275512695\n",
            "Step [200/390]\t Loss: 0.6878008842468262\n",
            "Step [250/390]\t Loss: 0.882847785949707\n",
            "Step [300/390]\t Loss: 0.8608838319778442\n",
            "Step [350/390]\t Loss: 0.8011808395385742\n",
            "Train Epoch [3]\t Average loss: 0.7847273695163238\n",
            "Test Epoch [3]\t Accuracy: 71.42\t Best Accuracy: 71.42\n",
            "\n",
            "4  ->  100\n",
            "Step [0/390]\t Loss: 0.866987407207489\n",
            "Step [50/390]\t Loss: 0.8322912454605103\n",
            "Step [100/390]\t Loss: 0.9353845715522766\n",
            "Step [150/390]\t Loss: 0.5572769641876221\n",
            "Step [200/390]\t Loss: 0.784566342830658\n",
            "Step [250/390]\t Loss: 0.8118947148323059\n",
            "Step [300/390]\t Loss: 0.9681059122085571\n",
            "Step [350/390]\t Loss: 0.9797654151916504\n",
            "Train Epoch [4]\t Average loss: 0.7753968954086303\n",
            "Test Epoch [4]\t Accuracy: 73.54\t Best Accuracy: 73.54\n",
            "\n",
            "5  ->  100\n",
            "Step [0/390]\t Loss: 0.820477306842804\n",
            "Step [50/390]\t Loss: 0.7074682116508484\n",
            "Step [100/390]\t Loss: 0.7219539880752563\n",
            "Step [150/390]\t Loss: 0.6346822381019592\n",
            "Step [200/390]\t Loss: 0.9389251470565796\n",
            "Step [250/390]\t Loss: 0.7329548597335815\n",
            "Step [300/390]\t Loss: 0.8856418132781982\n",
            "Step [350/390]\t Loss: 0.7991781234741211\n",
            "Train Epoch [5]\t Average loss: 0.7750233460695316\n",
            "Test Epoch [5]\t Accuracy: 71.61\t Best Accuracy: 73.54\n",
            "\n",
            "6  ->  100\n",
            "Step [0/390]\t Loss: 0.8443968892097473\n",
            "Step [50/390]\t Loss: 0.8328841924667358\n",
            "Step [100/390]\t Loss: 0.6177679300308228\n",
            "Step [150/390]\t Loss: 0.8042564392089844\n",
            "Step [200/390]\t Loss: 0.6497487425804138\n",
            "Step [250/390]\t Loss: 0.8306543827056885\n",
            "Step [300/390]\t Loss: 0.9087809324264526\n",
            "Step [350/390]\t Loss: 0.7206974029541016\n",
            "Train Epoch [6]\t Average loss: 0.7642284752466739\n",
            "Test Epoch [6]\t Accuracy: 73.53\t Best Accuracy: 73.54\n",
            "\n",
            "7  ->  100\n",
            "Step [0/390]\t Loss: 0.622541606426239\n",
            "Step [50/390]\t Loss: 0.8049030303955078\n",
            "Step [100/390]\t Loss: 0.8931748270988464\n",
            "Step [150/390]\t Loss: 0.7129507660865784\n",
            "Step [200/390]\t Loss: 0.7833709120750427\n",
            "Step [250/390]\t Loss: 0.8714590072631836\n",
            "Step [300/390]\t Loss: 0.6963585019111633\n",
            "Step [350/390]\t Loss: 0.6684777140617371\n",
            "Train Epoch [7]\t Average loss: 0.7598650721403268\n",
            "Test Epoch [7]\t Accuracy: 74.04\t Best Accuracy: 74.04\n",
            "\n",
            "8  ->  100\n",
            "Step [0/390]\t Loss: 0.7277676463127136\n",
            "Step [50/390]\t Loss: 0.734928548336029\n",
            "Step [100/390]\t Loss: 0.7537229657173157\n",
            "Step [150/390]\t Loss: 0.6373142600059509\n",
            "Step [200/390]\t Loss: 0.9573909640312195\n",
            "Step [250/390]\t Loss: 0.9112261533737183\n",
            "Step [300/390]\t Loss: 0.7681317329406738\n",
            "Step [350/390]\t Loss: 0.727940559387207\n",
            "Train Epoch [8]\t Average loss: 0.7488682503883656\n",
            "Test Epoch [8]\t Accuracy: 72.74\t Best Accuracy: 74.04\n",
            "\n",
            "9  ->  100\n",
            "Step [0/390]\t Loss: 0.7246832251548767\n",
            "Step [50/390]\t Loss: 0.7367241382598877\n",
            "Step [100/390]\t Loss: 0.7208181023597717\n",
            "Step [150/390]\t Loss: 0.9103864431381226\n",
            "Step [200/390]\t Loss: 0.8772530555725098\n",
            "Step [250/390]\t Loss: 0.5616005659103394\n",
            "Step [300/390]\t Loss: 0.8201392889022827\n",
            "Step [350/390]\t Loss: 1.1046617031097412\n",
            "Train Epoch [9]\t Average loss: 0.7447072055095282\n",
            "Test Epoch [9]\t Accuracy: 73.74\t Best Accuracy: 74.04\n",
            "\n",
            "10  ->  100\n",
            "Step [0/390]\t Loss: 0.7441707253456116\n",
            "Step [50/390]\t Loss: 0.6251056790351868\n",
            "Step [100/390]\t Loss: 0.7562417984008789\n",
            "Step [150/390]\t Loss: 0.8696907162666321\n",
            "Step [200/390]\t Loss: 0.832278847694397\n",
            "Step [250/390]\t Loss: 0.668189287185669\n",
            "Step [300/390]\t Loss: 0.6455633044242859\n",
            "Step [350/390]\t Loss: 0.5531941056251526\n",
            "Train Epoch [10]\t Average loss: 0.7367288421361874\n",
            "Test Epoch [10]\t Accuracy: 74.04\t Best Accuracy: 74.04\n",
            "\n",
            "11  ->  100\n",
            "Step [0/390]\t Loss: 0.6031007170677185\n",
            "Step [50/390]\t Loss: 0.5859017968177795\n",
            "Step [100/390]\t Loss: 0.6426554322242737\n",
            "Step [150/390]\t Loss: 0.6385877728462219\n",
            "Step [200/390]\t Loss: 0.7856158018112183\n",
            "Step [250/390]\t Loss: 0.6298791170120239\n",
            "Step [300/390]\t Loss: 0.6107791662216187\n",
            "Step [350/390]\t Loss: 0.7865839004516602\n",
            "Train Epoch [11]\t Average loss: 0.7305555194616318\n",
            "Test Epoch [11]\t Accuracy: 73.76\t Best Accuracy: 74.04\n",
            "\n",
            "12  ->  100\n",
            "Step [0/390]\t Loss: 0.7497610449790955\n",
            "Step [50/390]\t Loss: 0.73476642370224\n",
            "Step [100/390]\t Loss: 0.8084295988082886\n",
            "Step [150/390]\t Loss: 0.6758330464363098\n",
            "Step [200/390]\t Loss: 0.6835790276527405\n",
            "Step [250/390]\t Loss: 0.5777786374092102\n",
            "Step [300/390]\t Loss: 0.828386664390564\n",
            "Step [350/390]\t Loss: 0.6761202216148376\n",
            "Train Epoch [12]\t Average loss: 0.7376632590324451\n",
            "Test Epoch [12]\t Accuracy: 74.32\t Best Accuracy: 74.32\n",
            "\n",
            "13  ->  100\n",
            "Step [0/390]\t Loss: 0.6922885179519653\n",
            "Step [50/390]\t Loss: 0.6214616894721985\n",
            "Step [100/390]\t Loss: 0.7931829690933228\n",
            "Step [150/390]\t Loss: 0.7191250324249268\n",
            "Step [200/390]\t Loss: 0.7618494033813477\n",
            "Step [250/390]\t Loss: 0.7299114465713501\n",
            "Step [300/390]\t Loss: 0.8804999589920044\n",
            "Step [350/390]\t Loss: 0.6719263195991516\n",
            "Train Epoch [13]\t Average loss: 0.7169788851951941\n",
            "Test Epoch [13]\t Accuracy: 72.28\t Best Accuracy: 74.32\n",
            "\n",
            "14  ->  100\n",
            "Step [0/390]\t Loss: 0.5574167966842651\n",
            "Step [50/390]\t Loss: 1.1819252967834473\n",
            "Step [100/390]\t Loss: 0.7478082180023193\n",
            "Step [150/390]\t Loss: 0.5312134027481079\n",
            "Step [200/390]\t Loss: 0.5462321639060974\n",
            "Step [250/390]\t Loss: 0.7260425090789795\n",
            "Step [300/390]\t Loss: 0.6575448513031006\n",
            "Step [350/390]\t Loss: 0.802271842956543\n",
            "Train Epoch [14]\t Average loss: 0.7273244239580937\n",
            "Test Epoch [14]\t Accuracy: 75.18\t Best Accuracy: 75.18\n",
            "\n",
            "15  ->  100\n",
            "Step [0/390]\t Loss: 0.7748988270759583\n",
            "Step [50/390]\t Loss: 0.6183893084526062\n",
            "Step [100/390]\t Loss: 0.7190870642662048\n",
            "Step [150/390]\t Loss: 0.6450897455215454\n",
            "Step [200/390]\t Loss: 0.6987718939781189\n",
            "Step [250/390]\t Loss: 0.7009594440460205\n",
            "Step [300/390]\t Loss: 0.6969835162162781\n",
            "Step [350/390]\t Loss: 0.6754773855209351\n",
            "Train Epoch [15]\t Average loss: 0.7158169285609172\n",
            "Test Epoch [15]\t Accuracy: 73.83\t Best Accuracy: 75.18\n",
            "\n",
            "16  ->  100\n",
            "Step [0/390]\t Loss: 0.6701619029045105\n",
            "Step [50/390]\t Loss: 0.8019343018531799\n",
            "Step [100/390]\t Loss: 0.6615579724311829\n",
            "Step [150/390]\t Loss: 0.7256500124931335\n",
            "Step [200/390]\t Loss: 0.5244815349578857\n",
            "Step [250/390]\t Loss: 0.9403239488601685\n",
            "Step [300/390]\t Loss: 0.6268903017044067\n",
            "Step [350/390]\t Loss: 0.7409918904304504\n",
            "Train Epoch [16]\t Average loss: 0.7224696695804596\n",
            "Test Epoch [16]\t Accuracy: 73.59\t Best Accuracy: 75.18\n",
            "\n",
            "17  ->  100\n",
            "Step [0/390]\t Loss: 0.6527130007743835\n",
            "Step [50/390]\t Loss: 0.7081859707832336\n",
            "Step [100/390]\t Loss: 0.7711382508277893\n",
            "Step [150/390]\t Loss: 0.7798800468444824\n",
            "Step [200/390]\t Loss: 0.6999396085739136\n",
            "Step [250/390]\t Loss: 0.847061276435852\n",
            "Step [300/390]\t Loss: 0.8088018894195557\n",
            "Step [350/390]\t Loss: 0.6617118716239929\n",
            "Train Epoch [17]\t Average loss: 0.7198564701355421\n",
            "Test Epoch [17]\t Accuracy: 73.77\t Best Accuracy: 75.18\n",
            "\n",
            "18  ->  100\n",
            "Step [0/390]\t Loss: 0.7491397857666016\n",
            "Step [50/390]\t Loss: 0.6832726001739502\n",
            "Step [100/390]\t Loss: 0.7896091341972351\n",
            "Step [150/390]\t Loss: 0.7117931246757507\n",
            "Step [200/390]\t Loss: 0.7777329087257385\n",
            "Step [250/390]\t Loss: 0.6806865930557251\n",
            "Step [300/390]\t Loss: 0.7751598358154297\n",
            "Step [350/390]\t Loss: 0.722506582736969\n",
            "Train Epoch [18]\t Average loss: 0.7226234390949592\n",
            "Test Epoch [18]\t Accuracy: 69.58\t Best Accuracy: 75.18\n",
            "\n",
            "19  ->  100\n",
            "Step [0/390]\t Loss: 0.7474054098129272\n",
            "Step [50/390]\t Loss: 0.6563075184822083\n",
            "Step [100/390]\t Loss: 0.7897248268127441\n",
            "Step [150/390]\t Loss: 0.6388722062110901\n",
            "Step [200/390]\t Loss: 0.7156626582145691\n",
            "Step [250/390]\t Loss: 0.6793040037155151\n",
            "Step [300/390]\t Loss: 0.6900700926780701\n",
            "Step [350/390]\t Loss: 0.8209086656570435\n",
            "Train Epoch [19]\t Average loss: 0.7237597344013361\n",
            "Test Epoch [19]\t Accuracy: 72.55\t Best Accuracy: 75.18\n",
            "\n",
            "20  ->  100\n",
            "Step [0/390]\t Loss: 0.8503313064575195\n",
            "Step [50/390]\t Loss: 0.623993992805481\n",
            "Step [100/390]\t Loss: 0.6288049221038818\n",
            "Step [150/390]\t Loss: 0.7095673680305481\n",
            "Step [200/390]\t Loss: 0.6493887901306152\n",
            "Step [250/390]\t Loss: 0.6472749710083008\n",
            "Step [300/390]\t Loss: 0.5971352458000183\n",
            "Step [350/390]\t Loss: 0.5610297322273254\n",
            "Train Epoch [20]\t Average loss: 0.7143784341903833\n",
            "Test Epoch [20]\t Accuracy: 75.2\t Best Accuracy: 75.2\n",
            "\n",
            "21  ->  100\n",
            "Step [0/390]\t Loss: 0.8397927284240723\n",
            "Step [50/390]\t Loss: 0.7506580352783203\n",
            "Step [100/390]\t Loss: 0.6644175052642822\n",
            "Step [150/390]\t Loss: 0.9212580919265747\n",
            "Step [200/390]\t Loss: 0.7604308128356934\n",
            "Step [250/390]\t Loss: 1.0050123929977417\n",
            "Step [300/390]\t Loss: 0.607122004032135\n",
            "Step [350/390]\t Loss: 0.8499554395675659\n",
            "Train Epoch [21]\t Average loss: 0.7031896479618855\n",
            "Test Epoch [21]\t Accuracy: 73.97\t Best Accuracy: 75.2\n",
            "\n",
            "22  ->  100\n",
            "Step [0/390]\t Loss: 0.8059182167053223\n",
            "Step [50/390]\t Loss: 0.8033682703971863\n",
            "Step [100/390]\t Loss: 0.6403924822807312\n",
            "Step [150/390]\t Loss: 0.5732151865959167\n",
            "Step [200/390]\t Loss: 0.5281374454498291\n",
            "Step [250/390]\t Loss: 0.6763330698013306\n",
            "Step [300/390]\t Loss: 0.802359402179718\n",
            "Step [350/390]\t Loss: 0.5481709241867065\n",
            "Train Epoch [22]\t Average loss: 0.7054764618476231\n",
            "Test Epoch [22]\t Accuracy: 74.68\t Best Accuracy: 75.2\n",
            "\n",
            "23  ->  100\n",
            "Step [0/390]\t Loss: 0.6059340834617615\n",
            "Step [50/390]\t Loss: 0.7303584814071655\n",
            "Step [100/390]\t Loss: 0.8296651840209961\n",
            "Step [150/390]\t Loss: 0.7821284532546997\n",
            "Step [200/390]\t Loss: 0.6055618524551392\n",
            "Step [250/390]\t Loss: 0.7165963649749756\n",
            "Step [300/390]\t Loss: 0.7496223449707031\n",
            "Step [350/390]\t Loss: 0.6165609359741211\n",
            "Train Epoch [23]\t Average loss: 0.699495861851252\n",
            "Test Epoch [23]\t Accuracy: 71.21\t Best Accuracy: 75.2\n",
            "\n",
            "24  ->  100\n",
            "Step [0/390]\t Loss: 0.5802330374717712\n",
            "Step [50/390]\t Loss: 0.8334628343582153\n",
            "Step [100/390]\t Loss: 0.7738387584686279\n",
            "Step [150/390]\t Loss: 0.5743119716644287\n",
            "Step [200/390]\t Loss: 0.6718044281005859\n",
            "Step [250/390]\t Loss: 0.8503091931343079\n",
            "Step [300/390]\t Loss: 0.7299916744232178\n",
            "Step [350/390]\t Loss: 0.5899378061294556\n",
            "Train Epoch [24]\t Average loss: 0.7116574856715324\n",
            "Test Epoch [24]\t Accuracy: 74.85\t Best Accuracy: 75.2\n",
            "\n",
            "25  ->  100\n",
            "Step [0/390]\t Loss: 0.635623574256897\n",
            "Step [50/390]\t Loss: 0.7579807043075562\n",
            "Step [100/390]\t Loss: 0.8960326910018921\n",
            "Step [150/390]\t Loss: 0.7997438311576843\n",
            "Step [200/390]\t Loss: 0.5845382213592529\n",
            "Step [250/390]\t Loss: 0.6903051137924194\n",
            "Step [300/390]\t Loss: 0.711896538734436\n",
            "Step [350/390]\t Loss: 0.7766984105110168\n",
            "Train Epoch [25]\t Average loss: 0.7104316554008386\n",
            "Test Epoch [25]\t Accuracy: 73.76\t Best Accuracy: 75.2\n",
            "\n",
            "26  ->  100\n",
            "Step [0/390]\t Loss: 0.8133998513221741\n",
            "Step [50/390]\t Loss: 0.7181617617607117\n",
            "Step [100/390]\t Loss: 0.6659073829650879\n",
            "Step [150/390]\t Loss: 0.8477571606636047\n",
            "Step [200/390]\t Loss: 0.5931816697120667\n",
            "Step [250/390]\t Loss: 0.7989362478256226\n",
            "Step [300/390]\t Loss: 0.5483981966972351\n",
            "Step [350/390]\t Loss: 0.7959708571434021\n",
            "Train Epoch [26]\t Average loss: 0.6993910889594983\n",
            "Test Epoch [26]\t Accuracy: 72.93\t Best Accuracy: 75.2\n",
            "\n",
            "27  ->  100\n",
            "Step [0/390]\t Loss: 0.5735254883766174\n",
            "Step [50/390]\t Loss: 0.7168546915054321\n",
            "Step [100/390]\t Loss: 0.7131166458129883\n",
            "Step [150/390]\t Loss: 0.5660380125045776\n",
            "Step [200/390]\t Loss: 0.6924643516540527\n",
            "Step [250/390]\t Loss: 0.9314883351325989\n",
            "Step [300/390]\t Loss: 0.7501243352890015\n",
            "Step [350/390]\t Loss: 0.7846454977989197\n",
            "Train Epoch [27]\t Average loss: 0.7040480053577668\n",
            "Test Epoch [27]\t Accuracy: 73.6\t Best Accuracy: 75.2\n",
            "\n",
            "28  ->  100\n",
            "Step [0/390]\t Loss: 0.6859203577041626\n",
            "Step [50/390]\t Loss: 0.7098260521888733\n",
            "Step [100/390]\t Loss: 0.7645452618598938\n",
            "Step [150/390]\t Loss: 0.5879332423210144\n",
            "Step [200/390]\t Loss: 0.7308050394058228\n",
            "Step [250/390]\t Loss: 0.6244826912879944\n",
            "Step [300/390]\t Loss: 0.6289792656898499\n",
            "Step [350/390]\t Loss: 0.6774049401283264\n",
            "Train Epoch [28]\t Average loss: 0.7056973434411562\n",
            "Test Epoch [28]\t Accuracy: 72.57\t Best Accuracy: 75.2\n",
            "\n",
            "29  ->  100\n",
            "Step [0/390]\t Loss: 0.605915904045105\n",
            "Step [50/390]\t Loss: 0.6309371590614319\n",
            "Step [100/390]\t Loss: 0.7436870336532593\n",
            "Step [150/390]\t Loss: 0.6463702917098999\n",
            "Step [200/390]\t Loss: 0.7396978735923767\n",
            "Step [250/390]\t Loss: 0.6950188875198364\n",
            "Step [300/390]\t Loss: 0.6788575649261475\n",
            "Step [350/390]\t Loss: 0.6917921304702759\n",
            "Train Epoch [29]\t Average loss: 0.6911941438149183\n",
            "Test Epoch [29]\t Accuracy: 74.09\t Best Accuracy: 75.2\n",
            "\n",
            "30  ->  100\n",
            "Step [0/390]\t Loss: 0.6277064681053162\n",
            "Step [50/390]\t Loss: 0.6542554497718811\n",
            "Step [100/390]\t Loss: 0.7902096509933472\n",
            "Step [150/390]\t Loss: 0.8187870383262634\n",
            "Step [200/390]\t Loss: 0.6520795226097107\n",
            "Step [250/390]\t Loss: 0.7841180562973022\n",
            "Step [300/390]\t Loss: 0.5545447468757629\n",
            "Step [350/390]\t Loss: 0.5951510071754456\n",
            "Train Epoch [30]\t Average loss: 0.681381676135919\n",
            "Test Epoch [30]\t Accuracy: 73.03\t Best Accuracy: 75.2\n",
            "\n",
            "31  ->  100\n",
            "Step [0/390]\t Loss: 0.5567729473114014\n",
            "Step [50/390]\t Loss: 0.7375217080116272\n",
            "Step [100/390]\t Loss: 0.6813437342643738\n",
            "Step [150/390]\t Loss: 0.7437386512756348\n",
            "Step [200/390]\t Loss: 0.708106279373169\n",
            "Step [250/390]\t Loss: 0.6099241971969604\n",
            "Step [300/390]\t Loss: 0.6422701478004456\n",
            "Step [350/390]\t Loss: 0.498330295085907\n",
            "Train Epoch [31]\t Average loss: 0.6965824498580052\n",
            "Test Epoch [31]\t Accuracy: 74.41\t Best Accuracy: 75.2\n",
            "\n",
            "32  ->  100\n",
            "Step [0/390]\t Loss: 0.6685985326766968\n",
            "Step [50/390]\t Loss: 0.7406162619590759\n",
            "Step [100/390]\t Loss: 0.6151456236839294\n",
            "Step [150/390]\t Loss: 0.6071256995201111\n",
            "Step [200/390]\t Loss: 0.8182932734489441\n",
            "Step [250/390]\t Loss: 0.5918045043945312\n",
            "Step [300/390]\t Loss: 0.6698628067970276\n",
            "Step [350/390]\t Loss: 0.8234217762947083\n",
            "Train Epoch [32]\t Average loss: 0.6829759310453366\n",
            "Test Epoch [32]\t Accuracy: 73.81\t Best Accuracy: 75.2\n",
            "\n",
            "33  ->  100\n",
            "Step [0/390]\t Loss: 0.8249258399009705\n",
            "Step [50/390]\t Loss: 0.6551499366760254\n",
            "Step [100/390]\t Loss: 0.8906382322311401\n",
            "Step [150/390]\t Loss: 0.7511390447616577\n",
            "Step [200/390]\t Loss: 0.8648806810379028\n",
            "Step [250/390]\t Loss: 0.7439015507698059\n",
            "Step [300/390]\t Loss: 0.5946973562240601\n",
            "Step [350/390]\t Loss: 0.6060538291931152\n",
            "Train Epoch [33]\t Average loss: 0.6899143711114541\n",
            "Test Epoch [33]\t Accuracy: 74.99\t Best Accuracy: 75.2\n",
            "\n",
            "34  ->  100\n",
            "Step [0/390]\t Loss: 0.6868962049484253\n",
            "Step [50/390]\t Loss: 0.5610259771347046\n",
            "Step [100/390]\t Loss: 0.7191966772079468\n",
            "Step [150/390]\t Loss: 0.8336268663406372\n",
            "Step [200/390]\t Loss: 0.6160115003585815\n",
            "Step [250/390]\t Loss: 0.6727983355522156\n",
            "Step [300/390]\t Loss: 0.8212435245513916\n",
            "Step [350/390]\t Loss: 0.6110589504241943\n",
            "Train Epoch [34]\t Average loss: 0.6836646472796416\n",
            "Test Epoch [34]\t Accuracy: 74.51\t Best Accuracy: 75.2\n",
            "\n",
            "35  ->  100\n",
            "Step [0/390]\t Loss: 0.4568205177783966\n",
            "Step [50/390]\t Loss: 0.8656153082847595\n",
            "Step [100/390]\t Loss: 0.6269383430480957\n",
            "Step [150/390]\t Loss: 0.4874635934829712\n",
            "Step [200/390]\t Loss: 0.782372236251831\n",
            "Step [250/390]\t Loss: 0.6380579471588135\n",
            "Step [300/390]\t Loss: 0.6612098813056946\n",
            "Step [350/390]\t Loss: 0.838316798210144\n",
            "Train Epoch [35]\t Average loss: 0.6980698145352877\n",
            "Test Epoch [35]\t Accuracy: 74.1\t Best Accuracy: 75.2\n",
            "\n",
            "36  ->  100\n",
            "Step [0/390]\t Loss: 0.7543504238128662\n",
            "Step [50/390]\t Loss: 0.739519476890564\n",
            "Step [100/390]\t Loss: 0.6514019966125488\n",
            "Step [150/390]\t Loss: 0.8322635293006897\n",
            "Step [200/390]\t Loss: 0.9439889192581177\n",
            "Step [250/390]\t Loss: 0.5701653361320496\n",
            "Step [300/390]\t Loss: 0.6409619450569153\n",
            "Step [350/390]\t Loss: 0.8160856366157532\n",
            "Train Epoch [36]\t Average loss: 0.698594321960058\n",
            "Test Epoch [36]\t Accuracy: 74.31\t Best Accuracy: 75.2\n",
            "\n",
            "37  ->  100\n",
            "Step [0/390]\t Loss: 0.5510343909263611\n",
            "Step [50/390]\t Loss: 0.837328314781189\n",
            "Step [100/390]\t Loss: 0.6237382888793945\n",
            "Step [150/390]\t Loss: 0.7778222560882568\n",
            "Step [200/390]\t Loss: 0.8074214458465576\n",
            "Step [250/390]\t Loss: 0.4764792025089264\n",
            "Step [300/390]\t Loss: 0.6847411394119263\n",
            "Step [350/390]\t Loss: 0.760085940361023\n",
            "Train Epoch [37]\t Average loss: 0.6831261705129574\n",
            "Test Epoch [37]\t Accuracy: 73.14\t Best Accuracy: 75.2\n",
            "\n",
            "38  ->  100\n",
            "Step [0/390]\t Loss: 0.8526713848114014\n",
            "Step [50/390]\t Loss: 0.6747664213180542\n",
            "Step [100/390]\t Loss: 0.50993812084198\n",
            "Step [150/390]\t Loss: 0.6812223196029663\n",
            "Step [200/390]\t Loss: 0.7013058066368103\n",
            "Step [250/390]\t Loss: 0.9133027195930481\n",
            "Step [300/390]\t Loss: 0.6204535365104675\n",
            "Step [350/390]\t Loss: 0.7636198997497559\n",
            "Train Epoch [38]\t Average loss: 0.6935967224530685\n",
            "Test Epoch [38]\t Accuracy: 74.74\t Best Accuracy: 75.2\n",
            "\n",
            "39  ->  100\n",
            "Step [0/390]\t Loss: 0.6101001501083374\n",
            "Step [50/390]\t Loss: 0.49754399061203003\n",
            "Step [100/390]\t Loss: 0.741317629814148\n",
            "Step [150/390]\t Loss: 0.726046085357666\n",
            "Step [200/390]\t Loss: 0.6254847049713135\n",
            "Step [250/390]\t Loss: 0.72813481092453\n",
            "Step [300/390]\t Loss: 0.824268102645874\n",
            "Step [350/390]\t Loss: 0.775894045829773\n",
            "Train Epoch [39]\t Average loss: 0.686578567364277\n",
            "Test Epoch [39]\t Accuracy: 74.46\t Best Accuracy: 75.2\n",
            "\n",
            "40  ->  100\n",
            "Step [0/390]\t Loss: 0.806081235408783\n",
            "Step [50/390]\t Loss: 0.5713415145874023\n",
            "Step [100/390]\t Loss: 0.7770790457725525\n",
            "Step [150/390]\t Loss: 0.7668961882591248\n",
            "Step [200/390]\t Loss: 0.6976646184921265\n",
            "Step [250/390]\t Loss: 0.6682910323143005\n",
            "Step [300/390]\t Loss: 0.8903761506080627\n",
            "Step [350/390]\t Loss: 0.5731512904167175\n",
            "Train Epoch [40]\t Average loss: 0.6925682524075875\n",
            "Test Epoch [40]\t Accuracy: 74.89\t Best Accuracy: 75.2\n",
            "\n",
            "41  ->  100\n",
            "Step [0/390]\t Loss: 0.724540650844574\n",
            "Step [50/390]\t Loss: 0.7857693433761597\n",
            "Step [100/390]\t Loss: 0.5233027338981628\n",
            "Step [150/390]\t Loss: 0.5502225756645203\n",
            "Step [200/390]\t Loss: 0.6839378476142883\n",
            "Step [250/390]\t Loss: 0.6527054905891418\n",
            "Step [300/390]\t Loss: 0.7199258208274841\n",
            "Step [350/390]\t Loss: 0.5557397603988647\n",
            "Train Epoch [41]\t Average loss: 0.6770741789768904\n",
            "Test Epoch [41]\t Accuracy: 74.04\t Best Accuracy: 75.2\n",
            "\n",
            "42  ->  100\n",
            "Step [0/390]\t Loss: 0.7738013863563538\n",
            "Step [50/390]\t Loss: 0.7879558801651001\n",
            "Step [100/390]\t Loss: 0.488966166973114\n",
            "Step [150/390]\t Loss: 0.765155017375946\n",
            "Step [200/390]\t Loss: 0.6572561264038086\n",
            "Step [250/390]\t Loss: 0.661797046661377\n",
            "Step [300/390]\t Loss: 0.5987938046455383\n",
            "Step [350/390]\t Loss: 0.7574261426925659\n",
            "Train Epoch [42]\t Average loss: 0.6858051016544684\n",
            "Test Epoch [42]\t Accuracy: 71.96\t Best Accuracy: 75.2\n",
            "\n",
            "43  ->  100\n",
            "Step [0/390]\t Loss: 0.7889301180839539\n",
            "Step [50/390]\t Loss: 0.736962080001831\n",
            "Step [100/390]\t Loss: 0.6573528051376343\n",
            "Step [150/390]\t Loss: 0.8562887907028198\n",
            "Step [200/390]\t Loss: 0.6549832820892334\n",
            "Step [250/390]\t Loss: 0.5823084115982056\n",
            "Step [300/390]\t Loss: 0.603532612323761\n",
            "Step [350/390]\t Loss: 0.6965387463569641\n",
            "Train Epoch [43]\t Average loss: 0.6865023743647796\n",
            "Test Epoch [43]\t Accuracy: 74.09\t Best Accuracy: 75.2\n",
            "\n",
            "44  ->  100\n",
            "Step [0/390]\t Loss: 0.5830955505371094\n",
            "Step [50/390]\t Loss: 0.5542652606964111\n",
            "Step [100/390]\t Loss: 0.5464915633201599\n",
            "Step [150/390]\t Loss: 0.7984834909439087\n",
            "Step [200/390]\t Loss: 0.5865936279296875\n",
            "Step [250/390]\t Loss: 0.8141664266586304\n",
            "Step [300/390]\t Loss: 0.5411843061447144\n",
            "Step [350/390]\t Loss: 0.687740683555603\n",
            "Train Epoch [44]\t Average loss: 0.6826745640008878\n",
            "Test Epoch [44]\t Accuracy: 73.7\t Best Accuracy: 75.2\n",
            "\n",
            "45  ->  100\n",
            "Step [0/390]\t Loss: 0.6972348690032959\n",
            "Step [50/390]\t Loss: 0.6208195686340332\n",
            "Step [100/390]\t Loss: 0.6848778128623962\n",
            "Step [150/390]\t Loss: 0.7735190391540527\n",
            "Step [200/390]\t Loss: 0.6026938557624817\n",
            "Step [250/390]\t Loss: 0.8603925704956055\n",
            "Step [300/390]\t Loss: 0.6833847761154175\n",
            "Step [350/390]\t Loss: 0.5027379393577576\n",
            "Train Epoch [45]\t Average loss: 0.6717657819008216\n",
            "Test Epoch [45]\t Accuracy: 71.58\t Best Accuracy: 75.2\n",
            "\n",
            "46  ->  100\n",
            "Step [0/390]\t Loss: 0.669979453086853\n",
            "Step [50/390]\t Loss: 0.630310595035553\n",
            "Step [100/390]\t Loss: 0.5241262316703796\n",
            "Step [150/390]\t Loss: 0.6215667128562927\n",
            "Step [200/390]\t Loss: 0.8652923703193665\n",
            "Step [250/390]\t Loss: 0.6614463329315186\n",
            "Step [300/390]\t Loss: 0.4783521592617035\n",
            "Step [350/390]\t Loss: 0.6984480023384094\n",
            "Train Epoch [46]\t Average loss: 0.6844638210840714\n",
            "Test Epoch [46]\t Accuracy: 73.63\t Best Accuracy: 75.2\n",
            "\n",
            "47  ->  100\n",
            "Step [0/390]\t Loss: 0.6923390030860901\n",
            "Step [50/390]\t Loss: 0.7107419371604919\n",
            "Step [100/390]\t Loss: 0.5702072381973267\n",
            "Step [150/390]\t Loss: 0.7495337128639221\n",
            "Step [200/390]\t Loss: 0.7336885333061218\n",
            "Step [250/390]\t Loss: 0.8599426746368408\n",
            "Step [300/390]\t Loss: 0.5941063761711121\n",
            "Step [350/390]\t Loss: 0.7269041538238525\n",
            "Train Epoch [47]\t Average loss: 0.6793692540663939\n",
            "Test Epoch [47]\t Accuracy: 75.47\t Best Accuracy: 75.47\n",
            "\n",
            "48  ->  100\n",
            "Step [0/390]\t Loss: 0.559604823589325\n",
            "Step [50/390]\t Loss: 0.5105867385864258\n",
            "Step [100/390]\t Loss: 0.5983128547668457\n",
            "Step [150/390]\t Loss: 0.827003002166748\n",
            "Step [200/390]\t Loss: 0.6420004963874817\n",
            "Step [250/390]\t Loss: 0.6137863397598267\n",
            "Step [300/390]\t Loss: 0.6992851495742798\n",
            "Step [350/390]\t Loss: 0.6363063454627991\n",
            "Train Epoch [48]\t Average loss: 0.6730932841698328\n",
            "Test Epoch [48]\t Accuracy: 73.29\t Best Accuracy: 75.47\n",
            "\n",
            "49  ->  100\n",
            "Step [0/390]\t Loss: 0.863275408744812\n",
            "Step [50/390]\t Loss: 0.6016936898231506\n",
            "Step [100/390]\t Loss: 0.7219018936157227\n",
            "Step [150/390]\t Loss: 0.7240017056465149\n",
            "Step [200/390]\t Loss: 0.6513736248016357\n",
            "Step [250/390]\t Loss: 0.6806907653808594\n",
            "Step [300/390]\t Loss: 0.7945363521575928\n",
            "Step [350/390]\t Loss: 0.7500222325325012\n",
            "Train Epoch [49]\t Average loss: 0.671355106586065\n",
            "Test Epoch [49]\t Accuracy: 72.88\t Best Accuracy: 75.47\n",
            "\n",
            "50  ->  100\n",
            "Step [0/390]\t Loss: 0.925783634185791\n",
            "Step [50/390]\t Loss: 0.6526500582695007\n",
            "Step [100/390]\t Loss: 0.6272439956665039\n",
            "Step [150/390]\t Loss: 0.5804184079170227\n",
            "Step [200/390]\t Loss: 0.6441941261291504\n",
            "Step [250/390]\t Loss: 0.8696325421333313\n",
            "Step [300/390]\t Loss: 0.7598087787628174\n",
            "Step [350/390]\t Loss: 0.5905819535255432\n",
            "Train Epoch [50]\t Average loss: 0.6766870724849212\n",
            "Test Epoch [50]\t Accuracy: 75.17\t Best Accuracy: 75.47\n",
            "\n",
            "51  ->  100\n",
            "Step [0/390]\t Loss: 0.6280845403671265\n",
            "Step [50/390]\t Loss: 0.6220354437828064\n",
            "Step [100/390]\t Loss: 0.7513884902000427\n",
            "Step [150/390]\t Loss: 0.7252581119537354\n",
            "Step [200/390]\t Loss: 0.7205561399459839\n",
            "Step [250/390]\t Loss: 0.6077653169631958\n",
            "Step [300/390]\t Loss: 0.7420799732208252\n",
            "Step [350/390]\t Loss: 0.6685603857040405\n",
            "Train Epoch [51]\t Average loss: 0.6725127601470703\n",
            "Test Epoch [51]\t Accuracy: 74.3\t Best Accuracy: 75.47\n",
            "\n",
            "52  ->  100\n",
            "Step [0/390]\t Loss: 0.5929551124572754\n",
            "Step [50/390]\t Loss: 0.6498725414276123\n",
            "Step [100/390]\t Loss: 0.5887473821640015\n",
            "Step [150/390]\t Loss: 0.58258455991745\n",
            "Step [200/390]\t Loss: 0.9140281081199646\n",
            "Step [250/390]\t Loss: 0.9158058762550354\n",
            "Step [300/390]\t Loss: 0.7293810844421387\n",
            "Step [350/390]\t Loss: 0.6002552509307861\n",
            "Train Epoch [52]\t Average loss: 0.6755480733437416\n",
            "Test Epoch [52]\t Accuracy: 75.09\t Best Accuracy: 75.47\n",
            "\n",
            "53  ->  100\n",
            "Step [0/390]\t Loss: 0.6956791877746582\n",
            "Step [50/390]\t Loss: 0.6751120090484619\n",
            "Step [100/390]\t Loss: 0.8603050112724304\n",
            "Step [150/390]\t Loss: 0.6599674224853516\n",
            "Step [200/390]\t Loss: 0.650539755821228\n",
            "Step [250/390]\t Loss: 0.7086649537086487\n",
            "Step [300/390]\t Loss: 0.5928612351417542\n",
            "Step [350/390]\t Loss: 0.9085302948951721\n",
            "Train Epoch [53]\t Average loss: 0.6819823028185429\n",
            "Test Epoch [53]\t Accuracy: 74.85\t Best Accuracy: 75.47\n",
            "\n",
            "54  ->  100\n",
            "Step [0/390]\t Loss: 0.8515777587890625\n",
            "Step [50/390]\t Loss: 0.6684082746505737\n",
            "Step [100/390]\t Loss: 0.8087021112442017\n",
            "Step [150/390]\t Loss: 0.7120134234428406\n",
            "Step [200/390]\t Loss: 0.9529692530632019\n",
            "Step [250/390]\t Loss: 0.7195499539375305\n",
            "Step [300/390]\t Loss: 0.763351321220398\n",
            "Step [350/390]\t Loss: 0.48519545793533325\n",
            "Train Epoch [54]\t Average loss: 0.6746982737993583\n",
            "Test Epoch [54]\t Accuracy: 73.1\t Best Accuracy: 75.47\n",
            "\n",
            "55  ->  100\n",
            "Step [0/390]\t Loss: 0.5799406170845032\n",
            "Step [50/390]\t Loss: 0.5709506273269653\n",
            "Step [100/390]\t Loss: 0.6007527709007263\n",
            "Step [150/390]\t Loss: 0.6859515905380249\n",
            "Step [200/390]\t Loss: 0.5752562880516052\n",
            "Step [250/390]\t Loss: 0.7378588914871216\n",
            "Step [300/390]\t Loss: 0.6185117363929749\n",
            "Step [350/390]\t Loss: 0.7039507627487183\n",
            "Train Epoch [55]\t Average loss: 0.6740969096238797\n",
            "Test Epoch [55]\t Accuracy: 75.17\t Best Accuracy: 75.47\n",
            "\n",
            "56  ->  100\n",
            "Step [0/390]\t Loss: 0.6956635117530823\n",
            "Step [50/390]\t Loss: 0.6577841639518738\n",
            "Step [100/390]\t Loss: 0.7978373169898987\n",
            "Step [150/390]\t Loss: 0.5477572679519653\n",
            "Step [200/390]\t Loss: 0.6834440231323242\n",
            "Step [250/390]\t Loss: 0.7533906698226929\n",
            "Step [300/390]\t Loss: 0.6278698444366455\n",
            "Step [350/390]\t Loss: 0.7539888620376587\n",
            "Train Epoch [56]\t Average loss: 0.6761416676716927\n",
            "Test Epoch [56]\t Accuracy: 74.44\t Best Accuracy: 75.47\n",
            "\n",
            "57  ->  100\n",
            "Step [0/390]\t Loss: 0.6764917969703674\n",
            "Step [50/390]\t Loss: 0.6118073463439941\n",
            "Step [100/390]\t Loss: 0.6345247030258179\n",
            "Step [150/390]\t Loss: 0.7138488292694092\n",
            "Step [200/390]\t Loss: 0.7456894516944885\n",
            "Step [250/390]\t Loss: 0.8890983462333679\n",
            "Step [300/390]\t Loss: 0.8057500123977661\n",
            "Step [350/390]\t Loss: 0.7440354824066162\n",
            "Train Epoch [57]\t Average loss: 0.6768638080511338\n",
            "Test Epoch [57]\t Accuracy: 74.08\t Best Accuracy: 75.47\n",
            "\n",
            "58  ->  100\n",
            "Step [0/390]\t Loss: 0.7428371906280518\n",
            "Step [50/390]\t Loss: 0.6602846384048462\n",
            "Step [100/390]\t Loss: 0.5851900577545166\n",
            "Step [150/390]\t Loss: 0.5417683720588684\n",
            "Step [200/390]\t Loss: 0.6418316960334778\n",
            "Step [250/390]\t Loss: 0.5912238359451294\n",
            "Step [300/390]\t Loss: 0.6239063143730164\n",
            "Step [350/390]\t Loss: 0.6036996841430664\n",
            "Train Epoch [58]\t Average loss: 0.6759725948938957\n",
            "Test Epoch [58]\t Accuracy: 74.33\t Best Accuracy: 75.47\n",
            "\n",
            "59  ->  100\n",
            "Step [0/390]\t Loss: 0.8557857871055603\n",
            "Step [50/390]\t Loss: 0.5964329838752747\n",
            "Step [100/390]\t Loss: 0.6156529784202576\n",
            "Step [150/390]\t Loss: 0.6442456245422363\n",
            "Step [200/390]\t Loss: 0.6332175731658936\n",
            "Step [250/390]\t Loss: 0.5743921399116516\n",
            "Step [300/390]\t Loss: 0.44814008474349976\n",
            "Step [350/390]\t Loss: 0.6945692300796509\n",
            "Train Epoch [59]\t Average loss: 0.670496229483531\n",
            "Test Epoch [59]\t Accuracy: 74.58\t Best Accuracy: 75.47\n",
            "\n",
            "60  ->  100\n",
            "Step [0/390]\t Loss: 0.5148470401763916\n",
            "Step [50/390]\t Loss: 0.6023129224777222\n",
            "Step [100/390]\t Loss: 0.7064851522445679\n",
            "Step [150/390]\t Loss: 0.5664622187614441\n",
            "Step [200/390]\t Loss: 0.7868556976318359\n",
            "Step [250/390]\t Loss: 0.5554232597351074\n",
            "Step [300/390]\t Loss: 0.5876146554946899\n",
            "Step [350/390]\t Loss: 0.653467059135437\n",
            "Train Epoch [60]\t Average loss: 0.6040766114607835\n",
            "Test Epoch [60]\t Accuracy: 76.19\t Best Accuracy: 76.19\n",
            "\n",
            "61  ->  100\n",
            "Step [0/390]\t Loss: 0.5940473675727844\n",
            "Step [50/390]\t Loss: 0.5514439940452576\n",
            "Step [100/390]\t Loss: 0.5327333211898804\n",
            "Step [150/390]\t Loss: 0.5430183410644531\n",
            "Step [200/390]\t Loss: 0.6700483560562134\n",
            "Step [250/390]\t Loss: 0.5832252502441406\n",
            "Step [300/390]\t Loss: 0.5921294689178467\n",
            "Step [350/390]\t Loss: 0.5152447819709778\n",
            "Train Epoch [61]\t Average loss: 0.5980143446952869\n",
            "Test Epoch [61]\t Accuracy: 76.35\t Best Accuracy: 76.35\n",
            "\n",
            "62  ->  100\n",
            "Step [0/390]\t Loss: 0.6376686692237854\n",
            "Step [50/390]\t Loss: 0.5762892365455627\n",
            "Step [100/390]\t Loss: 0.7117592096328735\n",
            "Step [150/390]\t Loss: 0.6343753337860107\n",
            "Step [200/390]\t Loss: 0.6406495571136475\n",
            "Step [250/390]\t Loss: 0.40188395977020264\n",
            "Step [300/390]\t Loss: 0.6048336029052734\n",
            "Step [350/390]\t Loss: 0.4710688889026642\n",
            "Train Epoch [62]\t Average loss: 0.5989231436680524\n",
            "Test Epoch [62]\t Accuracy: 76.38\t Best Accuracy: 76.38\n",
            "\n",
            "63  ->  100\n",
            "Step [0/390]\t Loss: 0.5252881050109863\n",
            "Step [50/390]\t Loss: 0.6965291500091553\n",
            "Step [100/390]\t Loss: 0.4905250072479248\n",
            "Step [150/390]\t Loss: 0.6009974479675293\n",
            "Step [200/390]\t Loss: 0.6564903259277344\n",
            "Step [250/390]\t Loss: 0.6207696199417114\n",
            "Step [300/390]\t Loss: 0.8047472238540649\n",
            "Step [350/390]\t Loss: 0.5040317177772522\n",
            "Train Epoch [63]\t Average loss: 0.5975881400016638\n",
            "Test Epoch [63]\t Accuracy: 76.11\t Best Accuracy: 76.38\n",
            "\n",
            "64  ->  100\n",
            "Step [0/390]\t Loss: 0.6131348609924316\n",
            "Step [50/390]\t Loss: 0.48617833852767944\n",
            "Step [100/390]\t Loss: 0.6463462710380554\n",
            "Step [150/390]\t Loss: 0.618996262550354\n",
            "Step [200/390]\t Loss: 0.5160874724388123\n",
            "Step [250/390]\t Loss: 0.5415862202644348\n",
            "Step [300/390]\t Loss: 0.5420879125595093\n",
            "Step [350/390]\t Loss: 0.5455728769302368\n",
            "Train Epoch [64]\t Average loss: 0.5969157302226775\n",
            "Test Epoch [64]\t Accuracy: 76.42\t Best Accuracy: 76.42\n",
            "\n",
            "65  ->  100\n",
            "Step [0/390]\t Loss: 0.6478508710861206\n",
            "Step [50/390]\t Loss: 0.5382351875305176\n",
            "Step [100/390]\t Loss: 0.7110922932624817\n",
            "Step [150/390]\t Loss: 0.5086644887924194\n",
            "Step [200/390]\t Loss: 0.675453245639801\n",
            "Step [250/390]\t Loss: 0.6024752855300903\n",
            "Step [300/390]\t Loss: 0.6350831985473633\n",
            "Step [350/390]\t Loss: 0.7369582653045654\n",
            "Train Epoch [65]\t Average loss: 0.5970326672761869\n",
            "Test Epoch [65]\t Accuracy: 76.25\t Best Accuracy: 76.42\n",
            "\n",
            "66  ->  100\n",
            "Step [0/390]\t Loss: 0.5762572884559631\n",
            "Step [50/390]\t Loss: 0.6478771567344666\n",
            "Step [100/390]\t Loss: 0.48302727937698364\n",
            "Step [150/390]\t Loss: 0.6285163164138794\n",
            "Step [200/390]\t Loss: 0.7172738313674927\n",
            "Step [250/390]\t Loss: 0.6223592162132263\n",
            "Step [300/390]\t Loss: 0.5721952319145203\n",
            "Step [350/390]\t Loss: 0.6021745204925537\n",
            "Train Epoch [66]\t Average loss: 0.5973226864368487\n",
            "Test Epoch [66]\t Accuracy: 76.18\t Best Accuracy: 76.42\n",
            "\n",
            "67  ->  100\n",
            "Step [0/390]\t Loss: 0.5802783966064453\n",
            "Step [50/390]\t Loss: 0.5205491781234741\n",
            "Step [100/390]\t Loss: 0.6289923191070557\n",
            "Step [150/390]\t Loss: 0.5503527522087097\n",
            "Step [200/390]\t Loss: 0.8185187578201294\n",
            "Step [250/390]\t Loss: 0.6153582334518433\n",
            "Step [300/390]\t Loss: 0.5911594033241272\n",
            "Step [350/390]\t Loss: 0.5275806784629822\n",
            "Train Epoch [67]\t Average loss: 0.5964232727503165\n",
            "Test Epoch [67]\t Accuracy: 76.52\t Best Accuracy: 76.52\n",
            "\n",
            "68  ->  100\n",
            "Step [0/390]\t Loss: 0.58256995677948\n",
            "Step [50/390]\t Loss: 0.6097447872161865\n",
            "Step [100/390]\t Loss: 0.49256882071495056\n",
            "Step [150/390]\t Loss: 0.5737617015838623\n",
            "Step [200/390]\t Loss: 0.6797099709510803\n",
            "Step [250/390]\t Loss: 0.5135084986686707\n",
            "Step [300/390]\t Loss: 0.5604720115661621\n",
            "Step [350/390]\t Loss: 0.7263978123664856\n",
            "Train Epoch [68]\t Average loss: 0.5963206194150142\n",
            "Test Epoch [68]\t Accuracy: 76.49\t Best Accuracy: 76.52\n",
            "\n",
            "69  ->  100\n",
            "Step [0/390]\t Loss: 0.7334052920341492\n",
            "Step [50/390]\t Loss: 0.6054490804672241\n",
            "Step [100/390]\t Loss: 0.5926464796066284\n",
            "Step [150/390]\t Loss: 0.6445248126983643\n",
            "Step [200/390]\t Loss: 0.5542388558387756\n",
            "Step [250/390]\t Loss: 0.642928957939148\n",
            "Step [300/390]\t Loss: 0.6501820683479309\n",
            "Step [350/390]\t Loss: 0.6821446418762207\n",
            "Train Epoch [69]\t Average loss: 0.5970990705184448\n",
            "Test Epoch [69]\t Accuracy: 76.36\t Best Accuracy: 76.52\n",
            "\n",
            "70  ->  100\n",
            "Step [0/390]\t Loss: 0.4457157552242279\n",
            "Step [50/390]\t Loss: 0.5672808289527893\n",
            "Step [100/390]\t Loss: 0.6029455661773682\n",
            "Step [150/390]\t Loss: 0.6863290667533875\n",
            "Step [200/390]\t Loss: 0.7766122221946716\n",
            "Step [250/390]\t Loss: 0.6130622625350952\n",
            "Step [300/390]\t Loss: 0.7244148850440979\n",
            "Step [350/390]\t Loss: 0.5437561869621277\n",
            "Train Epoch [70]\t Average loss: 0.5968214178696657\n",
            "Test Epoch [70]\t Accuracy: 76.66\t Best Accuracy: 76.66\n",
            "\n",
            "71  ->  100\n",
            "Step [0/390]\t Loss: 0.5780106782913208\n",
            "Step [50/390]\t Loss: 0.660519540309906\n",
            "Step [100/390]\t Loss: 0.5324456691741943\n",
            "Step [150/390]\t Loss: 0.5386802554130554\n",
            "Step [200/390]\t Loss: 0.4768061637878418\n",
            "Step [250/390]\t Loss: 0.5136131644248962\n",
            "Step [300/390]\t Loss: 0.46626025438308716\n",
            "Step [350/390]\t Loss: 0.6473038196563721\n",
            "Train Epoch [71]\t Average loss: 0.596056146805103\n",
            "Test Epoch [71]\t Accuracy: 76.24\t Best Accuracy: 76.66\n",
            "\n",
            "72  ->  100\n",
            "Step [0/390]\t Loss: 0.60099196434021\n",
            "Step [50/390]\t Loss: 0.5510765910148621\n",
            "Step [100/390]\t Loss: 0.6653809547424316\n",
            "Step [150/390]\t Loss: 0.6388291120529175\n",
            "Step [200/390]\t Loss: 0.5542277693748474\n",
            "Step [250/390]\t Loss: 0.6517353057861328\n",
            "Step [300/390]\t Loss: 0.5473160147666931\n",
            "Step [350/390]\t Loss: 0.46777164936065674\n",
            "Train Epoch [72]\t Average loss: 0.5967241403384086\n",
            "Test Epoch [72]\t Accuracy: 76.49\t Best Accuracy: 76.66\n",
            "\n",
            "73  ->  100\n",
            "Step [0/390]\t Loss: 0.49911510944366455\n",
            "Step [50/390]\t Loss: 0.6333653330802917\n",
            "Step [100/390]\t Loss: 0.5962651371955872\n",
            "Step [150/390]\t Loss: 0.7526341676712036\n",
            "Step [200/390]\t Loss: 0.6115545034408569\n",
            "Step [250/390]\t Loss: 0.6061791777610779\n",
            "Step [300/390]\t Loss: 0.5208807587623596\n",
            "Step [350/390]\t Loss: 0.5898818373680115\n",
            "Train Epoch [73]\t Average loss: 0.5963965005599535\n",
            "Test Epoch [73]\t Accuracy: 76.49\t Best Accuracy: 76.66\n",
            "\n",
            "74  ->  100\n",
            "Step [0/390]\t Loss: 0.585137128829956\n",
            "Step [50/390]\t Loss: 0.6828752756118774\n",
            "Step [100/390]\t Loss: 0.5765838027000427\n",
            "Step [150/390]\t Loss: 0.5786818861961365\n",
            "Step [200/390]\t Loss: 0.5781117677688599\n",
            "Step [250/390]\t Loss: 0.5323996543884277\n",
            "Step [300/390]\t Loss: 0.5690973401069641\n",
            "Step [350/390]\t Loss: 0.5936232805252075\n",
            "Train Epoch [74]\t Average loss: 0.596031675965358\n",
            "Test Epoch [74]\t Accuracy: 76.24\t Best Accuracy: 76.66\n",
            "\n",
            "75  ->  100\n",
            "Step [0/390]\t Loss: 0.5121830105781555\n",
            "Step [50/390]\t Loss: 0.5609275102615356\n",
            "Step [100/390]\t Loss: 0.4867624044418335\n",
            "Step [150/390]\t Loss: 0.7008668780326843\n",
            "Step [200/390]\t Loss: 0.5609302520751953\n",
            "Step [250/390]\t Loss: 0.7944080829620361\n",
            "Step [300/390]\t Loss: 0.5184046030044556\n",
            "Step [350/390]\t Loss: 0.5983685851097107\n",
            "Train Epoch [75]\t Average loss: 0.5962259319348213\n",
            "Test Epoch [75]\t Accuracy: 76.54\t Best Accuracy: 76.66\n",
            "\n",
            "76  ->  100\n",
            "Step [0/390]\t Loss: 0.7007026672363281\n",
            "Step [50/390]\t Loss: 0.6328336596488953\n",
            "Step [100/390]\t Loss: 0.669215202331543\n",
            "Step [150/390]\t Loss: 0.5742042064666748\n",
            "Step [200/390]\t Loss: 0.6521605849266052\n",
            "Step [250/390]\t Loss: 0.6195005774497986\n",
            "Step [300/390]\t Loss: 0.596167802810669\n",
            "Step [350/390]\t Loss: 0.6807162761688232\n",
            "Train Epoch [76]\t Average loss: 0.5963704389639389\n",
            "Test Epoch [76]\t Accuracy: 76.19\t Best Accuracy: 76.66\n",
            "\n",
            "77  ->  100\n",
            "Step [0/390]\t Loss: 0.5371984243392944\n",
            "Step [50/390]\t Loss: 0.5224665403366089\n",
            "Step [100/390]\t Loss: 0.6806944608688354\n",
            "Step [150/390]\t Loss: 0.6801778078079224\n",
            "Step [200/390]\t Loss: 0.832882285118103\n",
            "Step [250/390]\t Loss: 0.5750203728675842\n",
            "Step [300/390]\t Loss: 0.5411918759346008\n",
            "Step [350/390]\t Loss: 0.6001277565956116\n",
            "Train Epoch [77]\t Average loss: 0.5968010383538711\n",
            "Test Epoch [77]\t Accuracy: 76.33\t Best Accuracy: 76.66\n",
            "\n",
            "78  ->  100\n",
            "Step [0/390]\t Loss: 0.6693857312202454\n",
            "Step [50/390]\t Loss: 0.5800776481628418\n",
            "Step [100/390]\t Loss: 0.5625753402709961\n",
            "Step [150/390]\t Loss: 0.40465065836906433\n",
            "Step [200/390]\t Loss: 0.5602551102638245\n",
            "Step [250/390]\t Loss: 0.6024028658866882\n",
            "Step [300/390]\t Loss: 0.5193927884101868\n",
            "Step [350/390]\t Loss: 0.6146784424781799\n",
            "Train Epoch [78]\t Average loss: 0.5962007751831642\n",
            "Test Epoch [78]\t Accuracy: 76.61\t Best Accuracy: 76.66\n",
            "\n",
            "79  ->  100\n",
            "Step [0/390]\t Loss: 0.5051805377006531\n",
            "Step [50/390]\t Loss: 0.5568660497665405\n",
            "Step [100/390]\t Loss: 0.47581425309181213\n",
            "Step [150/390]\t Loss: 0.8180215954780579\n",
            "Step [200/390]\t Loss: 0.6291434168815613\n",
            "Step [250/390]\t Loss: 0.5968216061592102\n",
            "Step [300/390]\t Loss: 0.47893813252449036\n",
            "Step [350/390]\t Loss: 0.5576484203338623\n",
            "Train Epoch [79]\t Average loss: 0.5956198844390038\n",
            "Test Epoch [79]\t Accuracy: 76.42\t Best Accuracy: 76.66\n",
            "\n",
            "80  ->  100\n",
            "Step [0/390]\t Loss: 0.5740127563476562\n",
            "Step [50/390]\t Loss: 0.6059759259223938\n",
            "Step [100/390]\t Loss: 0.7028139233589172\n",
            "Step [150/390]\t Loss: 0.638204038143158\n",
            "Step [200/390]\t Loss: 0.5903713703155518\n",
            "Step [250/390]\t Loss: 0.564598560333252\n",
            "Step [300/390]\t Loss: 0.6104065775871277\n",
            "Step [350/390]\t Loss: 0.40013831853866577\n",
            "Train Epoch [80]\t Average loss: 0.590375311481647\n",
            "Test Epoch [80]\t Accuracy: 76.58\t Best Accuracy: 76.66\n",
            "\n",
            "81  ->  100\n",
            "Step [0/390]\t Loss: 0.5403626561164856\n",
            "Step [50/390]\t Loss: 0.7118562459945679\n",
            "Step [100/390]\t Loss: 0.6002427935600281\n",
            "Step [150/390]\t Loss: 0.6556417942047119\n",
            "Step [200/390]\t Loss: 0.6408215165138245\n",
            "Step [250/390]\t Loss: 0.4489595293998718\n",
            "Step [300/390]\t Loss: 0.5283933281898499\n",
            "Step [350/390]\t Loss: 0.530548632144928\n",
            "Train Epoch [81]\t Average loss: 0.5897637523137607\n",
            "Test Epoch [81]\t Accuracy: 76.65\t Best Accuracy: 76.66\n",
            "\n",
            "82  ->  100\n",
            "Step [0/390]\t Loss: 0.7553932070732117\n",
            "Step [50/390]\t Loss: 0.614782452583313\n",
            "Step [100/390]\t Loss: 0.5816197395324707\n",
            "Step [150/390]\t Loss: 0.6088330745697021\n",
            "Step [200/390]\t Loss: 0.7076383829116821\n",
            "Step [250/390]\t Loss: 0.550585150718689\n",
            "Step [300/390]\t Loss: 0.5464099645614624\n",
            "Step [350/390]\t Loss: 0.6278948187828064\n",
            "Train Epoch [82]\t Average loss: 0.5896409960893484\n",
            "Test Epoch [82]\t Accuracy: 76.63\t Best Accuracy: 76.66\n",
            "\n",
            "83  ->  100\n",
            "Step [0/390]\t Loss: 0.5093333125114441\n",
            "Step [50/390]\t Loss: 0.4694236218929291\n",
            "Step [100/390]\t Loss: 0.560699999332428\n",
            "Step [150/390]\t Loss: 0.5151400566101074\n",
            "Step [200/390]\t Loss: 0.6155423521995544\n",
            "Step [250/390]\t Loss: 0.7154136896133423\n",
            "Step [300/390]\t Loss: 0.5230914950370789\n",
            "Step [350/390]\t Loss: 0.60357666015625\n",
            "Train Epoch [83]\t Average loss: 0.5899328815631377\n",
            "Test Epoch [83]\t Accuracy: 76.63\t Best Accuracy: 76.66\n",
            "\n",
            "84  ->  100\n",
            "Step [0/390]\t Loss: 0.5334715247154236\n",
            "Step [50/390]\t Loss: 0.6791723966598511\n",
            "Step [100/390]\t Loss: 0.5708696842193604\n",
            "Step [150/390]\t Loss: 0.5247225165367126\n",
            "Step [200/390]\t Loss: 0.5962761640548706\n",
            "Step [250/390]\t Loss: 0.6132287383079529\n",
            "Step [300/390]\t Loss: 0.46702414751052856\n",
            "Step [350/390]\t Loss: 0.5994310975074768\n",
            "Train Epoch [84]\t Average loss: 0.589716551013482\n",
            "Test Epoch [84]\t Accuracy: 76.7\t Best Accuracy: 76.7\n",
            "\n",
            "85  ->  100\n",
            "Step [0/390]\t Loss: 0.5351486206054688\n",
            "Step [50/390]\t Loss: 0.5302455425262451\n",
            "Step [100/390]\t Loss: 0.5060040354728699\n",
            "Step [150/390]\t Loss: 0.5884193181991577\n",
            "Step [200/390]\t Loss: 0.44450023770332336\n",
            "Step [250/390]\t Loss: 0.6180232167243958\n",
            "Step [300/390]\t Loss: 0.6124752163887024\n",
            "Step [350/390]\t Loss: 0.6262261271476746\n",
            "Train Epoch [85]\t Average loss: 0.5893489718437195\n",
            "Test Epoch [85]\t Accuracy: 76.62\t Best Accuracy: 76.7\n",
            "\n",
            "86  ->  100\n",
            "Step [0/390]\t Loss: 0.5757213830947876\n",
            "Step [50/390]\t Loss: 0.4567183554172516\n",
            "Step [100/390]\t Loss: 0.6360058784484863\n",
            "Step [150/390]\t Loss: 0.44055208563804626\n",
            "Step [200/390]\t Loss: 0.6085498929023743\n",
            "Step [250/390]\t Loss: 0.6373976469039917\n",
            "Step [300/390]\t Loss: 0.5145523548126221\n",
            "Step [350/390]\t Loss: 0.4907737970352173\n",
            "Train Epoch [86]\t Average loss: 0.5895332551155334\n",
            "Test Epoch [86]\t Accuracy: 76.6\t Best Accuracy: 76.7\n",
            "\n",
            "87  ->  100\n",
            "Step [0/390]\t Loss: 0.6388319134712219\n",
            "Step [50/390]\t Loss: 0.744678258895874\n",
            "Step [100/390]\t Loss: 0.4708087742328644\n",
            "Step [150/390]\t Loss: 0.6064319610595703\n",
            "Step [200/390]\t Loss: 0.6165575981140137\n",
            "Step [250/390]\t Loss: 0.6342570185661316\n",
            "Step [300/390]\t Loss: 0.5799261927604675\n",
            "Step [350/390]\t Loss: 0.49406781792640686\n",
            "Train Epoch [87]\t Average loss: 0.5894467025995255\n",
            "Test Epoch [87]\t Accuracy: 76.64\t Best Accuracy: 76.7\n",
            "\n",
            "88  ->  100\n",
            "Step [0/390]\t Loss: 0.5050644278526306\n",
            "Step [50/390]\t Loss: 0.7820540070533752\n",
            "Step [100/390]\t Loss: 0.5781296491622925\n",
            "Step [150/390]\t Loss: 0.5326557159423828\n",
            "Step [200/390]\t Loss: 0.4698619544506073\n",
            "Step [250/390]\t Loss: 0.6227988004684448\n",
            "Step [300/390]\t Loss: 0.6031336784362793\n",
            "Step [350/390]\t Loss: 0.6607245802879333\n",
            "Train Epoch [88]\t Average loss: 0.5899613504990553\n",
            "Test Epoch [88]\t Accuracy: 76.65\t Best Accuracy: 76.7\n",
            "\n",
            "89  ->  100\n",
            "Step [0/390]\t Loss: 0.6352817416191101\n",
            "Step [50/390]\t Loss: 0.4606238305568695\n",
            "Step [100/390]\t Loss: 0.6468147039413452\n",
            "Step [150/390]\t Loss: 0.6040142178535461\n",
            "Step [200/390]\t Loss: 0.670894980430603\n",
            "Step [250/390]\t Loss: 0.39165568351745605\n",
            "Step [300/390]\t Loss: 0.6290926933288574\n",
            "Step [350/390]\t Loss: 0.6864822506904602\n",
            "Train Epoch [89]\t Average loss: 0.5895378019565191\n",
            "Test Epoch [89]\t Accuracy: 76.68\t Best Accuracy: 76.7\n",
            "\n",
            "90  ->  100\n",
            "Step [0/390]\t Loss: 0.6959326863288879\n",
            "Step [50/390]\t Loss: 0.5646107792854309\n",
            "Step [100/390]\t Loss: 0.586900532245636\n",
            "Step [150/390]\t Loss: 0.6336479783058167\n",
            "Step [200/390]\t Loss: 0.5315317511558533\n",
            "Step [250/390]\t Loss: 0.588900625705719\n",
            "Step [300/390]\t Loss: 0.5143061876296997\n",
            "Step [350/390]\t Loss: 0.6079913377761841\n",
            "Train Epoch [90]\t Average loss: 0.5896098581644205\n",
            "Test Epoch [90]\t Accuracy: 76.59\t Best Accuracy: 76.7\n",
            "\n",
            "91  ->  100\n",
            "Step [0/390]\t Loss: 0.3925243020057678\n",
            "Step [50/390]\t Loss: 0.6069639921188354\n",
            "Step [100/390]\t Loss: 0.5967402458190918\n",
            "Step [150/390]\t Loss: 0.6496724486351013\n",
            "Step [200/390]\t Loss: 0.6411715149879456\n",
            "Step [250/390]\t Loss: 0.5431829690933228\n",
            "Step [300/390]\t Loss: 0.5800618529319763\n",
            "Step [350/390]\t Loss: 0.5965499877929688\n",
            "Train Epoch [91]\t Average loss: 0.5892660136406238\n",
            "Test Epoch [91]\t Accuracy: 76.7\t Best Accuracy: 76.7\n",
            "\n",
            "92  ->  100\n",
            "Step [0/390]\t Loss: 0.583575427532196\n",
            "Step [50/390]\t Loss: 0.605654776096344\n",
            "Step [100/390]\t Loss: 0.5025478005409241\n",
            "Step [150/390]\t Loss: 0.5526249408721924\n",
            "Step [200/390]\t Loss: 0.6753514409065247\n",
            "Step [250/390]\t Loss: 0.5260026454925537\n",
            "Step [300/390]\t Loss: 0.6692097187042236\n",
            "Step [350/390]\t Loss: 0.5241711139678955\n",
            "Train Epoch [92]\t Average loss: 0.5895083202765539\n",
            "Test Epoch [92]\t Accuracy: 76.74\t Best Accuracy: 76.74\n",
            "\n",
            "93  ->  100\n",
            "Step [0/390]\t Loss: 0.5134329199790955\n",
            "Step [50/390]\t Loss: 0.6133536696434021\n",
            "Step [100/390]\t Loss: 0.62150639295578\n",
            "Step [150/390]\t Loss: 0.7252863645553589\n",
            "Step [200/390]\t Loss: 0.5408502221107483\n",
            "Step [250/390]\t Loss: 0.7472290992736816\n",
            "Step [300/390]\t Loss: 0.501556932926178\n",
            "Step [350/390]\t Loss: 0.6142487525939941\n",
            "Train Epoch [93]\t Average loss: 0.5895722004083487\n",
            "Test Epoch [93]\t Accuracy: 76.67\t Best Accuracy: 76.74\n",
            "\n",
            "94  ->  100\n",
            "Step [0/390]\t Loss: 0.7202479839324951\n",
            "Step [50/390]\t Loss: 0.4728895425796509\n",
            "Step [100/390]\t Loss: 0.4691060781478882\n",
            "Step [150/390]\t Loss: 0.4974084496498108\n",
            "Step [200/390]\t Loss: 0.4961838126182556\n",
            "Step [250/390]\t Loss: 0.5994418859481812\n",
            "Step [300/390]\t Loss: 0.5358909368515015\n",
            "Step [350/390]\t Loss: 0.6806326508522034\n",
            "Train Epoch [94]\t Average loss: 0.5899744069729096\n",
            "Test Epoch [94]\t Accuracy: 76.67\t Best Accuracy: 76.74\n",
            "\n",
            "95  ->  100\n",
            "Step [0/390]\t Loss: 0.5861114859580994\n",
            "Step [50/390]\t Loss: 0.594584047794342\n",
            "Step [100/390]\t Loss: 0.4587281048297882\n",
            "Step [150/390]\t Loss: 0.5051047205924988\n",
            "Step [200/390]\t Loss: 0.6246258020401001\n",
            "Step [250/390]\t Loss: 0.5652410387992859\n",
            "Step [300/390]\t Loss: 0.6741702556610107\n",
            "Step [350/390]\t Loss: 0.6615810394287109\n",
            "Train Epoch [95]\t Average loss: 0.5892717411120733\n",
            "Test Epoch [95]\t Accuracy: 76.67\t Best Accuracy: 76.74\n",
            "\n",
            "96  ->  100\n",
            "Step [0/390]\t Loss: 0.6710076332092285\n",
            "Step [50/390]\t Loss: 0.46688979864120483\n",
            "Step [100/390]\t Loss: 0.6445094347000122\n",
            "Step [150/390]\t Loss: 0.44907820224761963\n",
            "Step [200/390]\t Loss: 0.6321037411689758\n",
            "Step [250/390]\t Loss: 0.4042167067527771\n",
            "Step [300/390]\t Loss: 0.5709686875343323\n",
            "Step [350/390]\t Loss: 0.7598707675933838\n",
            "Train Epoch [96]\t Average loss: 0.5893243686510966\n",
            "Test Epoch [96]\t Accuracy: 76.66\t Best Accuracy: 76.74\n",
            "\n",
            "97  ->  100\n",
            "Step [0/390]\t Loss: 0.6610627174377441\n",
            "Step [50/390]\t Loss: 0.724199652671814\n",
            "Step [100/390]\t Loss: 0.3891805410385132\n",
            "Step [150/390]\t Loss: 0.6071985960006714\n",
            "Step [200/390]\t Loss: 0.5410310626029968\n",
            "Step [250/390]\t Loss: 0.5260480046272278\n",
            "Step [300/390]\t Loss: 0.4409906268119812\n",
            "Step [350/390]\t Loss: 0.6951256394386292\n",
            "Train Epoch [97]\t Average loss: 0.5895614516276579\n",
            "Test Epoch [97]\t Accuracy: 76.62\t Best Accuracy: 76.74\n",
            "\n",
            "98  ->  100\n",
            "Step [0/390]\t Loss: 0.5976232886314392\n",
            "Step [50/390]\t Loss: 0.6379160284996033\n",
            "Step [100/390]\t Loss: 0.5456467866897583\n",
            "Step [150/390]\t Loss: 0.5818333625793457\n",
            "Step [200/390]\t Loss: 0.40791141986846924\n",
            "Step [250/390]\t Loss: 0.6745087504386902\n",
            "Step [300/390]\t Loss: 0.4359299838542938\n",
            "Step [350/390]\t Loss: 0.662652313709259\n",
            "Train Epoch [98]\t Average loss: 0.5898808594697561\n",
            "Test Epoch [98]\t Accuracy: 76.73\t Best Accuracy: 76.74\n",
            "\n",
            "99  ->  100\n",
            "Step [0/390]\t Loss: 0.6298992037773132\n",
            "Step [50/390]\t Loss: 0.6117210388183594\n",
            "Step [100/390]\t Loss: 0.5560458302497864\n",
            "Step [150/390]\t Loss: 0.771123468875885\n",
            "Step [200/390]\t Loss: 0.5823166370391846\n",
            "Step [250/390]\t Loss: 0.5798720717430115\n",
            "Step [300/390]\t Loss: 0.6364471912384033\n",
            "Step [350/390]\t Loss: 0.6998751759529114\n",
            "Train Epoch [99]\t Average loss: 0.5892373676483448\n",
            "Test Epoch [99]\t Accuracy: 76.65\t Best Accuracy: 76.74\n",
            "\n",
            "Final Best Accuracy: 76.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your loss values for each model\n",
        "model1_losses = [5.797138889019306, 5.484603649530655, 5.326670230963291, 5.223193144186949, 5.156870937347412, 5.109294336270064, 5.071587983155862, 5.049821726481119, 5.022656242664044, 5.008277736566005, 4.993439737955729, 4.979922458453056, 4.968852302355644, 4.962911847921518, 4.949985494369115, 4.942127875792674, 4.932570948967567, 4.930655325376071, 4.923004390031863, 4.915101765363644, 4.913053507682605, 4.904339650961069, 4.900300216674805, 4.897521280631041, 4.895509749192458, 4.889947695609851, 4.883710193634033, 4.884594215490879, 4.875672474885598, 4.872811131599622]\n",
        "\n",
        "model2_losses = [6.272327550252279, 5.621863714853922, 5.447281529353215, 5.3363340989137305, 5.273751158592028, 5.220987363962027, 5.178464366228153, 5.147619782961332, 5.124963833735539, 5.1034412677471455, 5.084142709389711, 5.07044622470171, 5.052771257742857, 5.041964433132073, 5.027755984281883, 5.01721997872377, 5.013136528699826, 5.004942267980331, 4.991191744193053, 4.98416594236325, 4.975762367248535, 4.9693294378427355, 4.965312434465457, 4.957261332487449, 4.9509335346710985, 4.943993898538443, 4.940752508701422, 4.93480636645586, 4.927824722192226, 4.925734431927021]\n",
        "\n",
        "model3_losses = [5.747309775230212, 5.361246272845146, 5.225735326913687, 5.1335507466242865, 5.060245284056053, 5.014018733684833, 4.968863389430902, 4.929931669968825, 4.891392641801101, 4.864063030634171, 4.828742971175756, 4.793576367696127, 4.7743206977844235, 4.747930954664182, 4.7289455731709795, 4.710396250700339, 4.701737313392835, 4.683246074578701, 4.671169973031068, 4.665059659419915, 4.64995560768323, 4.641148021893623, 4.634004475520207, 4.624818770090739, 4.621244545471974, 4.613050294533754, 4.60966236897004, 4.600691531254695, 4.593987222818228, 4.591921644944411]\n",
        "# Create a figure and axis object for your plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Set the title and labels for your plot\n",
        "ax.set_title('Model Losses')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "\n",
        "# Plot your loss values for each model\n",
        "ax.plot(range(1, len(model1_losses)+1), model1_losses, label='SimCLR + RC')\n",
        "ax.plot(range(1, len(model2_losses)+1), model2_losses, label='SimCLR')\n",
        "ax.plot(range(1, len(model3_losses)+1), model3_losses, label='SimCLR + LBE')\n",
        "\n",
        "# Add a legend to your plot\n",
        "ax.legend()\n",
        "\n",
        "# Display your plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uftDpTfjpS0e",
        "outputId": "5599d22a-575b-4dca-ea32-8a00c2c503fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}